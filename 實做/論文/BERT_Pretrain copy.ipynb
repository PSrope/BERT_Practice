{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定 Tokenizer 與使用 BertForPretraining 來做 BERT 預訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining, AdamW\n",
    "from transformers.models.bert.modeling_bert import BertForPreTrainingOutput, BertPreTrainingHeads, BertConfig, BERT_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "from transformers.models.albert.modeling_albert import AlbertSOPHead\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTrainingOutput(BertForPreTrainingOutput):\n",
    "    def __init__(self, loss=None, prediction_logits=None, seq_relationship_logits=None, hidden_states=None, attentions=None, mlm_loss=None, nsp_loss=None):\n",
    "        super().__init__(loss=loss, prediction_logits=prediction_logits, seq_relationship_logits=seq_relationship_logits, hidden_states=hidden_states, attentions=attentions)\n",
    "        self.mlm_loss = mlm_loss\n",
    "        self.nsp_loss = nsp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlbertSOPHead(torch.nn.Module):\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.classifier_dropout , config.num_labels)\n",
    "\n",
    "    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n",
    "        dropout_pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPretrainingHeadsWithSOP(BertPreTrainingHeads):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.seq_relationship = MyAlbertSOPHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTraining(BertForPreTraining):\n",
    "    def __init__(self, config, nspTask = \"NSP\"):\n",
    "        super().__init__(config)\n",
    "        if nspTask == \"SOP\":\n",
    "            self.cls = BertPretrainingHeadsWithSOP(config)\n",
    "            \n",
    "    # @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    # @replace_return_docstrings(output_type=MyBertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MyBertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return MyBertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            mlm_loss = masked_lm_loss,\n",
    "            nsp_loss = next_sentence_loss,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取出資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getData():\n",
    "    def __init__(self, modelType, datapath, maskPercent, nspTask = \"NSP\"):\n",
    "        self.datapath = datapath\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.maskPercent = maskPercent\n",
    "        self.nspTask = nspTask\n",
    "        self.text = self.toText()\n",
    "        self.inputs = None\n",
    "        self.nspPrepare()\n",
    "        self.mlmPrepare()\n",
    "    \n",
    "    def toText(self):\n",
    "        df = pd.read_csv(self.datapath)\n",
    "        text = []\n",
    "        for review in df[\"text\"]:\n",
    "            text.append(review)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def nspPrepare(self):\n",
    "        bag = [item for sentence in self.text for item in sentence.split('.') if item != '']\n",
    "        bag_size = len(bag)\n",
    "\n",
    "        if self.nspTask == \"NSP\":\n",
    "            (sentence_a, sentence_b, label) = self.nspData(bag, bag_size)\n",
    "        elif self.nspTask == \"SOP\":\n",
    "            (sentence_a, sentence_b, label) = self.sopData()\n",
    "\n",
    "        self.inputs = self.tokenizer(sentence_a, sentence_b, return_tensors='pt',\n",
    "                   max_length=512, truncation=True, padding='max_length')\n",
    "        self.inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "    \n",
    "    def nspData(self, bag, bag_size):\n",
    "        sentence_a = []\n",
    "        sentence_b = []\n",
    "        label = []\n",
    "        for paragraph in self.text:\n",
    "            sentences = [\n",
    "                sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "            ]\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    sentence_a.append(sentences[start])\n",
    "                    sentence_b.append(sentences[start+1])\n",
    "                    label.append(0)\n",
    "                else:\n",
    "                    index = random.randint(0, bag_size-1)\n",
    "                    # this is NotNextSentence\n",
    "                    sentence_a.append(sentences[start])\n",
    "                    sentence_b.append(bag[index])\n",
    "                    label.append(1)\n",
    "        \n",
    "        return (sentence_a, sentence_b, label)\n",
    "    \n",
    "    def sopData(self):\n",
    "        sentence_a = []\n",
    "        sentence_b = []\n",
    "        label = []\n",
    "        for paragraph in self.text:\n",
    "            sentences = [\n",
    "                sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "            ]\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    sentence_a.append(sentences[start])\n",
    "                    sentence_b.append(sentences[start+1])\n",
    "                    label.append(0)\n",
    "                else:\n",
    "                    # this is NotNextSentence\n",
    "                    sentence_a.append(sentences[start+1])\n",
    "                    sentence_b.append(sentences[start])\n",
    "                    label.append(1)\n",
    "        \n",
    "        return (sentence_a, sentence_b, label)\n",
    "\n",
    "    def mlmPrepare(self):\n",
    "        self.inputs['labels'] = self.inputs.input_ids.detach().clone()\n",
    "        rand = torch.rand(self.inputs.input_ids.shape)\n",
    "        # create mask array\n",
    "        mask_arr = (rand < self.maskPercent * 0.01) * (self.inputs.input_ids != 101) * \\\n",
    "                (self.inputs.input_ids != 102) * (self.inputs.input_ids != 0)\n",
    "        self.inputs['mask_arr'] = mask_arr\n",
    "        \n",
    "        selection = []\n",
    "\n",
    "        for i in range(self.inputs.input_ids.shape[0]):\n",
    "            selection.append(\n",
    "                torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            )\n",
    "\n",
    "        rand_mask_type = copy.deepcopy(selection)\n",
    "\n",
    "        for row in range(len(rand_mask_type)):\n",
    "            for col in range(len(rand_mask_type[row])):\n",
    "                rand_mask_type[row][col] = random.random()\n",
    "\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        special_tokens = [vocab['[CLS]'], vocab['[SEP]'], vocab['[MASK]'], vocab['[UNK]'],  vocab['[PAD]']]\n",
    "\n",
    "        for i in range(self.inputs.input_ids.shape[0]):\n",
    "            for j in range(len(selection[i])):\n",
    "                if rand_mask_type[i][j] < 0.10:\n",
    "                    continue\n",
    "                elif rand_mask_type[i][j] < 0.20:\n",
    "                    rand_num = vocab['[CLS]']\n",
    "                    while rand_num in special_tokens:\n",
    "                        rand_num = random.randint(1, vocab_size)\n",
    "                    self.inputs.input_ids[i, selection[i][j]] = rand_num\n",
    "                else:\n",
    "                    self.inputs.input_ids[i, selection[i][j]] = 103\n",
    "    \n",
    "    def returnInput(self):\n",
    "        return self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel():\n",
    "    def __init__(self, modelType, inputs, batch_size, epoch, maskPercent, saveModelName, saveCSV = True, nspTask = \"NSP\"):\n",
    "        config = BertConfig.from_pretrained(modelType)\n",
    "        self.model = MyBertForPreTraining(config, nspTask=nspTask)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.maskPercent = maskPercent\n",
    "        self.saveModelName = saveModelName\n",
    "        self.saveCSV = saveCSV\n",
    "        self.loader = torch.utils.data.DataLoader(OurDataset(self.inputs), \\\n",
    "                                             batch_size=self.batch_size, shuffle=True)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        self.optim = AdamW(self.model.parameters(), lr = 5e-5)\n",
    "        self.record = {\"mask_percent\": None,\n",
    "                  \"mlm_acc_each_epoch\": [], \n",
    "                  \"mlm_loss_each_epoch\": []}\n",
    "        \n",
    "        if os.path.isfile(\"record.csv\"):\n",
    "            self.rec = pd.read_csv(\"record.csv\")\n",
    "        else:\n",
    "            self.rec = pd.DataFrame()\n",
    "            \n",
    "        self.training()\n",
    "        self.save_model(self.saveModelName)\n",
    "    \n",
    "    def training(self):\n",
    "        acc_each_epoch = []\n",
    "        loss_each_epoch = []\n",
    "        for epoch in range(self.epoch):\n",
    "            # setup loop with TQDM and dataloader\n",
    "            mask_nums = 0\n",
    "            mlm_correct = 0\n",
    "            nsp_nums = 0\n",
    "            nsp_correct = 0\n",
    "            loop = tqdm(self.loader, leave=True)\n",
    "            for batch in loop:\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                self.optim.zero_grad()\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                next_sentence_label = batch['next_sentence_label'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                mask_arr = batch['mask_arr'].to(self.device)\n",
    "                # process\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                next_sentence_label=next_sentence_label,\n",
    "                                labels=labels)\n",
    "                \n",
    "                prediction_logits = outputs.prediction_logits[mask_arr]\n",
    "                predicted_ids = prediction_logits.argmax(-1)\n",
    "                \n",
    "                seq_relationship_logits = outputs.seq_relationship_logits\n",
    "                predicted_labels = torch.argmax(seq_relationship_logits, dim=1)\n",
    "                predicted_label = predicted_labels\n",
    "\n",
    "                mask_nums += len(predicted_ids)\n",
    "                mlm_correct += torch.eq(predicted_ids, labels[mask_arr]).sum().item()\n",
    "                nsp_nums += len(predicted_label)\n",
    "                nsp_correct += predicted_label.eq(torch.squeeze(next_sentence_label)).sum().item()\n",
    "                \n",
    "                # extract loss\n",
    "                loss = outputs.loss\n",
    "                mlm_loss = outputs.mlm_loss.item()\n",
    "                nsp_loss = outputs.nsp_loss.item()\n",
    "                mlm_acc = mlm_correct / mask_nums\n",
    "                nsp_acc = nsp_correct / nsp_nums\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                self.optim.step()\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(Total_loss='{:.4f}'.format(loss.item()), MLM_Accuracy='{:.4f}'.format(mlm_acc), NSP_Accuracy='{:.4f}'.format(nsp_acc), \\\n",
    "                                MLM_loss='{:.4f}'.format(mlm_loss), NSP_loss='{:.4f}'.format(nsp_loss))\n",
    "            acc_each_epoch.append(mlm_acc)\n",
    "            loss_each_epoch.append(mlm_loss)\n",
    "\n",
    "        if self.saveCSV:\n",
    "            self.record[\"mask_percent\"] = self.maskPercent\n",
    "            self.record[\"mlm_acc_each_epoch\"].append(acc_each_epoch)\n",
    "            self.record[\"mlm_loss_each_epoch\"].append(loss_each_epoch)\n",
    "            new_rec = self.rec.append(self.record, ignore_index=True)\n",
    "            new_rec.to_csv(\"record.csv\", index = None)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def save_model(self, maskPercent):\n",
    "        self.model.save_pretrained(maskPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'bbc-text.csv'\n",
    "modelType = 'bert-base-cased'\n",
    "epoch = 10\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_20124\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0:  82%|████████▏ | 303/371 [01:45<00:23,  2.86it/s, MLM_Accuracy=0.0299, MLM_loss=0.5482, NSP_Accuracy=0.5105, NSP_loss=0.7282, Total_loss=1.2763] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 12\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mask15_input \u001b[39m=\u001b[39m getData(modelType \u001b[39m=\u001b[39m modelType, datapath \u001b[39m=\u001b[39m datapath, maskPercent \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m, nspTask \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNSP\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mask15 \u001b[39m=\u001b[39m trainModel(modelType \u001b[39m=\u001b[39;49m modelType, inputs \u001b[39m=\u001b[39;49m mask15_input\u001b[39m.\u001b[39;49mreturnInput(), batch_size \u001b[39m=\u001b[39;49m batch_size, epoch \u001b[39m=\u001b[39;49m epoch, maskPercent \u001b[39m=\u001b[39;49m \u001b[39m15\u001b[39;49m, saveModelName \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msaved_model_mask15\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mask15_input \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mask15 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 12\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaveModelName)\n",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 12\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m mask_arr \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mmask_arr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# process\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                 token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                 next_sentence_label\u001b[39m=\u001b[39;49mnext_sentence_label,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                 labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m prediction_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mprediction_logits[mask_arr]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m predicted_ids \u001b[39m=\u001b[39m prediction_logits\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 12\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     input_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m sequence_output, pooled_output \u001b[39m=\u001b[39m outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X14sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m prediction_scores, seq_relationship_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output, pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1005\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1007\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1008\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1009\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1015\u001b[0m     embedding_output,\n\u001b[0;32m   1016\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1017\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1018\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1019\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1020\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1021\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1022\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1023\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1024\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1025\u001b[0m )\n\u001b[0;32m   1026\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1027\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    594\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    596\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    602\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[0;32m    605\u001b[0m         attention_mask,\n\u001b[0;32m    606\u001b[0m         layer_head_mask,\n\u001b[0;32m    607\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m         past_key_value,\n\u001b[0;32m    610\u001b[0m         output_attentions,\n\u001b[0;32m    611\u001b[0m     )\n\u001b[0;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    614\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:531\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    528\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    529\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 531\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    533\u001b[0m )\n\u001b[0;32m    534\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    536\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:543\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 543\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    544\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    545\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:443\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 443\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    444\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "mask15_input = getData(modelType = modelType, datapath = datapath, maskPercent = 15, nspTask = \"NSP\")\n",
    "mask15 = trainModel(modelType = modelType, inputs = mask15_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 15, saveModelName = \"saved_model_mask15\")\n",
    "mask15_input = None\n",
    "mask15 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_22952\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.0439, MLM_loss=0.4509, NSP_Accuracy=0.4894, NSP_loss=0.7335, Total_loss=1.1844] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1162, MLM_loss=0.2112, NSP_Accuracy=0.4962, NSP_loss=0.7149, Total_loss=0.9261]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1343, MLM_loss=0.1106, NSP_Accuracy=0.5074, NSP_loss=0.6038, Total_loss=0.7143]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:11<00:00,  2.81it/s, MLM_Accuracy=0.1431, MLM_loss=0.0881, NSP_Accuracy=0.4980, NSP_loss=0.6974, Total_loss=0.7854]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1504, MLM_loss=0.1165, NSP_Accuracy=0.5065, NSP_loss=0.7217, Total_loss=0.8382]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1614, MLM_loss=0.0919, NSP_Accuracy=0.5160, NSP_loss=0.6737, Total_loss=0.7656]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1771, MLM_loss=0.0786, NSP_Accuracy=0.5218, NSP_loss=0.6846, Total_loss=0.7632]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1820, MLM_loss=0.1228, NSP_Accuracy=0.6764, NSP_loss=0.4748, Total_loss=0.5976]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1805, MLM_loss=0.0968, NSP_Accuracy=0.8917, NSP_loss=0.1635, Total_loss=0.2603]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1841, MLM_loss=0.0942, NSP_Accuracy=0.9676, NSP_loss=0.0188, Total_loss=0.1131]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_22952\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask14_input = getData(modelType = modelType, datapath = datapath, maskPercent = 14, nspTask = \"NSP\")\n",
    "mask14 = trainModel(modelType = modelType, inputs = mask14_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 14, saveModelName = \"saved_model_mask14\")\n",
    "mask14_input = None\n",
    "mask14 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_28280\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.0371, MLM_loss=0.3893, NSP_Accuracy=0.5065, NSP_loss=0.5991, Total_loss=0.9885] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.1135, MLM_loss=0.1787, NSP_Accuracy=0.4944, NSP_loss=0.6860, Total_loss=0.8647]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.1317, MLM_loss=0.1527, NSP_Accuracy=0.5155, NSP_loss=0.6734, Total_loss=0.8261]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:12<00:00,  2.80it/s, MLM_Accuracy=0.1419, MLM_loss=0.1141, NSP_Accuracy=0.5029, NSP_loss=0.7231, Total_loss=0.8372]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1486, MLM_loss=0.0901, NSP_Accuracy=0.5088, NSP_loss=0.8216, Total_loss=0.9117]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1639, MLM_loss=0.1121, NSP_Accuracy=0.5164, NSP_loss=0.7256, Total_loss=0.8376]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1807, MLM_loss=0.1122, NSP_Accuracy=0.4962, NSP_loss=0.6649, Total_loss=0.7771]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:10<00:00,  2.83it/s, MLM_Accuracy=0.1966, MLM_loss=0.0890, NSP_Accuracy=0.5092, NSP_loss=0.7226, Total_loss=0.8115]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:11<00:00,  2.81it/s, MLM_Accuracy=0.2024, MLM_loss=0.1021, NSP_Accuracy=0.5964, NSP_loss=0.9309, Total_loss=1.0330]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:11<00:00,  2.81it/s, MLM_Accuracy=0.1977, MLM_loss=0.1219, NSP_Accuracy=0.7901, NSP_loss=0.0909, Total_loss=0.2127]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_28280\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask13_input = getData(modelType = modelType, datapath = datapath, maskPercent = 13, nspTask = \"NSP\")\n",
    "mask13 = trainModel(modelType = modelType, inputs = mask13_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 13, saveModelName = \"saved_model_mask13\")\n",
    "mask13_input = None\n",
    "mask13 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_28280\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.0386, MLM_loss=0.4177, NSP_Accuracy=0.4935, NSP_loss=0.6417, Total_loss=1.0594] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1064, MLM_loss=0.2090, NSP_Accuracy=0.4912, NSP_loss=0.7271, Total_loss=0.9361]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1251, MLM_loss=0.1298, NSP_Accuracy=0.5056, NSP_loss=0.6995, Total_loss=0.8293]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1320, MLM_loss=0.1279, NSP_Accuracy=0.4939, NSP_loss=0.6797, Total_loss=0.8076]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1404, MLM_loss=0.1040, NSP_Accuracy=0.4782, NSP_loss=0.6763, Total_loss=0.7803]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1553, MLM_loss=0.1003, NSP_Accuracy=0.5160, NSP_loss=0.6632, Total_loss=0.7634]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1741, MLM_loss=0.0566, NSP_Accuracy=0.5124, NSP_loss=0.6773, Total_loss=0.7339]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1897, MLM_loss=0.1029, NSP_Accuracy=0.5038, NSP_loss=0.6920, Total_loss=0.7949]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1976, MLM_loss=0.0625, NSP_Accuracy=0.5438, NSP_loss=0.8205, Total_loss=0.8830]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1971, MLM_loss=0.0895, NSP_Accuracy=0.7038, NSP_loss=0.5122, Total_loss=0.6017]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_28280\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask12_input = getData(modelType = modelType, datapath = datapath, maskPercent = 12, nspTask = \"NSP\")\n",
    "mask12 = trainModel(modelType = modelType, inputs = mask12_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 12, saveModelName = \"saved_model_mask12\")\n",
    "mask12_input = None\n",
    "mask12 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_28280\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.0424, MLM_loss=0.3882, NSP_Accuracy=0.4751, NSP_loss=0.6940, Total_loss=1.0822] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1107, MLM_loss=0.1972, NSP_Accuracy=0.5070, NSP_loss=0.7604, Total_loss=0.9576]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1292, MLM_loss=0.1308, NSP_Accuracy=0.4804, NSP_loss=0.7472, Total_loss=0.8780]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1382, MLM_loss=0.0916, NSP_Accuracy=0.4836, NSP_loss=0.7664, Total_loss=0.8580]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1455, MLM_loss=0.0942, NSP_Accuracy=0.5097, NSP_loss=0.6376, Total_loss=0.7317]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1599, MLM_loss=0.0610, NSP_Accuracy=0.4975, NSP_loss=0.7562, Total_loss=0.8173]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1768, MLM_loss=0.0815, NSP_Accuracy=0.4939, NSP_loss=0.7969, Total_loss=0.8785]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1885, MLM_loss=0.0706, NSP_Accuracy=0.5501, NSP_loss=0.7139, Total_loss=0.7845]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1896, MLM_loss=0.0981, NSP_Accuracy=0.7416, NSP_loss=0.4431, Total_loss=0.5411]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1893, MLM_loss=0.0672, NSP_Accuracy=0.9308, NSP_loss=0.0283, Total_loss=0.0955]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_28280\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask11_input = getData(modelType = modelType, datapath = datapath, maskPercent = 11, nspTask = \"NSP\")\n",
    "mask11 = trainModel(modelType = modelType, inputs = mask11_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 11, saveModelName = \"saved_model_mask11\")\n",
    "mask11_input = None\n",
    "mask11 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11152\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.0404, MLM_loss=0.3617, NSP_Accuracy=0.4903, NSP_loss=0.6868, Total_loss=1.0485] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1087, MLM_loss=0.1961, NSP_Accuracy=0.5065, NSP_loss=0.6216, Total_loss=0.8177]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1245, MLM_loss=0.1164, NSP_Accuracy=0.5052, NSP_loss=0.7912, Total_loss=0.9075]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.1362, MLM_loss=0.1081, NSP_Accuracy=0.4885, NSP_loss=0.6725, Total_loss=0.7806]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.1421, MLM_loss=0.0765, NSP_Accuracy=0.5124, NSP_loss=0.6734, Total_loss=0.7500]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1533, MLM_loss=0.0568, NSP_Accuracy=0.5196, NSP_loss=0.6938, Total_loss=0.7506]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1643, MLM_loss=0.1071, NSP_Accuracy=0.5254, NSP_loss=0.6826, Total_loss=0.7897]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1750, MLM_loss=0.1117, NSP_Accuracy=0.6022, NSP_loss=0.6320, Total_loss=0.7437]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1720, MLM_loss=0.0860, NSP_Accuracy=0.8454, NSP_loss=0.5745, Total_loss=0.6605]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1772, MLM_loss=0.0570, NSP_Accuracy=0.9672, NSP_loss=0.0085, Total_loss=0.0655]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11152\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask10_input = getData(modelType = modelType, datapath = datapath, maskPercent = 10, nspTask = \"NSP\")\n",
    "mask10 = trainModel(modelType = modelType, inputs = mask10_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 10, saveModelName = \"saved_model_mask10\")\n",
    "mask10_input = None\n",
    "mask10 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11152\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:10<00:00,  2.85it/s, MLM_Accuracy=0.0320, MLM_loss=0.2471, NSP_Accuracy=0.4971, NSP_loss=0.5328, Total_loss=0.7800] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1080, MLM_loss=0.2016, NSP_Accuracy=0.4993, NSP_loss=0.6857, Total_loss=0.8873]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:10<00:00,  2.85it/s, MLM_Accuracy=0.1282, MLM_loss=0.1119, NSP_Accuracy=0.4899, NSP_loss=0.5396, Total_loss=0.6515]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1348, MLM_loss=0.0958, NSP_Accuracy=0.5016, NSP_loss=0.7372, Total_loss=0.8330]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1405, MLM_loss=0.0536, NSP_Accuracy=0.4876, NSP_loss=0.7597, Total_loss=0.8133]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:10<00:00,  2.85it/s, MLM_Accuracy=0.1545, MLM_loss=0.0770, NSP_Accuracy=0.5070, NSP_loss=0.8027, Total_loss=0.8797]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1733, MLM_loss=0.0582, NSP_Accuracy=0.5290, NSP_loss=0.7863, Total_loss=0.8446]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1845, MLM_loss=0.0791, NSP_Accuracy=0.6355, NSP_loss=0.7014, Total_loss=0.7805]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1837, MLM_loss=0.0686, NSP_Accuracy=0.8593, NSP_loss=0.7211, Total_loss=0.7896]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.1813, MLM_loss=0.0675, NSP_Accuracy=0.9654, NSP_loss=1.0098, Total_loss=1.0774]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11152\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask9_input = getData(modelType = modelType, datapath = datapath, maskPercent = 9, nspTask = \"NSP\")\n",
    "mask9 = trainModel(modelType = modelType, inputs = mask9_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 9, saveModelName = \"saved_model_mask9\")\n",
    "mask9_input = None\n",
    "mask9 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11152\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.0244, MLM_loss=0.3479, NSP_Accuracy=0.4894, NSP_loss=0.6826, Total_loss=1.0305] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.0989, MLM_loss=0.1504, NSP_Accuracy=0.5020, NSP_loss=0.7325, Total_loss=0.8829]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.1129, MLM_loss=0.1196, NSP_Accuracy=0.5070, NSP_loss=0.6069, Total_loss=0.7266]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1232, MLM_loss=0.0657, NSP_Accuracy=0.5056, NSP_loss=0.7611, Total_loss=0.8268]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1348, MLM_loss=0.0615, NSP_Accuracy=0.5249, NSP_loss=0.7132, Total_loss=0.7747]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1447, MLM_loss=0.0558, NSP_Accuracy=0.5106, NSP_loss=0.6649, Total_loss=0.7207]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1601, MLM_loss=0.0625, NSP_Accuracy=0.5447, NSP_loss=0.6914, Total_loss=0.7539]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1735, MLM_loss=0.1062, NSP_Accuracy=0.6378, NSP_loss=0.5089, Total_loss=0.6151]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1694, MLM_loss=0.0744, NSP_Accuracy=0.8409, NSP_loss=0.7082, Total_loss=0.7826]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1749, MLM_loss=0.0692, NSP_Accuracy=0.9537, NSP_loss=0.0731, Total_loss=0.1423]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11152\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask8_input = getData(modelType = modelType, datapath = datapath, maskPercent = 8, nspTask = \"NSP\")\n",
    "mask8 = trainModel(modelType = modelType, inputs = mask8_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 8, saveModelName = \"saved_model_mask8\")\n",
    "mask8_input = None\n",
    "mask8 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_17896\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.0258, MLM_loss=0.4245, NSP_Accuracy=0.4935, NSP_loss=0.6992, Total_loss=1.1236] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.1028, MLM_loss=0.2688, NSP_Accuracy=0.4948, NSP_loss=0.6800, Total_loss=0.9488]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.1175, MLM_loss=0.0716, NSP_Accuracy=0.5079, NSP_loss=0.6945, Total_loss=0.7661]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.1264, MLM_loss=0.0791, NSP_Accuracy=0.5052, NSP_loss=0.8204, Total_loss=0.8996]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1365, MLM_loss=0.0302, NSP_Accuracy=0.5133, NSP_loss=0.6773, Total_loss=0.7075]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1464, MLM_loss=0.0595, NSP_Accuracy=0.5106, NSP_loss=0.5985, Total_loss=0.6581]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.1659, MLM_loss=0.0498, NSP_Accuracy=0.5128, NSP_loss=0.7155, Total_loss=0.7653]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1774, MLM_loss=0.0546, NSP_Accuracy=0.5470, NSP_loss=0.6849, Total_loss=0.7396]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1792, MLM_loss=0.0963, NSP_Accuracy=0.7308, NSP_loss=0.5019, Total_loss=0.5982]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1820, MLM_loss=0.0804, NSP_Accuracy=0.9151, NSP_loss=0.8798, Total_loss=0.9603]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_17896\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask7_input = getData(modelType = modelType, datapath = datapath, maskPercent = 7, nspTask = \"NSP\")\n",
    "mask7 = trainModel(modelType = modelType, inputs = mask7_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 7, saveModelName = \"saved_model_mask7\")\n",
    "mask7_input = None\n",
    "mask7 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_17896\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:10<00:00,  2.85it/s, MLM_Accuracy=0.0435, MLM_loss=0.3712, NSP_Accuracy=0.5115, NSP_loss=0.6751, Total_loss=1.0464] \n",
      "Epoch 1: 100%|██████████| 371/371 [02:10<00:00,  2.83it/s, MLM_Accuracy=0.1177, MLM_loss=0.1754, NSP_Accuracy=0.5061, NSP_loss=0.6727, Total_loss=0.8482]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1272, MLM_loss=0.0562, NSP_Accuracy=0.4858, NSP_loss=0.6446, Total_loss=0.7007]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1364, MLM_loss=0.1512, NSP_Accuracy=0.4930, NSP_loss=0.6876, Total_loss=0.8389]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.1451, MLM_loss=0.0433, NSP_Accuracy=0.5115, NSP_loss=0.7182, Total_loss=0.7615]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1551, MLM_loss=0.0905, NSP_Accuracy=0.5083, NSP_loss=0.6709, Total_loss=0.7615]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1703, MLM_loss=0.0659, NSP_Accuracy=0.5420, NSP_loss=0.7184, Total_loss=0.7843]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1730, MLM_loss=0.0573, NSP_Accuracy=0.7119, NSP_loss=0.5324, Total_loss=0.5898]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1778, MLM_loss=0.0642, NSP_Accuracy=0.8858, NSP_loss=0.3452, Total_loss=0.4094]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.1787, MLM_loss=0.0561, NSP_Accuracy=0.9663, NSP_loss=0.0111, Total_loss=0.0672]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_17896\\1527690133.py:89: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask6_input = getData(modelType = modelType, datapath = datapath, maskPercent = 6, nspTask = \"NSP\")\n",
    "mask6 = trainModel(modelType = modelType, inputs = mask6_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 6, saveModelName = \"saved_model_mask6\")\n",
    "mask6_input = None\n",
    "mask6 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of SymInts, but found element of type NoneType at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 22\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mask15_input_sop \u001b[39m=\u001b[39m getData(modelType \u001b[39m=\u001b[39m modelType, datapath \u001b[39m=\u001b[39m datapath, maskPercent \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m, nspTask \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSOP\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mask15_sop \u001b[39m=\u001b[39m trainModel(modelType \u001b[39m=\u001b[39;49m modelType, inputs \u001b[39m=\u001b[39;49m mask15_input_sop\u001b[39m.\u001b[39;49mreturnInput(), batch_size \u001b[39m=\u001b[39;49m \u001b[39m6\u001b[39;49m, epoch \u001b[39m=\u001b[39;49m epoch, maskPercent \u001b[39m=\u001b[39;49m \u001b[39m15\u001b[39;49m, saveModelName \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msaved_model_mask15_sop\u001b[39;49m\u001b[39m\"\u001b[39;49m, saveCSV \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, nspTask \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mSOP\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 22\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, modelType, inputs, batch_size, epoch, maskPercent, saveModelName, saveCSV \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, nspTask \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNSP\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     config \u001b[39m=\u001b[39m BertConfig\u001b[39m.\u001b[39mfrom_pretrained(modelType)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m MyBertForPreTraining(config, nspTask\u001b[39m=\u001b[39;49mnspTask)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(modelType)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m inputs\n",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 22\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m nspTask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSOP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls \u001b[39m=\u001b[39m BertPretrainingHeadsWithSOP(config)\n",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 22\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_relationship \u001b[39m=\u001b[39m MyAlbertSOPHead(config)\n",
      "\u001b[1;32md:\\NCHU\\Group Meeting\\我啦\\畢業論文相關\\BERT_Practice\\實做\\論文\\BERT_Pretrain copy.ipynb Cell 22\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()  \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mhidden_dropout_prob)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NCHU/Group%20Meeting/%E6%88%91%E5%95%A6/%E7%95%A2%E6%A5%AD%E8%AB%96%E6%96%87%E7%9B%B8%E9%97%9C/BERT_Practice/%E5%AF%A6%E5%81%9A/%E8%AB%96%E6%96%87/BERT_Pretrain%20copy.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mclassifier_dropout , config\u001b[39m.\u001b[39;49mnum_labels)\n",
      "File \u001b[1;32mc:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of SymInts, but found element of type NoneType at pos 2"
     ]
    }
   ],
   "source": [
    "mask15_input_sop = getData(modelType = modelType, datapath = datapath, maskPercent = 15, nspTask = \"SOP\")\n",
    "mask15_sop = trainModel(modelType = modelType, inputs = mask15_input_sop.returnInput(), batch_size = 6, epoch = epoch, maskPercent = 15, saveModelName = \"saved_model_mask15_sop\", saveCSV = False, nspTask = \"SOP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT_Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684b83e2f83316061361748e41b2620a10a3e9a8f2545480c20c18cf426689ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
