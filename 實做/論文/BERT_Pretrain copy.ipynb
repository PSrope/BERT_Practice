{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定 Tokenizer 與使用 BertForPretraining 來做 BERT 預訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining, AdamW\n",
    "from transformers.models.bert.modeling_bert import BertForPreTrainingOutput, BertPreTrainingHeads, BertConfig, BERT_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "from transformers.models.albert.modeling_albert import AlbertSOPHead\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTrainingOutput(BertForPreTrainingOutput):\n",
    "    def __init__(self, loss=None, prediction_logits=None, seq_relationship_logits=None, hidden_states=None, attentions=None, mlm_loss=None, nsp_loss=None):\n",
    "        super().__init__(loss=loss, prediction_logits=prediction_logits, seq_relationship_logits=seq_relationship_logits, hidden_states=hidden_states, attentions=attentions)\n",
    "        self.mlm_loss = mlm_loss\n",
    "        self.nsp_loss = nsp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlbertSOPHead(torch.nn.Module):\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n",
    "        dropout_pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPretrainingHeadsWithSOP(BertPreTrainingHeads):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.seq_relationship = MyAlbertSOPHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTraining(BertForPreTraining):\n",
    "    def __init__(self, config, nspTask = \"NSP\"):\n",
    "        super().__init__(config)\n",
    "        if nspTask == \"SOP\":\n",
    "            self.cls = BertPretrainingHeadsWithSOP(config)\n",
    "            \n",
    "    # @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    # @replace_return_docstrings(output_type=MyBertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MyBertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return MyBertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            mlm_loss = masked_lm_loss,\n",
    "            nsp_loss = next_sentence_loss,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取出資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getData():\n",
    "    def __init__(self, modelType, datapath, maskPercent, nspTask = \"NSP\"):\n",
    "        self.datapath = datapath\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.maskPercent = maskPercent\n",
    "        self.nspTask = nspTask\n",
    "        self.text = self.toText()\n",
    "        self.inputs = None\n",
    "        self.nspPrepare()\n",
    "        self.mlmPrepare()\n",
    "    \n",
    "    def toText(self):\n",
    "        df = pd.read_csv(self.datapath)\n",
    "        text = []\n",
    "        for review in df[\"text\"]:\n",
    "            text.append(review)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def nspPrepare(self):\n",
    "        bag = [item for sentence in self.text for item in sentence.split('.') if item != '']\n",
    "        bag_size = len(bag)\n",
    "\n",
    "        if self.nspTask == \"NSP\":\n",
    "            (sentence_a, sentence_b, label) = self.nspData(bag, bag_size)\n",
    "        elif self.nspTask == \"SOP\":\n",
    "            (sentence_a, sentence_b, label) = self.sopData()\n",
    "\n",
    "        self.inputs = self.tokenizer(sentence_a, sentence_b, return_tensors='pt',\n",
    "                   max_length=512, truncation=True, padding='max_length')\n",
    "        self.inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "    \n",
    "    def nspData(self, bag, bag_size):\n",
    "        sentence_a = []\n",
    "        sentence_b = []\n",
    "        label = []\n",
    "        for paragraph in self.text:\n",
    "            sentences = [\n",
    "                sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "            ]\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    sentence_a.append(sentences[start])\n",
    "                    sentence_b.append(sentences[start+1])\n",
    "                    label.append(0)\n",
    "                else:\n",
    "                    index = random.randint(0, bag_size-1)\n",
    "                    # this is NotNextSentence\n",
    "                    sentence_a.append(sentences[start])\n",
    "                    sentence_b.append(bag[index])\n",
    "                    label.append(1)\n",
    "        \n",
    "        return (sentence_a, sentence_b, label)\n",
    "    \n",
    "    def sopData(self):\n",
    "        sentence_a = []\n",
    "        sentence_b = []\n",
    "        label = []\n",
    "        for paragraph in self.text:\n",
    "            sentences = [\n",
    "                sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "            ]\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    sentence_a.append(sentences[start])\n",
    "                    sentence_b.append(sentences[start+1])\n",
    "                    label.append(0)\n",
    "                else:\n",
    "                    # this is NotNextSentence\n",
    "                    sentence_a.append(sentences[start+1])\n",
    "                    sentence_b.append(sentences[start])\n",
    "                    label.append(1)\n",
    "        \n",
    "        return (sentence_a, sentence_b, label)\n",
    "\n",
    "    def mlmPrepare(self):\n",
    "        self.inputs['labels'] = self.inputs.input_ids.detach().clone()\n",
    "        rand = torch.rand(self.inputs.input_ids.shape)\n",
    "        # create mask array\n",
    "        mask_arr = (rand < self.maskPercent * 0.01) * (self.inputs.input_ids != 101) * \\\n",
    "                (self.inputs.input_ids != 102) * (self.inputs.input_ids != 0)\n",
    "        self.inputs['mask_arr'] = mask_arr\n",
    "        \n",
    "        selection = []\n",
    "\n",
    "        for i in range(self.inputs.input_ids.shape[0]):\n",
    "            selection.append(\n",
    "                torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            )\n",
    "\n",
    "        rand_mask_type = copy.deepcopy(selection)\n",
    "\n",
    "        for row in range(len(rand_mask_type)):\n",
    "            for col in range(len(rand_mask_type[row])):\n",
    "                rand_mask_type[row][col] = random.random()\n",
    "\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        special_tokens = [vocab['[CLS]'], vocab['[SEP]'], vocab['[MASK]'], vocab['[UNK]'],  vocab['[PAD]']]\n",
    "\n",
    "        for i in range(self.inputs.input_ids.shape[0]):\n",
    "            for j in range(len(selection[i])):\n",
    "                if rand_mask_type[i][j] < 0.10:\n",
    "                    continue\n",
    "                elif rand_mask_type[i][j] < 0.20:\n",
    "                    rand_num = vocab['[CLS]']\n",
    "                    while rand_num in special_tokens:\n",
    "                        rand_num = random.randint(1, vocab_size)\n",
    "                    self.inputs.input_ids[i, selection[i][j]] = rand_num\n",
    "                else:\n",
    "                    self.inputs.input_ids[i, selection[i][j]] = 103\n",
    "    \n",
    "    def returnInput(self):\n",
    "        return self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel():\n",
    "    def __init__(self, modelType, inputs, batch_size, epoch, maskPercent, saveModelName, saveCSV = True, nspTask = \"NSP\"):\n",
    "        config = BertConfig.from_pretrained(modelType)\n",
    "        self.model = MyBertForPreTraining(config, nspTask=nspTask)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.maskPercent = maskPercent\n",
    "        self.saveModelName = saveModelName\n",
    "        self.saveCSV = saveCSV\n",
    "        self.loader = torch.utils.data.DataLoader(OurDataset(self.inputs), \\\n",
    "                                             batch_size=self.batch_size, shuffle=True)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        self.optim = AdamW(self.model.parameters(), lr = 5e-5)\n",
    "        self.record = {\"mask_percent\": None,\n",
    "                  \"mlm_acc_each_epoch\": [], \n",
    "                  \"mlm_loss_each_epoch\": []}\n",
    "        \n",
    "        if os.path.isfile(\"record.csv\"):\n",
    "            self.rec = pd.read_csv(\"record.csv\")\n",
    "        else:\n",
    "            self.rec = pd.DataFrame()\n",
    "            \n",
    "        self.training()\n",
    "        self.save_model(self.saveModelName)\n",
    "    \n",
    "    def training(self):\n",
    "        acc_each_epoch = []\n",
    "        loss_each_epoch = []\n",
    "        for epoch in range(self.epoch):\n",
    "            # setup loop with TQDM and dataloader\n",
    "            mask_nums = 0\n",
    "            mlm_correct = 0\n",
    "            nsp_nums = 0\n",
    "            nsp_correct = 0\n",
    "            loop = tqdm(self.loader, leave=True)\n",
    "            for batch in loop:\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                self.optim.zero_grad()\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                next_sentence_label = batch['next_sentence_label'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                mask_arr = batch['mask_arr'].to(self.device)\n",
    "                # process\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                next_sentence_label=next_sentence_label,\n",
    "                                labels=labels)\n",
    "                \n",
    "                prediction_logits = outputs.prediction_logits[mask_arr]\n",
    "                predicted_ids = prediction_logits.argmax(-1)\n",
    "                \n",
    "                seq_relationship_logits = outputs.seq_relationship_logits\n",
    "                predicted_labels = torch.argmax(seq_relationship_logits, dim=1)\n",
    "                predicted_label = predicted_labels\n",
    "\n",
    "                mask_nums += len(predicted_ids)\n",
    "                mlm_correct += torch.eq(predicted_ids, labels[mask_arr]).sum().item()\n",
    "                nsp_nums += len(predicted_label)\n",
    "                nsp_correct += predicted_label.eq(torch.squeeze(next_sentence_label)).sum().item()\n",
    "                \n",
    "                # extract loss\n",
    "                loss = outputs.loss\n",
    "                mlm_loss = outputs.mlm_loss.item()\n",
    "                nsp_loss = outputs.nsp_loss.item()\n",
    "                mlm_acc = mlm_correct / mask_nums\n",
    "                nsp_acc = nsp_correct / nsp_nums\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                self.optim.step()\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(Total_loss='{:.4f}'.format(loss.item()), MLM_Accuracy='{:.4f}'.format(mlm_acc), NSP_Accuracy='{:.4f}'.format(nsp_acc), \\\n",
    "                                MLM_loss='{:.4f}'.format(mlm_loss), NSP_loss='{:.4f}'.format(nsp_loss))\n",
    "            acc_each_epoch.append(mlm_acc)\n",
    "            loss_each_epoch.append(mlm_loss)\n",
    "\n",
    "        if self.saveCSV:\n",
    "            self.record[\"mask_percent\"] = self.maskPercent\n",
    "            self.record[\"mlm_acc_each_epoch\"].append(acc_each_epoch)\n",
    "            self.record[\"mlm_loss_each_epoch\"].append(loss_each_epoch)\n",
    "            new_rec = self.rec.append(self.record, ignore_index=True)\n",
    "            new_rec.to_csv(\"record.csv\", index = None)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def save_model(self, maskPercent):\n",
    "        self.model.save_pretrained(maskPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'bbc-text.csv'\n",
    "modelType = 'bert-base-cased'\n",
    "epoch = 10\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_100\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:16<00:00,  2.71it/s, MLM_Accuracy=0.3318, MLM_loss=0.0743, NSP_Accuracy=0.8139, NSP_loss=0.6796, Total_loss=0.7539]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.4108, MLM_loss=0.0440, NSP_Accuracy=0.9533, NSP_loss=0.1754, Total_loss=0.2194]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4296, MLM_loss=0.0460, NSP_Accuracy=0.9789, NSP_loss=0.0578, Total_loss=0.1038]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4410, MLM_loss=0.0647, NSP_Accuracy=0.9784, NSP_loss=0.0059, Total_loss=0.0707]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4522, MLM_loss=0.0593, NSP_Accuracy=0.9811, NSP_loss=0.0474, Total_loss=0.1067]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4670, MLM_loss=0.0702, NSP_Accuracy=0.9901, NSP_loss=0.0143, Total_loss=0.0845]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4958, MLM_loss=0.0375, NSP_Accuracy=0.9924, NSP_loss=0.0025, Total_loss=0.0400]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5062, MLM_loss=0.0471, NSP_Accuracy=0.9874, NSP_loss=0.1254, Total_loss=0.1725]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.5226, MLM_loss=0.0240, NSP_Accuracy=0.9847, NSP_loss=0.0050, Total_loss=0.0290]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5071, MLM_loss=0.0521, NSP_Accuracy=0.9798, NSP_loss=0.0018, Total_loss=0.0539]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_100\\1117452963.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask15_input = getData(modelType = modelType, datapath = datapath, maskPercent = 15, nspTask = \"NSP\")\n",
    "mask15 = trainModel(modelType = modelType, inputs = mask15_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 15, saveModelName = \"saved_model_mask15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_20492\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.3329, MLM_loss=0.0776, NSP_Accuracy=0.8099, NSP_loss=0.3889, Total_loss=0.4665]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4133, MLM_loss=0.0827, NSP_Accuracy=0.9425, NSP_loss=0.4372, Total_loss=0.5199]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4220, MLM_loss=0.0536, NSP_Accuracy=0.9658, NSP_loss=0.0066, Total_loss=0.0602]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4455, MLM_loss=0.0467, NSP_Accuracy=0.9838, NSP_loss=0.0488, Total_loss=0.0954]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4581, MLM_loss=0.0282, NSP_Accuracy=0.9901, NSP_loss=0.0002, Total_loss=0.0284]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4802, MLM_loss=0.0246, NSP_Accuracy=0.9870, NSP_loss=0.0001, Total_loss=0.0246]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4920, MLM_loss=0.0199, NSP_Accuracy=0.9834, NSP_loss=0.0001, Total_loss=0.0200]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5086, MLM_loss=0.0263, NSP_Accuracy=0.9933, NSP_loss=0.0046, Total_loss=0.0309]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5088, MLM_loss=0.0415, NSP_Accuracy=0.9398, NSP_loss=0.0002, Total_loss=0.0417]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5576, MLM_loss=0.0197, NSP_Accuracy=0.9978, NSP_loss=0.0001, Total_loss=0.0198]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_20492\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask14_input = getData(modelType = modelType, datapath = datapath, maskPercent = 14, nspTask = \"NSP\")\n",
    "mask14 = trainModel(modelType = modelType, inputs = mask14_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 14, saveModelName = \"saved_model_mask14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_31472\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:16<00:00,  2.71it/s, MLM_Accuracy=0.3197, MLM_loss=0.0854, NSP_Accuracy=0.8126, NSP_loss=0.6025, Total_loss=0.6880]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.4084, MLM_loss=0.0702, NSP_Accuracy=0.9353, NSP_loss=0.0860, Total_loss=0.1562]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:10<00:00,  2.85it/s, MLM_Accuracy=0.4323, MLM_loss=0.0485, NSP_Accuracy=0.9735, NSP_loss=0.0014, Total_loss=0.0499]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.4466, MLM_loss=0.0528, NSP_Accuracy=0.9771, NSP_loss=0.0756, Total_loss=0.1284]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.90it/s, MLM_Accuracy=0.4596, MLM_loss=0.0198, NSP_Accuracy=0.9798, NSP_loss=0.0008, Total_loss=0.0206]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4750, MLM_loss=0.0303, NSP_Accuracy=0.9901, NSP_loss=0.0087, Total_loss=0.0390]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4916, MLM_loss=0.0487, NSP_Accuracy=0.9888, NSP_loss=0.0005, Total_loss=0.0492]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4898, MLM_loss=0.0423, NSP_Accuracy=0.9829, NSP_loss=0.0016, Total_loss=0.0440]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5126, MLM_loss=0.0458, NSP_Accuracy=0.9901, NSP_loss=0.0055, Total_loss=0.0513]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5453, MLM_loss=0.0361, NSP_Accuracy=0.9942, NSP_loss=0.0002, Total_loss=0.0364]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_31472\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask13_input = getData(modelType = modelType, datapath = datapath, maskPercent = 13, nspTask = \"NSP\")\n",
    "mask13 = trainModel(modelType = modelType, inputs = mask13_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 13, saveModelName = \"saved_model_mask13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_10496\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.3216, MLM_loss=0.0882, NSP_Accuracy=0.8072, NSP_loss=0.1692, Total_loss=0.2575]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:07<00:00,  2.90it/s, MLM_Accuracy=0.4110, MLM_loss=0.0443, NSP_Accuracy=0.9492, NSP_loss=0.0191, Total_loss=0.0634]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.90it/s, MLM_Accuracy=0.4280, MLM_loss=0.0596, NSP_Accuracy=0.9699, NSP_loss=0.0339, Total_loss=0.0935]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.90it/s, MLM_Accuracy=0.4400, MLM_loss=0.0495, NSP_Accuracy=0.9825, NSP_loss=0.0005, Total_loss=0.0500]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.90it/s, MLM_Accuracy=0.4508, MLM_loss=0.0576, NSP_Accuracy=0.9780, NSP_loss=0.0019, Total_loss=0.0595]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4619, MLM_loss=0.0393, NSP_Accuracy=0.9757, NSP_loss=0.0744, Total_loss=0.1137]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4933, MLM_loss=0.0412, NSP_Accuracy=0.9946, NSP_loss=0.0001, Total_loss=0.0413]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.90it/s, MLM_Accuracy=0.5082, MLM_loss=0.0277, NSP_Accuracy=0.9969, NSP_loss=0.0001, Total_loss=0.0277]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5057, MLM_loss=0.0314, NSP_Accuracy=0.9816, NSP_loss=0.2091, Total_loss=0.2405]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5341, MLM_loss=0.0204, NSP_Accuracy=0.9906, NSP_loss=0.0005, Total_loss=0.0209]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_10496\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask12_input = getData(modelType = modelType, datapath = datapath, maskPercent = 12, nspTask = \"NSP\")\n",
    "mask12 = trainModel(modelType = modelType, inputs = mask12_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 12, saveModelName = \"saved_model_mask12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_29776\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.3226, MLM_loss=0.0737, NSP_Accuracy=0.8247, NSP_loss=0.1876, Total_loss=0.2613]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.90it/s, MLM_Accuracy=0.4023, MLM_loss=0.0500, NSP_Accuracy=0.9479, NSP_loss=0.0057, Total_loss=0.0557]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4339, MLM_loss=0.0526, NSP_Accuracy=0.9780, NSP_loss=0.6309, Total_loss=0.6835]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4534, MLM_loss=0.0378, NSP_Accuracy=0.9825, NSP_loss=0.0002, Total_loss=0.0380]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.4657, MLM_loss=0.0393, NSP_Accuracy=0.9739, NSP_loss=0.0054, Total_loss=0.0447]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4668, MLM_loss=0.0320, NSP_Accuracy=0.9847, NSP_loss=0.0002, Total_loss=0.0322]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5052, MLM_loss=0.0309, NSP_Accuracy=0.9982, NSP_loss=0.0025, Total_loss=0.0334]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5512, MLM_loss=0.0230, NSP_Accuracy=0.9996, NSP_loss=0.0001, Total_loss=0.0230]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.6006, MLM_loss=0.0304, NSP_Accuracy=1.0000, NSP_loss=0.0000, Total_loss=0.0305]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.6438, MLM_loss=0.0341, NSP_Accuracy=1.0000, NSP_loss=0.0000, Total_loss=0.0342]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_29776\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask11_input = getData(modelType = modelType, datapath = datapath, maskPercent = 11, nspTask = \"NSP\")\n",
    "mask11 = trainModel(modelType = modelType, inputs = mask11_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 11, saveModelName = \"saved_model_mask11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_22468\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:11<00:00,  2.83it/s, MLM_Accuracy=0.3212, MLM_loss=0.0982, NSP_Accuracy=0.8297, NSP_loss=0.8100, Total_loss=0.9081]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:09<00:00,  2.88it/s, MLM_Accuracy=0.4010, MLM_loss=0.0784, NSP_Accuracy=0.9483, NSP_loss=0.0619, Total_loss=0.1403]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.4237, MLM_loss=0.0333, NSP_Accuracy=0.9784, NSP_loss=0.0005, Total_loss=0.0338]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:10<00:00,  2.85it/s, MLM_Accuracy=0.4483, MLM_loss=0.0355, NSP_Accuracy=0.9838, NSP_loss=0.0039, Total_loss=0.0393]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:10<00:00,  2.84it/s, MLM_Accuracy=0.4675, MLM_loss=0.0295, NSP_Accuracy=0.9879, NSP_loss=0.0000, Total_loss=0.0295]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:10<00:00,  2.85it/s, MLM_Accuracy=0.4797, MLM_loss=0.0271, NSP_Accuracy=0.9910, NSP_loss=0.1086, Total_loss=0.1357]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4909, MLM_loss=0.0435, NSP_Accuracy=0.9820, NSP_loss=0.3211, Total_loss=0.3646]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5084, MLM_loss=0.0191, NSP_Accuracy=0.9915, NSP_loss=0.6993, Total_loss=0.7183]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5465, MLM_loss=0.0456, NSP_Accuracy=0.9955, NSP_loss=0.0001, Total_loss=0.0457]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5266, MLM_loss=0.0289, NSP_Accuracy=0.9775, NSP_loss=1.8708, Total_loss=1.8997]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_22468\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask10_input = getData(modelType = modelType, datapath = datapath, maskPercent = 10, nspTask = \"NSP\")\n",
    "mask10 = trainModel(modelType = modelType, inputs = mask10_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 10, saveModelName = \"saved_model_mask10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11228\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.3210, MLM_loss=0.0480, NSP_Accuracy=0.8279, NSP_loss=0.2482, Total_loss=0.2962]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4185, MLM_loss=0.0309, NSP_Accuracy=0.9604, NSP_loss=0.4728, Total_loss=0.5037]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4333, MLM_loss=0.0263, NSP_Accuracy=0.9699, NSP_loss=0.0066, Total_loss=0.0329]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.4660, MLM_loss=0.0326, NSP_Accuracy=0.9825, NSP_loss=0.0615, Total_loss=0.0941]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4810, MLM_loss=0.0291, NSP_Accuracy=0.9816, NSP_loss=0.0421, Total_loss=0.0712]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.4795, MLM_loss=0.0262, NSP_Accuracy=0.9843, NSP_loss=0.0007, Total_loss=0.0268]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.5055, MLM_loss=0.0287, NSP_Accuracy=0.9901, NSP_loss=0.0050, Total_loss=0.0337]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.5194, MLM_loss=0.0253, NSP_Accuracy=0.9852, NSP_loss=0.0018, Total_loss=0.0272]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5441, MLM_loss=0.0243, NSP_Accuracy=0.9964, NSP_loss=0.0001, Total_loss=0.0244]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5636, MLM_loss=0.0187, NSP_Accuracy=0.9883, NSP_loss=0.0020, Total_loss=0.0207]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11228\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask9_input = getData(modelType = modelType, datapath = datapath, maskPercent = 9, nspTask = \"NSP\")\n",
    "mask9 = trainModel(modelType = modelType, inputs = mask9_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 9, saveModelName = \"saved_model_mask9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11536\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.3547, MLM_loss=0.0497, NSP_Accuracy=0.8094, NSP_loss=0.4703, Total_loss=0.5200]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4216, MLM_loss=0.0343, NSP_Accuracy=0.9452, NSP_loss=0.3653, Total_loss=0.3996]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4466, MLM_loss=0.0335, NSP_Accuracy=0.9721, NSP_loss=0.0023, Total_loss=0.0357]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4604, MLM_loss=0.0224, NSP_Accuracy=0.9834, NSP_loss=0.0002, Total_loss=0.0225]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:09<00:00,  2.88it/s, MLM_Accuracy=0.4758, MLM_loss=0.0159, NSP_Accuracy=0.9793, NSP_loss=0.0667, Total_loss=0.0826]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5125, MLM_loss=0.0190, NSP_Accuracy=0.9937, NSP_loss=0.0224, Total_loss=0.0414]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.5126, MLM_loss=0.0368, NSP_Accuracy=0.9816, NSP_loss=0.0032, Total_loss=0.0400]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.5155, MLM_loss=0.0191, NSP_Accuracy=0.9825, NSP_loss=0.3438, Total_loss=0.3629]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5293, MLM_loss=0.0433, NSP_Accuracy=0.9852, NSP_loss=0.0298, Total_loss=0.0731]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5541, MLM_loss=0.0164, NSP_Accuracy=0.9901, NSP_loss=0.0003, Total_loss=0.0167]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_11536\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask8_input = getData(modelType = modelType, datapath = datapath, maskPercent = 8, nspTask = \"NSP\")\n",
    "mask8 = trainModel(modelType = modelType, inputs = mask8_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 8, saveModelName = \"saved_model_mask8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_26728\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.3451, MLM_loss=0.0531, NSP_Accuracy=0.8364, NSP_loss=0.0268, Total_loss=0.0799]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.90it/s, MLM_Accuracy=0.4270, MLM_loss=0.0287, NSP_Accuracy=0.9542, NSP_loss=0.0296, Total_loss=0.0583]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4475, MLM_loss=0.0385, NSP_Accuracy=0.9753, NSP_loss=0.0152, Total_loss=0.0537]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4616, MLM_loss=0.0320, NSP_Accuracy=0.9802, NSP_loss=0.0001, Total_loss=0.0322]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4762, MLM_loss=0.0301, NSP_Accuracy=0.9775, NSP_loss=0.0011, Total_loss=0.0312]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4923, MLM_loss=0.0321, NSP_Accuracy=0.9897, NSP_loss=0.0004, Total_loss=0.0325]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.5155, MLM_loss=0.0264, NSP_Accuracy=0.9861, NSP_loss=0.0086, Total_loss=0.0350]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.5294, MLM_loss=0.0218, NSP_Accuracy=0.9843, NSP_loss=0.0419, Total_loss=0.0637]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.5571, MLM_loss=0.0046, NSP_Accuracy=0.9856, NSP_loss=0.5140, Total_loss=0.5186]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:09<00:00,  2.87it/s, MLM_Accuracy=0.5685, MLM_loss=0.0225, NSP_Accuracy=0.9915, NSP_loss=0.0013, Total_loss=0.0238]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_26728\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask7_input = getData(modelType = modelType, datapath = datapath, maskPercent = 7, nspTask = \"NSP\")\n",
    "mask7 = trainModel(modelType = modelType, inputs = mask7_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 7, saveModelName = \"saved_model_mask7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_27844\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:11<00:00,  2.82it/s, MLM_Accuracy=0.3487, MLM_loss=0.0631, NSP_Accuracy=0.8342, NSP_loss=0.2617, Total_loss=0.3248]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.4211, MLM_loss=0.0198, NSP_Accuracy=0.9519, NSP_loss=0.2661, Total_loss=0.2859]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4472, MLM_loss=0.0335, NSP_Accuracy=0.9730, NSP_loss=0.4178, Total_loss=0.4513]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.4533, MLM_loss=0.0393, NSP_Accuracy=0.9793, NSP_loss=0.0034, Total_loss=0.0427]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:09<00:00,  2.86it/s, MLM_Accuracy=0.4844, MLM_loss=0.0166, NSP_Accuracy=0.9775, NSP_loss=0.0003, Total_loss=0.0169]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:08<00:00,  2.88it/s, MLM_Accuracy=0.4939, MLM_loss=0.0211, NSP_Accuracy=0.9906, NSP_loss=0.0009, Total_loss=0.0220]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5043, MLM_loss=0.0150, NSP_Accuracy=0.9892, NSP_loss=0.0011, Total_loss=0.0161]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5276, MLM_loss=0.0217, NSP_Accuracy=0.9892, NSP_loss=0.0026, Total_loss=0.0243]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5667, MLM_loss=0.0142, NSP_Accuracy=0.9955, NSP_loss=0.0001, Total_loss=0.0143]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:08<00:00,  2.89it/s, MLM_Accuracy=0.5603, MLM_loss=0.0249, NSP_Accuracy=0.9870, NSP_loss=0.5958, Total_loss=0.6207]\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_27844\\4154122956.py:85: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rec = self.rec.append(self.record, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "mask6_input = getData(modelType = modelType, datapath = datapath, maskPercent = 6, nspTask = \"NSP\")\n",
    "mask6 = trainModel(modelType = modelType, inputs = mask6_input.returnInput(), batch_size = batch_size, epoch = epoch, maskPercent = 6, saveModelName = \"saved_model_mask6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/445 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_16460\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 445/445 [02:15<00:00,  3.28it/s, MLM_Accuracy=0.0403, MLM_loss=0.5997, NSP_Accuracy=0.5110, NSP_loss=0.9812, Total_loss=1.5808]\n",
      "Epoch 1: 100%|██████████| 445/445 [02:14<00:00,  3.32it/s, MLM_Accuracy=0.0925, MLM_loss=0.3507, NSP_Accuracy=0.4993, NSP_loss=1.1418, Total_loss=1.4925]\n",
      "Epoch 2: 100%|██████████| 445/445 [02:14<00:00,  3.32it/s, MLM_Accuracy=0.1243, MLM_loss=0.2971, NSP_Accuracy=0.4962, NSP_loss=0.8415, Total_loss=1.1386]\n",
      "Epoch 3: 100%|██████████| 445/445 [02:14<00:00,  3.31it/s, MLM_Accuracy=0.1464, MLM_loss=0.1916, NSP_Accuracy=0.5083, NSP_loss=0.6393, Total_loss=0.8310]\n",
      "Epoch 4: 100%|██████████| 445/445 [02:14<00:00,  3.30it/s, MLM_Accuracy=0.1693, MLM_loss=0.1257, NSP_Accuracy=0.5164, NSP_loss=0.6703, Total_loss=0.7960]\n",
      "Epoch 5: 100%|██████████| 445/445 [02:16<00:00,  3.26it/s, MLM_Accuracy=0.1831, MLM_loss=0.1013, NSP_Accuracy=0.5164, NSP_loss=0.3760, Total_loss=0.4773]\n",
      "Epoch 6: 100%|██████████| 445/445 [02:15<00:00,  3.30it/s, MLM_Accuracy=0.1888, MLM_loss=0.1201, NSP_Accuracy=0.5245, NSP_loss=0.5501, Total_loss=0.6702]\n",
      "Epoch 7: 100%|██████████| 445/445 [02:15<00:00,  3.28it/s, MLM_Accuracy=0.1880, MLM_loss=0.1731, NSP_Accuracy=0.6503, NSP_loss=0.6875, Total_loss=0.8606]\n",
      "Epoch 8: 100%|██████████| 445/445 [02:16<00:00,  3.25it/s, MLM_Accuracy=0.1941, MLM_loss=0.2064, NSP_Accuracy=0.8427, NSP_loss=0.0337, Total_loss=0.2400]\n",
      "Epoch 9: 100%|██████████| 445/445 [02:17<00:00,  3.23it/s, MLM_Accuracy=0.1998, MLM_loss=0.1193, NSP_Accuracy=0.9452, NSP_loss=0.0151, Total_loss=0.1344]\n"
     ]
    }
   ],
   "source": [
    "mask15_input_sop = getData(modelType = modelType, datapath = datapath, maskPercent = 15, nspTask = \"SOP\")\n",
    "mask15_sop = trainModel(modelType = modelType, inputs = mask15_input_sop.returnInput(), batch_size = 5, epoch = epoch, maskPercent = 15, saveModelName = \"saved_model_mask15_sop\", saveCSV = False, nspTask = \"SOP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT_Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684b83e2f83316061361748e41b2620a10a3e9a8f2545480c20c18cf426689ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
