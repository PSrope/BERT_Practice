{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修改 Masking 位置策略\n",
    "原本 Masking 位置為完全隨機 => 以前被 Mask 過的位置，之後不會再 Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining, AdamW\n",
    "from transformers.models.bert.modeling_bert import BertForPreTrainingOutput, BertPreTrainingHeads, BertConfig, BERT_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "from transformers.models.albert.modeling_albert import AlbertSOPHead\n",
    "from transformers.utils import ModelOutput\n",
    "from transformers.utils.doc import add_start_docstrings_to_model_forward, replace_return_docstrings\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 BertForPreTraining 輸出形式\n",
    "# 主要是想從原本的 BertForPreTrainingOutput 多輸出 mlm_loss 和 nsp_loss\n",
    "class MyBertForPreTrainingOutput(BertForPreTrainingOutput):\n",
    "    \"\"\"\n",
    "    Output type of [`MyBertForPreTraining`].\n",
    "    Args:\n",
    "        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        mlm_loss (`float`):\n",
    "            MLM loss.\n",
    "        nsp_loss (`float`):\n",
    "            NSP loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, loss=None, prediction_logits=None, seq_relationship_logits=None, hidden_states=None, attentions=None, mlm_loss=None, nsp_loss=None):\n",
    "        super().__init__(loss=loss, prediction_logits=prediction_logits, seq_relationship_logits=seq_relationship_logits, hidden_states=hidden_states, attentions=attentions)\n",
    "        self.mlm_loss = mlm_loss\n",
    "        self.nsp_loss = nsp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlbertSOPHead(torch.nn.Module):\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size , config.num_labels)\n",
    "\n",
    "    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n",
    "        dropout_pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPretrainingHeadsWithSOP(BertPreTrainingHeads):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.seq_relationship = MyAlbertSOPHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改本來的 BertForPreTraining\n",
    "class MyBertForPreTraining(BertForPreTraining):\n",
    "    def __init__(self, config, nspTask = \"NSP\"):\n",
    "        super().__init__(config)\n",
    "        if nspTask == \"SOP\":\n",
    "            self.cls = BertPretrainingHeadsWithSOP(config)\n",
    "            \n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=MyBertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MyBertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return MyBertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            mlm_loss = masked_lm_loss,\n",
    "            nsp_loss = next_sentence_loss,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取出資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getData():\n",
    "    def __init__(self, modelType, datapath, nspTask = \"NSP\"):\n",
    "        self.datapath = datapath\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.nspTask = nspTask\n",
    "        self.df = pd.read_pickle(self.datapath)\n",
    "        self.sentence_a = []\n",
    "        self.sentence_b = []\n",
    "        self.label = []\n",
    "        self.important_label = []\n",
    "        self.inputs = None\n",
    "        self.nspPrepare()\n",
    "        self.inputs['labels'] = self.inputs.input_ids.detach().clone()\n",
    "    \n",
    "    def nspPrepare(self):\n",
    "        if self.nspTask == \"NSP\":\n",
    "            self.nspData()\n",
    "        # elif self.nspTask == \"SOP\":\n",
    "        #     self.sopData()\n",
    "\n",
    "        self.inputs = self.tokenizer(self.sentence_a, self.sentence_b, return_tensors='pt',\n",
    "                   max_length=512, truncation=True, padding='max_length')\n",
    "        self.inputs['next_sentence_label'] = torch.LongTensor([self.label]).T\n",
    "\n",
    "        fixed_len = 512\n",
    "        self.important_label = [sublist + [0] * (fixed_len - len(sublist)) for sublist in self.important_label]\n",
    "        self.important_label = torch.LongTensor(self.important_label)\n",
    "        # self.inputs['important_label'] = self.important_label\n",
    "        \n",
    "        mask_important = torch.full(self.important_label.shape, False)\n",
    "        mask_important[self.important_label >= 1] = True\n",
    "        self.inputs['mask_important'] = mask_important\n",
    "    \n",
    "    def nspData(self):  \n",
    "        text_idx = 0\n",
    "        sen_idx = 0\n",
    "        text_num = len(self.df)\n",
    "        while text_idx < text_num:\n",
    "            num_sentences = len(self.df.iloc[text_idx, 2])\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    self.sentence_a.append(self.df.iloc[text_idx, 2][start])\n",
    "                    self.sentence_b.append(self.df.iloc[text_idx, 2][start+1])\n",
    "                    self.label.append(0)\n",
    "                    combine = self.df.iloc[text_idx, 3][start] + self.df.iloc[text_idx, 3][start+1][1:]\n",
    "                    combine_len = len(combine)\n",
    "                    if combine_len <= 512:\n",
    "                        self.important_label.append(combine)\n",
    "                    else:\n",
    "                        a_cpy = copy.deepcopy(self.df.iloc[text_idx, 3][start])\n",
    "                        b_cpy = copy.deepcopy(self.df.iloc[text_idx, 3][start+1])\n",
    "                        len_a = len(a_cpy)\n",
    "                        len_b = len(b_cpy)\n",
    "                        while combine_len > 512:\n",
    "                            if len_a >= len_b:\n",
    "                                a_cpy.pop(-2)\n",
    "                                len_a -= 1\n",
    "                            else:\n",
    "                                b_cpy.pop(-2)\n",
    "                                len_b -= 1\n",
    "                            combine_len -= 1\n",
    "                        self.important_label.append(a_cpy + b_cpy[1:])\n",
    "                else:\n",
    "                    text_rand = text_idx\n",
    "                    rand_sen = sen_idx\n",
    "                    while (text_rand == text_idx) and (rand_sen in [sen_idx, sen_idx+1]):\n",
    "                        text_rand = random.randint(0, text_num-1)\n",
    "                        rand_sen = random.randint(0, len(self.df.iloc[text_rand, 2])-1)\n",
    "                    # this is NotNextSentence\n",
    "                    self.sentence_a.append(self.df.iloc[text_idx, 2][start])\n",
    "                    self.sentence_b.append(self.df.iloc[text_rand, 2][rand_sen])\n",
    "                    self.label.append(1)\n",
    "                    combine = self.df.iloc[text_idx, 3][start] + self.df.iloc[text_rand, 3][rand_sen][1:]\n",
    "                    combine_len = len(combine)\n",
    "                    if combine_len <= 512:\n",
    "                        self.important_label.append(combine)\n",
    "                    else:\n",
    "                        a_cpy = copy.deepcopy(self.df.iloc[text_idx, 3][start])\n",
    "                        b_cpy = copy.deepcopy(self.df.iloc[text_rand, 3][rand_sen])\n",
    "                        len_a = len(a_cpy)\n",
    "                        len_b = len(b_cpy)\n",
    "                        while combine_len > 512:\n",
    "                            if len_a >= len_b:\n",
    "                                a_cpy.pop(-2)\n",
    "                                len_a -= 1\n",
    "                            else:\n",
    "                                b_cpy.pop(-2)\n",
    "                                len_b -= 1\n",
    "                            combine_len -= 1\n",
    "                        self.important_label.append(a_cpy + b_cpy[1:])\n",
    "            text_idx += 1\n",
    "    \n",
    "    # def sopData(self):\n",
    "    #     for paragraph in text:\n",
    "    #         sentences = [\n",
    "    #             sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "    #         ]\n",
    "    #         num_sentences = len(sentences)\n",
    "    #         if num_sentences > 1:\n",
    "    #             start = random.randint(0, num_sentences-2)\n",
    "    #             # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "    #             if random.random() >= 0.5:\n",
    "    #                 # this is IsNextSentence\n",
    "    #                 self.sentence_a.append(sentences[start])\n",
    "    #                 self.sentence_b.append(sentences[start+1])\n",
    "    #                 self.label.append(0)\n",
    "    #             else:\n",
    "    #                 # this is NotNextSentence\n",
    "    #                 self.sentence_a.append(sentences[start+1])\n",
    "    #                 self.sentence_b.append(sentences[start])\n",
    "    #                 self.label.append(1)\n",
    "    \n",
    "    def returnInput(self):\n",
    "        return self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel():\n",
    "    def __init__(self, modelType, inputs, batch_size, epoch, acc_goal_each_epoch, masking_method = \"propose\", saveModelName = \"\", saveCSV = True, nspTask = \"NSP\"):\n",
    "        self.model = MyBertForPreTraining.from_pretrained(modelType)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.acc_goal_each_epoch = acc_goal_each_epoch  # 每個 epoch 的 MLM 正確率基準\n",
    "        self.masking_method = masking_method\n",
    "        self.saveModelName = saveModelName\n",
    "        self.saveCSV = saveCSV\n",
    "        self.loader = torch.utils.data.DataLoader(OurDataset(self.inputs), \\\n",
    "                                             batch_size=self.batch_size, shuffle=True)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        self.optim = AdamW(self.model.parameters(), lr = 5e-5)\n",
    "        self.last_acc = 0.0\n",
    "        \n",
    "        if os.path.isfile(\"record_maskPos.csv\"):\n",
    "            self.rec = pd.read_csv(\"record_maskPos.csv\")\n",
    "        else:\n",
    "            self.rec = pd.DataFrame({\"mlm_acc_each_epoch\":[], \"mlm_loss_each_epoch\":[], 'Mask_Percent_each_epoch':[]})\n",
    "            \n",
    "        self.training()\n",
    "        # self.save_model(self.saveModelName)\n",
    "    \n",
    "    # 把輸入序列對做 Masking\n",
    "    #   mask_ori: 原始可被 Mask 的位置 (非特殊 token 的地方)\n",
    "    #   mask_imp: 重要 token 的位置\n",
    "    def mlmPrepare(self, input_sentences, maskPercentNow, mask_ori, mask_imp):\n",
    "        # mask_arr: 表示本次要 Mask 的位置，True 的地方表示 Mask \n",
    "        mask_arr = torch.full(mask_ori.shape, False)    # 先初始化 (全部先填成 False)\n",
    "        \n",
    "        # 把輸入 batch 內的序列對依序處理\n",
    "        for i in range(len(mask_ori)):\n",
    "            num_to_mask = round(len(torch.where(mask_ori[i])[0]) * (maskPercentNow * 0.01)) # 表示該序列對有幾個 token 要 Mask\n",
    "            imp_pos = torch.where(mask_imp[i])  # 表示 important tokens 的位置 (index)\n",
    "            imp_len = len(imp_pos[0])           # 表示有多少個 important tokens\n",
    "\n",
    "            # 判斷 important tokens 是否夠用\n",
    "            #   如果不夠的話，拿 not important tokens 來補\n",
    "            if num_to_mask <= imp_len:\n",
    "                mask_index = torch.randperm(imp_len)[:num_to_mask]  # 從 imp_pos 中隨機提取 num_to_mask 個元素做為要被 Mask 的 index\n",
    "                mask_arr[i, imp_pos[0][mask_index]] = True          # 更新 mask_arr，將位置在 mask_index 的元素改為 True，表示 \"要做 Mask\"\n",
    "            else:\n",
    "                mask_notImp = mask_ori[i] ^ mask_imp[i]             # mask_notImp 表示不重要 token 的位置\n",
    "                # 先把 imp_pos Mask 掉\n",
    "                mask_index = torch.randperm(imp_len)[:imp_len]\n",
    "                num_to_mask -= imp_len\n",
    "                mask_arr[i, imp_pos[0][mask_index]] = True\n",
    "\n",
    "                # 剩下的 num_to_mask 由不重要的 tokens mask\n",
    "                notImp_pos = torch.where(mask_notImp)\n",
    "                notImp_pos_len = len(notImp_pos[0])\n",
    "                new_index = torch.randperm(notImp_pos_len)[:num_to_mask]\n",
    "                # 更新 mask_arr、mask_avai\n",
    "                mask_arr[i, notImp_pos[0][new_index]] = True\n",
    "\n",
    "        selection = []\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            selection.append(\n",
    "                torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            )\n",
    "\n",
    "        rand_mask_type = copy.deepcopy(selection)\n",
    "\n",
    "        for row in range(len(rand_mask_type)):\n",
    "            for col in range(len(rand_mask_type[row])):\n",
    "                rand_mask_type[row][col] = random.random()\n",
    "\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        special_tokens = [vocab['[CLS]'], vocab['[SEP]'], vocab['[MASK]'], vocab['[UNK]'],  vocab['[PAD]']]\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            for j in range(len(selection[i])):\n",
    "                if rand_mask_type[i][j] < 0.10:\n",
    "                    continue\n",
    "                elif rand_mask_type[i][j] < 0.20:\n",
    "                    rand_num = vocab['[CLS]']\n",
    "                    while rand_num in special_tokens:\n",
    "                        rand_num = random.randint(1, vocab_size-1)\n",
    "                    input_sentences[i, selection[i][j]] = rand_num\n",
    "                else:\n",
    "                    input_sentences[i, selection[i][j]] = 103\n",
    "        \n",
    "        return input_sentences, mask_arr\n",
    "\n",
    "    def training(self):\n",
    "        acc_each_epoch = []\n",
    "        loss_each_epoch = []\n",
    "        Mask_Percent_each_epoch = []\n",
    "        stay = 0\n",
    "        percent_now = 6\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            # setup loop with TQDM and dataloader\n",
    "            mask_nums = 0\n",
    "            mlm_correct = 0\n",
    "            nsp_nums = 0\n",
    "            nsp_correct = 0\n",
    "            loop = tqdm(self.loader, leave=True)\n",
    "\n",
    "            for batch_index, batch in enumerate(loop):\n",
    "                can_mask = (batch[\"input_ids\"] != 101) * (batch[\"input_ids\"] != 102) * (batch[\"input_ids\"] != 0)\n",
    "\n",
    "                input_sentences, mask_arr = self.mlmPrepare(batch[\"input_ids\"].detach().clone(), percent_now, \\\n",
    "                                                            can_mask, batch[\"mask_important\"])\n",
    "                \n",
    "                # initialize calculated gradients (from prev step)\n",
    "                self.optim.zero_grad()\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = input_sentences.to(self.device)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                next_sentence_label = batch['next_sentence_label'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                # process\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                next_sentence_label=next_sentence_label,\n",
    "                                labels=labels)\n",
    "                \n",
    "                prediction_logits = outputs.prediction_logits[mask_arr]\n",
    "                predicted_ids = prediction_logits.argmax(-1)\n",
    "                \n",
    "                seq_relationship_logits = outputs.seq_relationship_logits\n",
    "                predicted_labels = torch.argmax(seq_relationship_logits, dim=1)\n",
    "                predicted_label = predicted_labels\n",
    "\n",
    "                mask_nums += len(predicted_ids)\n",
    "                mlm_correct += torch.eq(predicted_ids, labels[mask_arr]).sum().item()\n",
    "                nsp_nums += len(predicted_label)\n",
    "                nsp_correct += predicted_label.eq(torch.squeeze(next_sentence_label)).sum().item()\n",
    "                \n",
    "                # extract loss\n",
    "                loss = outputs.loss\n",
    "                mlm_loss = outputs.mlm_loss.item()\n",
    "                nsp_loss = outputs.nsp_loss.item()\n",
    "                mlm_acc = mlm_correct / mask_nums\n",
    "                nsp_acc = nsp_correct / nsp_nums\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                self.optim.step()\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(Total_loss='{:.4f}'.format(loss.item()), MLM_Accuracy='{:.4f}'.format(mlm_acc), NSP_Accuracy='{:.4f}'.format(nsp_acc), \\\n",
    "                                MLM_loss='{:.4f}'.format(mlm_loss), NSP_loss='{:.4f}'.format(nsp_loss), Mask_Percent=percent_now)\n",
    "            \n",
    "            acc_each_epoch.append(mlm_acc)\n",
    "            loss_each_epoch.append(mlm_loss)\n",
    "            Mask_Percent_each_epoch.append(percent_now)\n",
    "\n",
    "            if self.masking_method == \"DMLM\":\n",
    "                percent_now += 1\n",
    "            elif self.masking_method == \"propose\":\n",
    "                if (mlm_acc >= self.acc_goal_each_epoch[epoch] * 0.01) or stay >= 2:\n",
    "                    stay = 0\n",
    "                    percent_now = 6 + epoch + 1\n",
    "                else:\n",
    "                    stay += 1\n",
    "            elif self.masking_method == \"adaptive\":\n",
    "                if mlm_acc > self.last_acc:\n",
    "                    percent_now += 1\n",
    "                else:\n",
    "                    percent_now -= 1\n",
    "                self.last_acc = mlm_acc\n",
    "            \n",
    "            if epoch % 2 == 1:\n",
    "                self.save_model(self.saveModelName + \"_epoch\" + str(epoch + 1))\n",
    "\n",
    "\n",
    "        if self.saveCSV:\n",
    "            \n",
    "            new_rec = pd.concat([self.rec, pd.DataFrame(pd.DataFrame({'mlm_acc_each_epoch': [acc_each_epoch], 'mlm_loss_each_epoch': [loss_each_epoch], 'Mask_Percent_each_epoch': [Mask_Percent_each_epoch]}))], ignore_index=True)\n",
    "            new_rec.to_csv(\"record_maskPos.csv\", index = False)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        self.model.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'bbc-text-importantLabeled.pkl'\n",
    "modelType = 'bert-base-cased'\n",
    "epoch = 10\n",
    "batch_size = 6\n",
    "nsp_input = getData(modelType = modelType, datapath = datapath, nspTask = \"NSP\")\n",
    "epoch_acc = [39.3, 45.6, 46.4, 47.2, 47.4, 47.8, 48.2, 48.8, 49.0 , 49.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:40<00:00,  2.31it/s, MLM_Accuracy=0.3985, MLM_loss=0.0369, Mask_Percent=6, NSP_Accuracy=0.8427, NSP_loss=0.4395, Total_loss=0.4764]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4642, MLM_loss=0.0216, Mask_Percent=7, NSP_Accuracy=0.9537, NSP_loss=0.3103, Total_loss=0.3319]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.4552, MLM_loss=0.0255, Mask_Percent=8, NSP_Accuracy=0.9663, NSP_loss=0.1223, Total_loss=0.1478]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.4621, MLM_loss=0.0315, Mask_Percent=9, NSP_Accuracy=0.9694, NSP_loss=0.0022, Total_loss=0.0337]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:49<00:00,  2.18it/s, MLM_Accuracy=0.4741, MLM_loss=0.0360, Mask_Percent=10, NSP_Accuracy=0.9816, NSP_loss=0.0828, Total_loss=0.1188]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:49<00:00,  2.18it/s, MLM_Accuracy=0.4703, MLM_loss=0.0291, Mask_Percent=11, NSP_Accuracy=0.9739, NSP_loss=0.0476, Total_loss=0.0767]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:49<00:00,  2.18it/s, MLM_Accuracy=0.4818, MLM_loss=0.0511, Mask_Percent=12, NSP_Accuracy=0.9874, NSP_loss=0.0609, Total_loss=0.1119]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.4693, MLM_loss=0.0910, Mask_Percent=13, NSP_Accuracy=0.9753, NSP_loss=0.4099, Total_loss=0.5009]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4645, MLM_loss=0.0490, Mask_Percent=14, NSP_Accuracy=0.9802, NSP_loss=0.0014, Total_loss=0.0503]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4582, MLM_loss=0.0499, Mask_Percent=15, NSP_Accuracy=0.9820, NSP_loss=0.2115, Total_loss=0.2614]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn_grow1 = trainModel(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, masking_method = \"DMLM\", saveModelName = \"saved_model/saved_model_SelMask_DMLM\")\n",
    "mask_dyn_grow1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.3970, MLM_loss=0.0531, Mask_Percent=6, NSP_Accuracy=0.8526, NSP_loss=0.2120, Total_loss=0.2652]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.4651, MLM_loss=0.0259, Mask_Percent=7, NSP_Accuracy=0.9465, NSP_loss=0.0688, Total_loss=0.0947]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.4699, MLM_loss=0.0251, Mask_Percent=8, NSP_Accuracy=0.9717, NSP_loss=0.6029, Total_loss=0.6280]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4732, MLM_loss=0.0448, Mask_Percent=9, NSP_Accuracy=0.9739, NSP_loss=0.4422, Total_loss=0.4870]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4640, MLM_loss=0.0501, Mask_Percent=10, NSP_Accuracy=0.9703, NSP_loss=0.0439, Total_loss=0.0940]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4623, MLM_loss=0.0355, Mask_Percent=10, NSP_Accuracy=0.9784, NSP_loss=0.1011, Total_loss=0.1366]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4791, MLM_loss=0.0213, Mask_Percent=10, NSP_Accuracy=0.9780, NSP_loss=0.0002, Total_loss=0.0215]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.4770, MLM_loss=0.0467, Mask_Percent=13, NSP_Accuracy=0.9915, NSP_loss=0.0002, Total_loss=0.0469]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.4668, MLM_loss=0.0393, Mask_Percent=13, NSP_Accuracy=0.9847, NSP_loss=0.0418, Total_loss=0.0811]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.4700, MLM_loss=0.0571, Mask_Percent=13, NSP_Accuracy=0.9780, NSP_loss=0.0023, Total_loss=0.0594]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn = trainModel(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_SelMask_propose\")\n",
    "mask_dyn = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Mask Percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel_SMP():\n",
    "    def __init__(self, modelType, inputs, batch_size, epoch, acc_goal_each_epoch, masking_percent,masking_method = \"propose\", saveModelName = \"\", saveCSV = True, nspTask = \"NSP\"):\n",
    "        self.model = MyBertForPreTraining.from_pretrained(modelType)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.masking_percent = masking_percent\n",
    "        self.acc_goal_each_epoch = acc_goal_each_epoch  # 每個 epoch 的 MLM 正確率基準\n",
    "        self.masking_method = masking_method\n",
    "        self.saveModelName = saveModelName\n",
    "        self.saveCSV = saveCSV\n",
    "        self.loader = torch.utils.data.DataLoader(OurDataset(self.inputs), \\\n",
    "                                             batch_size=self.batch_size, shuffle=True)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        self.optim = AdamW(self.model.parameters(), lr = 5e-5)\n",
    "        self.last_acc = 0.0\n",
    "        self.imp_cnt = 0\n",
    "        \n",
    "        if os.path.isfile(\"record_maskPos.csv\"):\n",
    "            self.rec = pd.read_csv(\"record_maskPos.csv\")\n",
    "        else:\n",
    "            self.rec = pd.DataFrame({\"mlm_acc_each_epoch\":[], \"mlm_loss_each_epoch\":[], 'Mask_Percent_each_epoch':[]})\n",
    "            \n",
    "        self.training()\n",
    "        # self.save_model(self.saveModelName)\n",
    "    \n",
    "    # 把輸入序列對做 Masking\n",
    "    #   mask_ori: 原始可被 Mask 的位置 (非特殊 token 的地方)\n",
    "    #   mask_imp: 重要 token 的位置\n",
    "    def mlmPrepare(self, input_sentences, maskPercentNow, mask_ori, mask_imp):\n",
    "        # mask_arr: 表示本次要 Mask 的位置，True 的地方表示 Mask \n",
    "        mask_arr = torch.full(mask_ori.shape, False)    # 先初始化 (全部先填成 False)\n",
    "        \n",
    "        # 把輸入 batch 內的序列對依序處理\n",
    "        for i in range(len(mask_ori)):\n",
    "            num_to_mask = round(len(torch.where(mask_ori[i])[0]) * (maskPercentNow * 0.01)) # 表示該序列對有幾個 token 要 Mask\n",
    "            imp_pos = torch.where(mask_imp[i])  # 表示 important tokens 的位置 (index)\n",
    "            imp_len = len(imp_pos[0])           # 表示有多少個 important tokens\n",
    "            \n",
    "            # 判斷 important tokens 是否夠用\n",
    "            #   如果不夠的話，拿 not important tokens 來補\n",
    "            if num_to_mask <= imp_len:\n",
    "                mask_index = torch.randperm(imp_len)[:num_to_mask]  # 從 imp_pos 中隨機提取 num_to_mask 個元素做為要被 Mask 的 index\n",
    "                mask_arr[i, imp_pos[0][mask_index]] = True          # 更新 mask_arr，將位置在 mask_index 的元素改為 True，表示 \"要做 Mask\"\n",
    "                self.imp_cnt += num_to_mask\n",
    "            else:\n",
    "                mask_notImp = mask_ori[i] ^ mask_imp[i]             # mask_notImp 表示不重要 token 的位置\n",
    "                # 先把 imp_pos Mask 掉\n",
    "                mask_index = torch.randperm(imp_len)[:imp_len]\n",
    "                num_to_mask -= imp_len\n",
    "                mask_arr[i, imp_pos[0][mask_index]] = True\n",
    "                self.imp_cnt += imp_len\n",
    "\n",
    "                # 剩下的 num_to_mask 由不重要的 tokens mask\n",
    "                notImp_pos = torch.where(mask_notImp)\n",
    "                notImp_pos_len = len(notImp_pos[0])\n",
    "                new_index = torch.randperm(notImp_pos_len)[:num_to_mask]\n",
    "                # 更新 mask_arr、mask_avai\n",
    "                mask_arr[i, notImp_pos[0][new_index]] = True\n",
    "\n",
    "        selection = []\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            selection.append(\n",
    "                torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            )\n",
    "\n",
    "        rand_mask_type = copy.deepcopy(selection)\n",
    "\n",
    "        for row in range(len(rand_mask_type)):\n",
    "            for col in range(len(rand_mask_type[row])):\n",
    "                rand_mask_type[row][col] = random.random()\n",
    "\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        special_tokens = [vocab['[CLS]'], vocab['[SEP]'], vocab['[MASK]'], vocab['[UNK]'],  vocab['[PAD]']]\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            for j in range(len(selection[i])):\n",
    "                if rand_mask_type[i][j] < 0.10:\n",
    "                    continue\n",
    "                elif rand_mask_type[i][j] < 0.20:\n",
    "                    rand_num = vocab['[CLS]']\n",
    "                    while rand_num in special_tokens:\n",
    "                        rand_num = random.randint(1, vocab_size-1)\n",
    "                    input_sentences[i, selection[i][j]] = rand_num\n",
    "                else:\n",
    "                    input_sentences[i, selection[i][j]] = 103\n",
    "        \n",
    "        return input_sentences, mask_arr\n",
    "\n",
    "    def training(self):\n",
    "        acc_each_epoch = []\n",
    "        loss_each_epoch = []\n",
    "        Mask_Percent_each_epoch = []\n",
    "        stay = 0\n",
    "        percent_now = self.masking_percent\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            # setup loop with TQDM and dataloader\n",
    "            mask_nums = 0\n",
    "            mlm_correct = 0\n",
    "            nsp_nums = 0\n",
    "            nsp_correct = 0\n",
    "            loop = tqdm(self.loader, leave=True)\n",
    "            self.imp_cnt = 0\n",
    "\n",
    "            for batch_index, batch in enumerate(loop):\n",
    "                can_mask = (batch[\"input_ids\"] != 101) * (batch[\"input_ids\"] != 102) * (batch[\"input_ids\"] != 0)\n",
    "                \n",
    "                input_sentences, mask_arr = self.mlmPrepare(batch[\"input_ids\"].detach().clone(), percent_now, \\\n",
    "                                                            can_mask, batch[\"mask_important\"])\n",
    "                \n",
    "                # initialize calculated gradients (from prev step)\n",
    "                self.optim.zero_grad()\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = input_sentences.to(self.device)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                next_sentence_label = batch['next_sentence_label'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                # process\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                next_sentence_label=next_sentence_label,\n",
    "                                labels=labels)\n",
    "                \n",
    "                prediction_logits = outputs.prediction_logits[mask_arr]\n",
    "                predicted_ids = prediction_logits.argmax(-1)\n",
    "                \n",
    "                seq_relationship_logits = outputs.seq_relationship_logits\n",
    "                predicted_labels = torch.argmax(seq_relationship_logits, dim=1)\n",
    "                predicted_label = predicted_labels\n",
    "\n",
    "                mask_nums += len(predicted_ids)\n",
    "                mlm_correct += torch.eq(predicted_ids, labels[mask_arr]).sum().item()\n",
    "                nsp_nums += len(predicted_label)\n",
    "                nsp_correct += predicted_label.eq(torch.squeeze(next_sentence_label)).sum().item()\n",
    "                \n",
    "                # extract loss\n",
    "                loss = outputs.loss\n",
    "                mlm_loss = outputs.mlm_loss.item()\n",
    "                nsp_loss = outputs.nsp_loss.item()\n",
    "                mlm_acc = mlm_correct / mask_nums\n",
    "                nsp_acc = nsp_correct / nsp_nums\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                self.optim.step()\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(Total_loss='{:.4f}'.format(loss.item()), MLM_Accuracy='{:.4f}'.format(mlm_acc), NSP_Accuracy='{:.4f}'.format(nsp_acc), \\\n",
    "                                MLM_loss='{:.4f}'.format(mlm_loss), NSP_loss='{:.4f}'.format(nsp_loss), Mask_Percent=percent_now, \\\n",
    "                                Masked_num='{:d}'.format(self.imp_cnt))\n",
    "                \n",
    "            acc_each_epoch.append(mlm_acc)\n",
    "            loss_each_epoch.append(mlm_loss)\n",
    "            Mask_Percent_each_epoch.append(percent_now)\n",
    "            if self.masking_method == \"DMLM\":\n",
    "                percent_now += 1\n",
    "            elif self.masking_method == \"propose\":\n",
    "                if (mlm_acc >= self.acc_goal_each_epoch[epoch] * 0.01) or stay >= 2:\n",
    "                    stay = 0\n",
    "                    percent_now = 6 + epoch + 1\n",
    "                else:\n",
    "                    stay += 1\n",
    "            elif self.masking_method == \"adaptive\":\n",
    "                if mlm_acc > self.last_acc:\n",
    "                    percent_now += 1\n",
    "                else:\n",
    "                    percent_now -= 1\n",
    "                self.last_acc = mlm_acc\n",
    "            \n",
    "            if epoch % 2 == 1:\n",
    "                self.save_model(self.saveModelName + \"_epoch\" + str(epoch + 1))\n",
    "\n",
    "        if self.saveCSV:\n",
    "            \n",
    "            new_rec = pd.concat([self.rec, pd.DataFrame(pd.DataFrame({'mlm_acc_each_epoch': [acc_each_epoch], 'mlm_loss_each_epoch': [loss_each_epoch], 'Mask_Percent_each_epoch': [Mask_Percent_each_epoch]}))], ignore_index=True)\n",
    "            new_rec.to_csv(\"record_maskPos.csv\", index = False)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        self.model.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.4000, MLM_loss=0.0460, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.8458, NSP_loss=0.7615, Total_loss=0.8075]\n",
      "Epoch 1: 100%|██████████| 371/371 [03:01<00:00,  2.05it/s, MLM_Accuracy=0.4720, MLM_loss=0.0355, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9519, NSP_loss=0.4054, Total_loss=0.4409]\n",
      "Epoch 2: 100%|██████████| 371/371 [03:03<00:00,  2.03it/s, MLM_Accuracy=0.4665, MLM_loss=0.0148, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9721, NSP_loss=0.0062, Total_loss=0.0209]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:53<00:00,  2.14it/s, MLM_Accuracy=0.4828, MLM_loss=0.0139, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9771, NSP_loss=0.0797, Total_loss=0.0936]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.4794, MLM_loss=0.0197, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9784, NSP_loss=0.0068, Total_loss=0.0265]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.4970, MLM_loss=0.0149, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9829, NSP_loss=0.1289, Total_loss=0.1437]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.4784, MLM_loss=0.0233, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9802, NSP_loss=0.0063, Total_loss=0.0296]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.4982, MLM_loss=0.0217, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9870, NSP_loss=0.0059, Total_loss=0.0276]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.5282, MLM_loss=0.0108, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9915, NSP_loss=0.0027, Total_loss=0.0135]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.5265, MLM_loss=0.0215, Mask_Percent=6, Masked_num=3876, NSP_Accuracy=0.9865, NSP_loss=0.0107, Total_loss=0.0321]\n"
     ]
    }
   ],
   "source": [
    "mask6 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 6, acc_goal_each_epoch = epoch_acc, masking_method = \"M6\", saveModelName = \"saved_model/saved_model_SelMask_M6\")\n",
    "mask6 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4013, MLM_loss=0.0511, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.8548, NSP_loss=0.0355, Total_loss=0.0866]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4608, MLM_loss=0.0257, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9488, NSP_loss=0.0009, Total_loss=0.0266]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4798, MLM_loss=0.0302, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9780, NSP_loss=0.0330, Total_loss=0.0632]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.5052, MLM_loss=0.0194, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9793, NSP_loss=0.0056, Total_loss=0.0251]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4781, MLM_loss=0.0233, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9667, NSP_loss=0.3559, Total_loss=0.3792]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4951, MLM_loss=0.0313, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9793, NSP_loss=0.0007, Total_loss=0.0321]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.5078, MLM_loss=0.0174, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9825, NSP_loss=0.0123, Total_loss=0.0297]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.5097, MLM_loss=0.0406, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9906, NSP_loss=0.0315, Total_loss=0.0722]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4929, MLM_loss=0.0299, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9861, NSP_loss=0.0007, Total_loss=0.0306]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.5059, MLM_loss=0.0108, Mask_Percent=7, Masked_num=4105, NSP_Accuracy=0.9613, NSP_loss=0.0003, Total_loss=0.0112]\n"
     ]
    }
   ],
   "source": [
    "mask7 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 7, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M7\")\n",
    "mask7 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4006, MLM_loss=0.0598, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.8454, NSP_loss=0.7016, Total_loss=0.7614]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4664, MLM_loss=0.0441, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9443, NSP_loss=0.0128, Total_loss=0.0569]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4678, MLM_loss=0.0324, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9609, NSP_loss=0.5140, Total_loss=0.5463]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4750, MLM_loss=0.0377, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9730, NSP_loss=0.0020, Total_loss=0.0397]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4950, MLM_loss=0.0289, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9807, NSP_loss=0.0198, Total_loss=0.0487]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4913, MLM_loss=0.0353, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9874, NSP_loss=0.0048, Total_loss=0.0401]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4962, MLM_loss=0.0298, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9865, NSP_loss=0.0112, Total_loss=0.0410]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4860, MLM_loss=0.0258, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9784, NSP_loss=0.1130, Total_loss=0.1388]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.5085, MLM_loss=0.0259, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9906, NSP_loss=0.0031, Total_loss=0.0291]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.5076, MLM_loss=0.0225, Mask_Percent=8, Masked_num=4226, NSP_Accuracy=0.9951, NSP_loss=0.0004, Total_loss=0.0229]\n"
     ]
    }
   ],
   "source": [
    "mask8 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 8, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M8\")\n",
    "mask8 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:57<00:00,  2.09it/s, MLM_Accuracy=0.3986, MLM_loss=0.0712, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.8458, NSP_loss=0.0601, Total_loss=0.1313]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.4694, MLM_loss=0.0188, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9371, NSP_loss=0.0240, Total_loss=0.0428]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:50<00:00,  2.17it/s, MLM_Accuracy=0.4714, MLM_loss=0.0211, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9694, NSP_loss=0.0019, Total_loss=0.0229]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4785, MLM_loss=0.0413, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9771, NSP_loss=0.0235, Total_loss=0.0649]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4868, MLM_loss=0.0258, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9762, NSP_loss=0.0303, Total_loss=0.0561]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4920, MLM_loss=0.0516, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9829, NSP_loss=0.0018, Total_loss=0.0534]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4957, MLM_loss=0.0508, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9820, NSP_loss=0.0217, Total_loss=0.0725]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4972, MLM_loss=0.0255, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9901, NSP_loss=0.0002, Total_loss=0.0257]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4950, MLM_loss=0.0302, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9798, NSP_loss=0.0035, Total_loss=0.0338]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4985, MLM_loss=0.0247, Mask_Percent=9, Masked_num=4308, NSP_Accuracy=0.9888, NSP_loss=0.0004, Total_loss=0.0251]\n"
     ]
    }
   ],
   "source": [
    "mask9 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 9, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M9\")\n",
    "mask9 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.3960, MLM_loss=0.0724, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.8485, NSP_loss=0.1229, Total_loss=0.1953]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4510, MLM_loss=0.0340, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9375, NSP_loss=0.0953, Total_loss=0.1294]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4655, MLM_loss=0.0332, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9685, NSP_loss=0.1013, Total_loss=0.1345]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4773, MLM_loss=0.0461, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9784, NSP_loss=0.0053, Total_loss=0.0513]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4676, MLM_loss=0.0216, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9753, NSP_loss=0.2011, Total_loss=0.2227]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4641, MLM_loss=0.0254, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9757, NSP_loss=0.0088, Total_loss=0.0342]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:50<00:00,  2.17it/s, MLM_Accuracy=0.4875, MLM_loss=0.0284, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9874, NSP_loss=0.0032, Total_loss=0.0316]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:50<00:00,  2.17it/s, MLM_Accuracy=0.4991, MLM_loss=0.0506, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9852, NSP_loss=0.0015, Total_loss=0.0521]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:50<00:00,  2.17it/s, MLM_Accuracy=0.5007, MLM_loss=0.0370, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9933, NSP_loss=0.0008, Total_loss=0.0378]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.5087, MLM_loss=0.0464, Mask_Percent=10, Masked_num=4380, NSP_Accuracy=0.9892, NSP_loss=0.0018, Total_loss=0.0482]\n"
     ]
    }
   ],
   "source": [
    "mask10 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 10, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M10\")\n",
    "mask10 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.3978, MLM_loss=0.0644, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.8440, NSP_loss=0.3357, Total_loss=0.4000]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4592, MLM_loss=0.0646, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9420, NSP_loss=0.0644, Total_loss=0.1291]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4720, MLM_loss=0.0321, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9640, NSP_loss=0.0319, Total_loss=0.0640]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4732, MLM_loss=0.0288, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9717, NSP_loss=0.0035, Total_loss=0.0323]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4878, MLM_loss=0.0439, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9838, NSP_loss=0.0001, Total_loss=0.0440]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4799, MLM_loss=0.0322, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9766, NSP_loss=0.0035, Total_loss=0.0357]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4757, MLM_loss=0.0340, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9735, NSP_loss=0.2452, Total_loss=0.2792]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4836, MLM_loss=0.0280, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9820, NSP_loss=0.0012, Total_loss=0.0291]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4771, MLM_loss=0.0415, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9838, NSP_loss=0.0011, Total_loss=0.0425]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4739, MLM_loss=0.0403, Mask_Percent=11, Masked_num=4416, NSP_Accuracy=0.9726, NSP_loss=0.0010, Total_loss=0.0413]\n"
     ]
    }
   ],
   "source": [
    "mask11 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 11, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M11\")\n",
    "mask11 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.3844, MLM_loss=0.0847, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.8364, NSP_loss=0.1680, Total_loss=0.2526]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.4586, MLM_loss=0.0309, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9290, NSP_loss=0.2086, Total_loss=0.2395]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4667, MLM_loss=0.0383, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9596, NSP_loss=0.0135, Total_loss=0.0518]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4621, MLM_loss=0.0432, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9708, NSP_loss=0.0028, Total_loss=0.0460]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4672, MLM_loss=0.0356, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9739, NSP_loss=0.1358, Total_loss=0.1714]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4801, MLM_loss=0.0493, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9897, NSP_loss=0.0052, Total_loss=0.0546]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4833, MLM_loss=0.0507, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9811, NSP_loss=0.2782, Total_loss=0.3289]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.4716, MLM_loss=0.0495, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9766, NSP_loss=0.0276, Total_loss=0.0771]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4804, MLM_loss=0.0574, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9888, NSP_loss=0.0554, Total_loss=0.1127]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4897, MLM_loss=0.0581, Mask_Percent=12, Masked_num=4441, NSP_Accuracy=0.9807, NSP_loss=0.0046, Total_loss=0.0627]\n"
     ]
    }
   ],
   "source": [
    "mask12 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 12, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M12\")\n",
    "mask12 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.3870, MLM_loss=0.0869, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.8306, NSP_loss=0.5302, Total_loss=0.6171]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.4463, MLM_loss=0.0349, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9375, NSP_loss=0.1503, Total_loss=0.1852]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4502, MLM_loss=0.0357, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9631, NSP_loss=0.0340, Total_loss=0.0697]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4575, MLM_loss=0.0254, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9753, NSP_loss=0.0033, Total_loss=0.0287]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4581, MLM_loss=0.0570, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9676, NSP_loss=0.0261, Total_loss=0.0831]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4655, MLM_loss=0.0487, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9807, NSP_loss=0.0026, Total_loss=0.0514]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4643, MLM_loss=0.0486, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9811, NSP_loss=0.0550, Total_loss=0.1037]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4804, MLM_loss=0.0467, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9879, NSP_loss=0.4011, Total_loss=0.4478]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4779, MLM_loss=0.0504, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9838, NSP_loss=0.0008, Total_loss=0.0511]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4812, MLM_loss=0.0238, Mask_Percent=13, Masked_num=4455, NSP_Accuracy=0.9789, NSP_loss=1.9182, Total_loss=1.9420]\n"
     ]
    }
   ],
   "source": [
    "mask13 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 13, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M13\")\n",
    "mask13 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.3829, MLM_loss=0.0721, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.8413, NSP_loss=0.1499, Total_loss=0.2221]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4371, MLM_loss=0.0429, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9348, NSP_loss=0.1037, Total_loss=0.1465]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4493, MLM_loss=0.0303, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9600, NSP_loss=0.2141, Total_loss=0.2444]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4534, MLM_loss=0.0708, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9708, NSP_loss=0.0316, Total_loss=0.1024]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4583, MLM_loss=0.0589, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9690, NSP_loss=0.0586, Total_loss=0.1175]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4620, MLM_loss=0.0529, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9708, NSP_loss=0.2143, Total_loss=0.2672]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4717, MLM_loss=0.0464, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9843, NSP_loss=0.0003, Total_loss=0.0467]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4832, MLM_loss=0.0437, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9829, NSP_loss=0.0035, Total_loss=0.0472]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4685, MLM_loss=0.0388, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9802, NSP_loss=0.0188, Total_loss=0.0577]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.4834, MLM_loss=0.0424, Mask_Percent=14, Masked_num=4463, NSP_Accuracy=0.9879, NSP_loss=0.0125, Total_loss=0.0549]\n"
     ]
    }
   ],
   "source": [
    "mask14 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 14, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M14\")\n",
    "mask14 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_1632\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.3824, MLM_loss=0.0768, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.8512, NSP_loss=0.1501, Total_loss=0.2269]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4355, MLM_loss=0.0637, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9200, NSP_loss=0.6433, Total_loss=0.7071]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.4521, MLM_loss=0.0964, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9622, NSP_loss=0.3905, Total_loss=0.4869]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.4543, MLM_loss=0.0607, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9636, NSP_loss=0.0319, Total_loss=0.0926]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4583, MLM_loss=0.0628, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9712, NSP_loss=0.0084, Total_loss=0.0712]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4574, MLM_loss=0.0636, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9766, NSP_loss=0.3721, Total_loss=0.4357]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4625, MLM_loss=0.0455, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9690, NSP_loss=0.0593, Total_loss=0.1049]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4662, MLM_loss=0.0537, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9870, NSP_loss=0.0028, Total_loss=0.0565]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4747, MLM_loss=0.0600, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9870, NSP_loss=0.0030, Total_loss=0.0629]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4657, MLM_loss=0.0604, Mask_Percent=15, Masked_num=4469, NSP_Accuracy=0.9771, NSP_loss=0.0039, Total_loss=0.0643]\n"
     ]
    }
   ],
   "source": [
    "mask15 = trainModel_SMP(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, masking_percent = 15, acc_goal_each_epoch = epoch_acc, masking_method = \"Static\", saveModelName = \"saved_model/saved_model_SelMask_M15\")\n",
    "mask15 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mlm_acc_each_epoch</th>\n",
       "      <th>mlm_loss_each_epoch</th>\n",
       "      <th>Mask_Percent_each_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.4, 0.47195490789111905, 0.466455870222711, ...</td>\n",
       "      <td>[0.04602385312318802, 0.03552519902586937, 0.0...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.40133740028155795, 0.46083489681050654, 0.4...</td>\n",
       "      <td>[0.05105382949113846, 0.02566910721361637, 0.0...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.40063740104862755, 0.46635158738312954, 0.4...</td>\n",
       "      <td>[0.059829700738191605, 0.04412558674812317, 0....</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.3986374516663598, 0.4693595877806404, 0.471...</td>\n",
       "      <td>[0.0711909681558609, 0.018787456676363945, 0.0...</td>\n",
       "      <td>[9, 9, 9, 9, 9, 9, 9, 9, 9, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.3959842001316656, 0.45097071405067457, 0.46...</td>\n",
       "      <td>[0.072414830327034, 0.03401566669344902, 0.033...</td>\n",
       "      <td>[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.3977689600958299, 0.45916991309559485, 0.47...</td>\n",
       "      <td>[0.0643717497587204, 0.06463858485221863, 0.03...</td>\n",
       "      <td>[11, 11, 11, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.384367279714521, 0.4586393904029656, 0.4667...</td>\n",
       "      <td>[0.08465049415826797, 0.03085910715162754, 0.0...</td>\n",
       "      <td>[12, 12, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.38701067615658363, 0.4462888761686701, 0.45...</td>\n",
       "      <td>[0.0869370847940445, 0.03490055352449417, 0.03...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.38286251248017855, 0.4371366492454049, 0.44...</td>\n",
       "      <td>[0.07213782519102097, 0.042857006192207336, 0....</td>\n",
       "      <td>[14, 14, 14, 14, 14, 14, 14, 14, 14, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.38236911570611526, 0.4355308737009952, 0.45...</td>\n",
       "      <td>[0.07680151611566544, 0.06371640413999557, 0.0...</td>\n",
       "      <td>[15, 15, 15, 15, 15, 15, 15, 15, 15, 15]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mlm_acc_each_epoch  \\\n",
       "2   [0.4, 0.47195490789111905, 0.466455870222711, ...   \n",
       "3   [0.40133740028155795, 0.46083489681050654, 0.4...   \n",
       "4   [0.40063740104862755, 0.46635158738312954, 0.4...   \n",
       "5   [0.3986374516663598, 0.4693595877806404, 0.471...   \n",
       "6   [0.3959842001316656, 0.45097071405067457, 0.46...   \n",
       "7   [0.3977689600958299, 0.45916991309559485, 0.47...   \n",
       "8   [0.384367279714521, 0.4586393904029656, 0.4667...   \n",
       "9   [0.38701067615658363, 0.4462888761686701, 0.45...   \n",
       "10  [0.38286251248017855, 0.4371366492454049, 0.44...   \n",
       "11  [0.38236911570611526, 0.4355308737009952, 0.45...   \n",
       "\n",
       "                                  mlm_loss_each_epoch  \\\n",
       "2   [0.04602385312318802, 0.03552519902586937, 0.0...   \n",
       "3   [0.05105382949113846, 0.02566910721361637, 0.0...   \n",
       "4   [0.059829700738191605, 0.04412558674812317, 0....   \n",
       "5   [0.0711909681558609, 0.018787456676363945, 0.0...   \n",
       "6   [0.072414830327034, 0.03401566669344902, 0.033...   \n",
       "7   [0.0643717497587204, 0.06463858485221863, 0.03...   \n",
       "8   [0.08465049415826797, 0.03085910715162754, 0.0...   \n",
       "9   [0.0869370847940445, 0.03490055352449417, 0.03...   \n",
       "10  [0.07213782519102097, 0.042857006192207336, 0....   \n",
       "11  [0.07680151611566544, 0.06371640413999557, 0.0...   \n",
       "\n",
       "                     Mask_Percent_each_epoch  \n",
       "2             [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]  \n",
       "3             [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]  \n",
       "4             [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]  \n",
       "5             [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]  \n",
       "6   [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]  \n",
       "7   [11, 11, 11, 11, 11, 11, 11, 11, 11, 11]  \n",
       "8   [12, 12, 12, 12, 12, 12, 12, 12, 12, 12]  \n",
       "9   [13, 13, 13, 13, 13, 13, 13, 13, 13, 13]  \n",
       "10  [14, 14, 14, 14, 14, 14, 14, 14, 14, 14]  \n",
       "11  [15, 15, 15, 15, 15, 15, 15, 15, 15, 15]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_rec = pd.read_csv(\"record_maskPos.csv\")\n",
    "df_rec = df_rec[2:]\n",
    "df_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38236911570611526, 0.4355308737009952, 0.45210601561640823, 0.4542654419448875, 0.4582737768004398, 0.4573690286405365, 0.4624896949711459, 0.4661534231509486, 0.4747030356357237, 0.465742879137798], [0.38286251248017855, 0.4371366492454049, 0.4492600422832981, 0.45342964529011043, 0.458281956432388, 0.4620487232169064, 0.47172802536550995, 0.4831533223761446, 0.46854995007928585, 0.483437096205802], [0.38701067615658363, 0.4462888761686701, 0.4502002161062734, 0.4575387744724129, 0.4580526248887759, 0.46545477658424966, 0.4642766336130181, 0.4803560076287349, 0.47787104158718047, 0.4812436419125127], [0.384367279714521, 0.4586393904029656, 0.4667490220300597, 0.4620874219446922, 0.46717431570281953, 0.4800549261929283, 0.48329101763535304, 0.471602225121901, 0.48037331869338457, 0.4896769325742506], [0.3977689600958299, 0.45916991309559485, 0.4720245674481312, 0.47316817603472794, 0.4878322725570947, 0.47985923929320157, 0.4757303370786517, 0.4836102379883251, 0.4770855174479557, 0.47387724550898], [0.3959842001316656, 0.45097071405067457, 0.4654760925026747, 0.47725590194949413, 0.4675548976067111, 0.46407703069706197, 0.48753599341834636, 0.49913573133591244, 0.5007404969557347, 0.5087228439763], [0.3986374516663598, 0.4693595877806404, 0.4714009394860459, 0.4784931380676061, 0.4868348370465844, 0.49198747467305215, 0.4957186262775067, 0.4972375690607735, 0.4949838932351588, 0.498480523068422], [0.40063740104862755, 0.46635158738312954, 0.4677833727263385, 0.4750205592105263, 0.49501490389556996, 0.49126413155190135, 0.4962498715709442, 0.48602835422231355, 0.508481546211576, 0.507606907894736], [0.40133740028155795, 0.46083489681050654, 0.479770141902193, 0.5052187170165358, 0.4780670889045273, 0.4951331066025566, 0.5077987568898792, 0.5096774193548387, 0.49290322580645163, 0.505864414731409], [0.4, 0.47195490789111905, 0.466455870222711, 0.4827538820942696, 0.4793842770753161, 0.4970446735395189, 0.4784162771514985, 0.4982128127577674, 0.5281748213304013, 0.526525563496426]]\n"
     ]
    }
   ],
   "source": [
    "mask_range = [i for i in range(15, 5, -1)]\n",
    "epochs = [i for i in range(10)]\n",
    "mlm_acc = []\n",
    "for i in mask_range:\n",
    "    cur_mask = df_rec[df_rec[\"Mask_Percent_each_epoch\"] == ('[' + str(i) + (', ' + str(i)) * 9 + ']')]\n",
    "    acc_this_percent = []\n",
    "    for i in range(len(cur_mask)):\n",
    "        acc_this_percent.append(cur_mask.iloc[i][0][2:-2].split(','))\n",
    "        for j in range(len(acc_this_percent[-1])):\n",
    "            acc_this_percent[-1][j] = float(acc_this_percent[-1][j])\n",
    "    mlm_acc.append(np.mean(np.array(acc_this_percent), axis=0).tolist())\n",
    "print((mlm_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3930975 , 0.45562374, 0.46412263, 0.47192317, 0.4736471 ,\n",
       "       0.47842931, 0.48232352, 0.48751671, 0.49038668, 0.4941178 ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(mlm_acc)\n",
    "a = np.sort(a.T)\n",
    "total_acc_mean = np.mean(a, axis=1)\n",
    "total_acc_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT_Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684b83e2f83316061361748e41b2620a10a3e9a8f2545480c20c18cf426689ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
