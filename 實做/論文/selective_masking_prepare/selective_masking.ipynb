{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Masking 流程準備\n",
    "參考論文：[Train No Evil: Selective Masking for Task-Guided Pre-Training](https://arxiv.org/abs/2004.09733)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datapath = '../bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 決定 tokenizer 類型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# 決定資料集中各分類對應的 id\n",
    "labels = {'business':0,\n",
    "          'entertainment':1,\n",
    "          'sport':2,\n",
    "          'tech':3,\n",
    "          'politics':4\n",
    "          }\n",
    "\n",
    "# 資料集處理\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        # 把每一筆資料的類別改成 id\n",
    "        self.labels = [labels[label] for label in df['category']]  \n",
    "        # 對每筆資料做 BERT tokenize\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    # 回傳資料集各類別 (id)\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    # 回傳該 label 的資料數\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # 取得當前資料的 label\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    # 取得當前資料的 text\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780 222 223\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, batch_size, model_name, save_path):\n",
    "    if os.path.isfile(\"fine_tune_record_epoch.csv\"):\n",
    "        rec = pd.read_csv(\"fine_tune_record_epoch.csv\")\n",
    "    else:\n",
    "        rec = pd.DataFrame({\"model_name\":[], \"train_acc\":[], \"train_loss\":[], \"val_acc\":[], \"val_loss\":[]})\n",
    "\n",
    "    # 把原本的資料經過 Dataset 類別包裝起來\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    # 把訓練、驗證資料集丟進 Dataloader 定義取樣資訊 (ex: 設定 batch_size...等等)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "\n",
    "    # 偵測有 GPU，有就用\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()                       # Loss Function: Categorical cross entropy\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate) # Optimizer: Adam\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    val_acc = []\n",
    "    val_loss = []\n",
    "    # 每次完整訓練 (每個 epoch) 要做的事\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            # ---------- 訓練的部分 ----------\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            # 這邊加上 tqdm 模組來顯示 dataloader 處理進度條\n",
    "            # 所以在程式意義上，可以直接把這行當作 for train_input, train_label in train_dataloader:\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                # .to(device): 把東西 (tensor) 丟到 GPU 的概念\n",
    "                train_label = train_label.type(torch.LongTensor).to(device)\n",
    "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                # 把 data 丟進 BERT\n",
    "                output = model(input_ids=input_id, attention_mask=mask, labels=train_label)\n",
    "                \n",
    "                # 計算 Cross Entropy，以此計算 loss\n",
    "                batch_loss = output[0]\n",
    "                total_loss_train += batch_loss.item()               # .item(): tensor 轉 純量\n",
    "                \n",
    "                # 看 model output \"可能性最高\" 的 label 是不是和 data 一樣，是的話，acc + 1\n",
    "                logits = output[1]\n",
    "                pred_label = logits.argmax(dim=1)\n",
    "                acc = (pred_label == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()       # 清空前一次 Gradient\n",
    "                batch_loss.backward()   # 根據 lost 計算 back propagation\n",
    "                optimizer.step()        # 做 Gradient Decent\n",
    "            \n",
    "            # ---------- 驗證的部分 ----------\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            # 步驟和訓練時差不多，差在沒做 Gradient Decent\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.type(torch.LongTensor).to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_ids=input_id, attention_mask=mask, labels=val_label)\n",
    "\n",
    "                    batch_loss = output[0]\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    logits = output[1]\n",
    "                    pred_label = logits.argmax(dim=1)\n",
    "                    acc = (pred_label == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            train_loss.append(total_loss_train / len(train_data))\n",
    "            train_acc.append(total_acc_train / len(train_data))\n",
    "            val_loss.append(total_loss_val / len(val_data))\n",
    "            val_acc.append(total_acc_val / len(val_data))\n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "    \n",
    "    new_rec = pd.concat([rec, pd.DataFrame(pd.DataFrame({'model_name': model_name, 'train_acc': [train_acc], 'train_loss': [train_loss], 'val_acc': [val_acc], 'val_loss': [val_loss]}))], ignore_index=True)\n",
    "    new_rec.to_csv(\"fine_tune_record_epoch.csv\", index = None)\n",
    "    model.save_pretrained(save_path)\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 223/223 [01:19<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.065                 | Train Accuracy:  0.842                 | Val Loss:  0.007                 | Val Accuracy:  0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [01:24<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.008                 | Train Accuracy:  0.984                 | Val Loss:  0.006                 | Val Accuracy:  0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [01:24<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.005                 | Train Accuracy:  0.990                 | Val Loss:  0.003                 | Val Accuracy:  0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [01:23<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.003                 | Train Accuracy:  0.994                 | Val Loss:  0.012                 | Val Accuracy:  0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [01:23<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.003                 | Train Accuracy:  0.993                 | Val Loss:  0.011                 | Val Accuracy:  0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [01:23<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.001                 | Train Accuracy:  0.997                 | Val Loss:  0.007                 | Val Accuracy:  0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [01:22<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.001                 | Train Accuracy:  0.998                 | Val Loss:  0.006                 | Val Accuracy:  0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [01:23<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.004                 | Train Accuracy:  0.993                 | Val Loss:  0.011                 | Val Accuracy:  0.986\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "LR = 2e-5\n",
    "batch_size = 8\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=5)     \n",
    "train(model, df_train, df_val, LR, EPOCHS, batch_size, \"Fine-tuned_BERT\", \"fine_tuned_bert\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downstream Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>important_labels</th>\n",
       "      <th>most_important</th>\n",
       "      <th>second_important</th>\n",
       "      <th>third_important</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "  important_labels most_important second_important third_important  \n",
       "0                                                                   \n",
       "1                                                                   \n",
       "2                                                                   \n",
       "3                                                                   \n",
       "4                                                                   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()\n",
    "df[\"important_labels\"] = [\" \" for _ in range(len(df.index))]\n",
    "df[\"most_important\"] = [\" \" for _ in range(len(df.index))]\n",
    "df[\"second_important\"] = [\" \" for _ in range(len(df.index))]\n",
    "df[\"third_important\"] = [\" \" for _ in range(len(df.index))]\n",
    "df.to_csv(\"bbc-text-with-important.csv\", index=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = 'bbc-text-with-important.csv'\n",
    "# df = pd.read_csv(datapath)\n",
    "# df[2202:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sentences.append(df.iloc[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_threshold = 0.05\n",
    "second_threshold = 0.07\n",
    "third_threshold = 0.10\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    inputs_sentence = tokenizer(sentences[i], padding='max_length', \\\n",
    "                                max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    model.to(device)\n",
    "    inputs_sentence = inputs_sentence.to(device)\n",
    "    outputs_sentence = model(**inputs_sentence)\n",
    "\n",
    "    logits_sentence = outputs_sentence.logits\n",
    "    probs_sentence = F.softmax(logits_sentence, dim=-1)\n",
    "    pred_label_sentence = torch.argmax(probs_sentence, dim=-1).item()\n",
    "    confidence_sentence = probs_sentence[0][pred_label_sentence].item()\n",
    "\n",
    "    sentence2_idx = 0\n",
    "    sentence2_input = torch.full(inputs_sentence['input_ids'].shape, 0)\n",
    "    sentence2_tkn_type = torch.full(inputs_sentence['token_type_ids'].shape, 0)\n",
    "    sentence2_att_mask = torch.full(inputs_sentence['attention_mask'].shape, 0)\n",
    "\n",
    "    for j in range(512):\n",
    "        token_now = inputs_sentence['input_ids'][0][j]\n",
    "        token_now_int = token_now.item()\n",
    "        \n",
    "        sentence2_input[0][sentence2_idx] = token_now\n",
    "        sentence2_att_mask[0][sentence2_idx] = 1\n",
    "        sentence2_idx += 1\n",
    "        if token_now_int == 101:\n",
    "            df.iloc[i, 2] += '0'\n",
    "            continue\n",
    "        elif token_now_int == 102:\n",
    "            df.iloc[i, 2] += \", 0\"\n",
    "            break\n",
    "        \n",
    "        sentence2_input = sentence2_input.to(device)\n",
    "        sentence2_tkn_type = sentence2_tkn_type.to(device)\n",
    "        sentence2_att_mask = sentence2_att_mask.to(device)\n",
    "        # print(tokenizer.decode(sentence2_input[0]))\n",
    "        outputs_sentence2 = model(input_ids=sentence2_input, attention_mask=sentence2_att_mask, token_type_ids=sentence2_tkn_type)\n",
    "\n",
    "        logits_sentence2 = outputs_sentence2.logits\n",
    "        probs_sentence2 = F.softmax(logits_sentence2, dim=-1)\n",
    "        pred_label_sentence2 = torch.argmax(probs_sentence2, dim=-1).item()\n",
    "        confidence_sentence2 = probs_sentence2[0][pred_label_sentence2].item()\n",
    "        \n",
    "        if pred_label_sentence != pred_label_sentence2:\n",
    "            df.iloc[i, 2] += \", 0\"\n",
    "        else:\n",
    "            score = abs(confidence_sentence - confidence_sentence2)\n",
    "            \n",
    "            if score <= most_threshold:\n",
    "                df.iloc[i, 2] += \", 3\"\n",
    "                df.iloc[i, 3] += \", \" + str(token_now_int)\n",
    "                sentence2_input[0][sentence2_idx] = 0\n",
    "                sentence2_att_mask[0][sentence2_idx] = 0\n",
    "                sentence2_idx -= 1\n",
    "            elif score <= second_threshold:\n",
    "                df.iloc[i, 2] += \", 2\"\n",
    "                df.iloc[i, 4] += \", \" + str(token_now_int)\n",
    "                sentence2_input[0][sentence2_idx] = 0\n",
    "                sentence2_att_mask[0][sentence2_idx] = 0\n",
    "                sentence2_idx -= 1\n",
    "            elif score <= third_threshold:\n",
    "                df.iloc[i, 2] += \", 1\"\n",
    "                df.iloc[i, 5] += \", \" + str(token_now_int)\n",
    "                sentence2_input[0][sentence2_idx] = 0\n",
    "                sentence2_att_mask[0][sentence2_idx] = 0\n",
    "                sentence2_idx -= 1\n",
    "            else:\n",
    "                df.iloc[i, 2] += \", 0\"\n",
    "df.to_csv(\"bbc-text-with-important.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category                                                     business\n",
       "text                telegraph newspapers axe 90 jobs the daily and...\n",
       "important_labels     0, 0, 0, 0, 0, 3, 1, 3, 2, 1, 0, 0, 0, 2, 2, ...\n",
       "most_important       , 5448, 3828, 118, 110, 1104, 2546, 1372, 515...\n",
       "second_important     , 1105, 1132, 170, 3078, 1103, 1867, 5841, 12...\n",
       "third_important      , 1103, 3336, 5448, 1542, 9378, 119, 1106, 11...\n",
       "Name: 60, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jobs daily - % of staff group investment firm financial'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.iloc[60,3].split(', ')[1:]\n",
    "for k in range(len(test)):\n",
    "    test[k] = int(test[k])\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and are a 90 the says fund new facilities journalists management facilities'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.iloc[60,4].split(', ')[1:]\n",
    "for k in range(len(test)):\n",
    "    test[k] = int(test[k])\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the sun jobs 17 editorial. to50m in printing. revenues journalists has on the to recall the of redunda by mid mon p. said. hasbloid shrink size.bloid the telegraph was by the year owned of businesses retailer. telegraph executive newspapers journalists theblood of newspaper daily telegraphday readers production machinery'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.iloc[60,5].split(', ')[1:]\n",
    "for k in range(len(test)):\n",
    "    test[k] = int(test[k])\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>important_labels</th>\n",
       "      <th>most_important</th>\n",
       "      <th>second_important</th>\n",
       "      <th>third_important</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "  important_labels most_important second_important third_important  \n",
       "0                                                                   \n",
       "1                                                                   \n",
       "2                                                                   \n",
       "3                                                                   \n",
       "4                                                                   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "\n",
    "df[\"important_labels\"] = [\" \" for _ in range(len(df.index))]\n",
    "df[\"most_important\"] = [\" \" for _ in range(len(df.index))]\n",
    "df[\"second_important\"] = [\" \" for _ in range(len(df.index))]\n",
    "df[\"third_important\"] = [\" \" for _ in range(len(df.index))]\n",
    "df.to_csv(\"bbc-text-with-important-003.csv\", index=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sentences.append(df.iloc[i, 1])\n",
    "\n",
    "most_threshold = 0.03\n",
    "second_threshold = 0.05\n",
    "third_threshold = 0.07\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [4:41:42<00:00,  7.60s/it]  \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in tqdm(range(len(sentences))):\n",
    "    inputs_sentence = tokenizer(sentences[i], padding='max_length', \\\n",
    "                                max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    model.to(device)\n",
    "    inputs_sentence = inputs_sentence.to(device)\n",
    "    outputs_sentence = model(**inputs_sentence)\n",
    "\n",
    "    logits_sentence = outputs_sentence.logits\n",
    "    probs_sentence = F.softmax(logits_sentence, dim=-1)\n",
    "    pred_label_sentence = torch.argmax(probs_sentence, dim=-1).item()\n",
    "    confidence_sentence = probs_sentence[0][pred_label_sentence].item()\n",
    "\n",
    "    sentence2_idx = 0\n",
    "    sentence2_input = torch.full(inputs_sentence['input_ids'].shape, 0)\n",
    "    sentence2_tkn_type = torch.full(inputs_sentence['token_type_ids'].shape, 0)\n",
    "    sentence2_att_mask = torch.full(inputs_sentence['attention_mask'].shape, 0)\n",
    "\n",
    "    for j in range(512):\n",
    "        token_now = inputs_sentence['input_ids'][0][j]\n",
    "        token_now_int = token_now.item()\n",
    "        \n",
    "        sentence2_input[0][sentence2_idx] = token_now\n",
    "        sentence2_att_mask[0][sentence2_idx] = 1\n",
    "        sentence2_idx += 1\n",
    "        if token_now_int == 101:\n",
    "            df.iloc[i, 2] += '0'\n",
    "            continue\n",
    "        elif token_now_int == 102:\n",
    "            df.iloc[i, 2] += \", 0\"\n",
    "            break\n",
    "        \n",
    "        sentence2_input = sentence2_input.to(device)\n",
    "        sentence2_tkn_type = sentence2_tkn_type.to(device)\n",
    "        sentence2_att_mask = sentence2_att_mask.to(device)\n",
    "        # print(tokenizer.decode(sentence2_input[0]))\n",
    "        outputs_sentence2 = model(input_ids=sentence2_input, attention_mask=sentence2_att_mask, token_type_ids=sentence2_tkn_type)\n",
    "\n",
    "        logits_sentence2 = outputs_sentence2.logits\n",
    "        probs_sentence2 = F.softmax(logits_sentence2, dim=-1)\n",
    "        pred_label_sentence2 = torch.argmax(probs_sentence2, dim=-1).item()\n",
    "        confidence_sentence2 = probs_sentence2[0][pred_label_sentence2].item()\n",
    "        \n",
    "        if pred_label_sentence != pred_label_sentence2:\n",
    "            df.iloc[i, 2] += \", 0\"\n",
    "        else:\n",
    "            score = abs(confidence_sentence - confidence_sentence2)\n",
    "            \n",
    "            if score <= most_threshold:\n",
    "                df.iloc[i, 2] += \", 3\"\n",
    "                df.iloc[i, 3] += \", \" + str(token_now_int)\n",
    "                sentence2_input[0][sentence2_idx] = 0\n",
    "                sentence2_att_mask[0][sentence2_idx] = 0\n",
    "                sentence2_idx -= 1\n",
    "            elif score <= second_threshold:\n",
    "                df.iloc[i, 2] += \", 2\"\n",
    "                df.iloc[i, 4] += \", \" + str(token_now_int)\n",
    "                sentence2_input[0][sentence2_idx] = 0\n",
    "                sentence2_att_mask[0][sentence2_idx] = 0\n",
    "                sentence2_idx -= 1\n",
    "            elif score <= third_threshold:\n",
    "                df.iloc[i, 2] += \", 1\"\n",
    "                df.iloc[i, 5] += \", \" + str(token_now_int)\n",
    "                sentence2_input[0][sentence2_idx] = 0\n",
    "                sentence2_att_mask[0][sentence2_idx] = 0\n",
    "                sentence2_idx -= 1\n",
    "            else:\n",
    "                df.iloc[i, 2] += \", 0\"\n",
    "df.to_csv(\"bbc-text-with-important-003.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category                                                        sport\n",
       "text                tigers wary of farrell  gamble  leicester say ...\n",
       "important_labels     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "most_important       , 2074, 3495, 4958, 9812, 2016, 4896, 3779, 3...\n",
       "second_important                                   , 5656, 5656, 1342\n",
       "third_important      , 1195, 1103, 1132, 1253, 1280, 1397, 176, 13...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'league captain decide codes stage rugby union clubs signing union league union centre rugby league forwards club'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.iloc[2,3].split(', ')[1:]\n",
    "for k in range(len(test)):\n",
    "    test[k] = int(test[k])\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knee knee game'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.iloc[2,4].split(', ')[1:]\n",
    "for k in range(len(test)):\n",
    "    test[k] = int(test[k])\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we the are still going next grell who has had persistent problems had an weeks ago is for another three months the list interested - playing backs'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.iloc[2,5].split(', ')[1:]\n",
    "for k in range(len(test)):\n",
    "    test[k] = int(test[k])\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score = 0.03, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "\n",
    "df[\"text_separate\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"important_labels\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"most_important\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"second_important\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"third_important\"] = [[] for _ in range(len(df.index))]\n",
    "df.to_pickle(\"bbc-text-with-important-003.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_separate</th>\n",
       "      <th>important_labels</th>\n",
       "      <th>most_important</th>\n",
       "      <th>second_important</th>\n",
       "      <th>third_important</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv future in the hands of viewers with home t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom boss  left books alone  former world...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers wary of farrell  gamble  leicester say...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading face newcastle in fa cup premiership ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean s twelve raids box office ocean s twelv...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                       text_separate important_labels  \\\n",
       "0  [tv future in the hands of viewers with home t...               []   \n",
       "1  [worldcom boss  left books alone  former world...               []   \n",
       "2  [tigers wary of farrell  gamble  leicester say...               []   \n",
       "3  [yeading face newcastle in fa cup premiership ...               []   \n",
       "4  [ocean s twelve raids box office ocean s twelv...               []   \n",
       "\n",
       "  most_important second_important third_important  \n",
       "0             []               []              []  \n",
       "1             []               []              []  \n",
       "2             []               []              []  \n",
       "3             []               []              []  \n",
       "4             []               []              []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    for str in df.iloc[i,1].split('. '):\n",
    "        if '! ' in str:\n",
    "            for idx, str_e in enumerate(str.split('! ')):\n",
    "                if idx == len(str.split('! ')) - 1:\n",
    "                    df.iloc[i,2].append(str_e + '!')\n",
    "                else:\n",
    "                    df.iloc[i,2].append(str_e + '.')\n",
    "        elif '? ' in str:\n",
    "            for idx, str_q in enumerate(str.split('? ')):\n",
    "                if idx == len(str.split('? ')) - 1:\n",
    "                    df.iloc[i,2].append(str_q + '?')\n",
    "                else:\n",
    "                    df.iloc[i,2].append(str_q + '.')\n",
    "        else:\n",
    "            df.iloc[i,2].append(str + '.')\n",
    "    df.iloc[i,2][-1] = df.iloc[i,2][-1][:-1]\n",
    "df.to_pickle(\"bbc-text-with-important-003.pkl\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sentences.append(df.iloc[i, 1])\n",
    "\n",
    "most_threshold = 0.03\n",
    "second_threshold = 0.05\n",
    "third_threshold = 0.07\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 161/2225 [22:35<4:18:50,  7.52s/it]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tok_SEP = tokenizer(\"[SEP]\", add_special_tokens=False, return_tensors='pt')\n",
    "tok_SEP_ids = tok_SEP['input_ids'][0]\n",
    "att_mask_1 = tok_SEP['attention_mask'][0]\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    inputs_text = tokenizer(df.iloc[i,1], padding='max_length', \\\n",
    "                            max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    model.to(device)\n",
    "    inputs_text = inputs_text.to(device)\n",
    "    outputs_sentence = model(**inputs_text)\n",
    "\n",
    "    logits_sentence = outputs_sentence.logits\n",
    "    probs_sentence = F.softmax(logits_sentence, dim=-1)\n",
    "    pred_label_sentence = torch.argmax(probs_sentence, dim=-1).item()\n",
    "    confidence_sentence = probs_sentence[0][pred_label_sentence].item()\n",
    "\n",
    "    sentence2_idx = 0\n",
    "    sentence2_input = torch.full(inputs_text['input_ids'].shape, 0)\n",
    "    sentence2_tkn_type = torch.full(inputs_text['token_type_ids'].shape, 0)\n",
    "    sentence2_att_mask = torch.full(inputs_text['attention_mask'].shape, 0)\n",
    "\n",
    "    sen_i = 0\n",
    "    word_i = 0\n",
    "    sen_num = len(df.iloc[i,2])\n",
    "    sen_label = []\n",
    "    \n",
    "    inputs_sentence = tokenizer(df.iloc[i,2][sen_i], padding='max_length', \\\n",
    "                                max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    while True:\n",
    "        token_now = inputs_sentence['input_ids'][0][word_i]\n",
    "        token_now_int = token_now.item()\n",
    "        \n",
    "        if sentence2_idx == 511:\n",
    "            for s2_idx in range(1, 510):\n",
    "                sentence2_input[0][s2_idx] = sentence2_input[0][s2_idx + 1].detach().clone()\n",
    "            sentence2_idx -= 1\n",
    "        sentence2_input[0][sentence2_idx] = token_now.detach().clone()\n",
    "        sentence2_att_mask[0][sentence2_idx] = att_mask_1.detach().clone()\n",
    "            \n",
    "        if sen_i == 0 and word_i == 0:\n",
    "            sen_label.append(0)\n",
    "            word_i += 1\n",
    "            sentence2_idx += 1\n",
    "            continue\n",
    "          \n",
    "        sentence2_input[0][sentence2_idx + 1] = tok_SEP_ids.detach().clone()\n",
    "        sentence2_att_mask[0][sentence2_idx + 1] = att_mask_1.detach().clone()\n",
    "        sentence2_input = sentence2_input.to(device)\n",
    "        sentence2_tkn_type = sentence2_tkn_type.to(device)\n",
    "        sentence2_att_mask = sentence2_att_mask.to(device)\n",
    "        \n",
    "        outputs_sentence2 = model(input_ids=sentence2_input, attention_mask=sentence2_att_mask, \\\n",
    "                                  token_type_ids=sentence2_tkn_type)\n",
    "\n",
    "        logits_sentence2 = outputs_sentence2.logits\n",
    "        probs_sentence2 = F.softmax(logits_sentence2, dim=-1)\n",
    "        pred_label_sentence2 = torch.argmax(probs_sentence2, dim=-1).item()\n",
    "        confidence_sentence2 = probs_sentence2[0][pred_label_sentence2].item()\n",
    "\n",
    "        if pred_label_sentence != pred_label_sentence2:\n",
    "            sen_label.append(0)\n",
    "            sentence2_idx += 1\n",
    "        else:\n",
    "            score = abs(confidence_sentence - confidence_sentence2)\n",
    "            \n",
    "            if score <= most_threshold:\n",
    "                sen_label.append(3)\n",
    "                df.iloc[i, 4].append(token_now_int)\n",
    "            elif score <= second_threshold:\n",
    "                sen_label.append(2)\n",
    "                df.iloc[i, 5].append(token_now_int)\n",
    "            elif score <= third_threshold:\n",
    "                sen_label.append(1)\n",
    "                df.iloc[i, 6].append(token_now_int)\n",
    "            else:\n",
    "                sen_label.append(0)\n",
    "                sentence2_idx += 1\n",
    "        word_i += 1\n",
    "        if inputs_sentence['input_ids'][0][word_i].item() == 102:\n",
    "            sen_label.append(0)\n",
    "            df.iloc[i, 3].append(sen_label)\n",
    "            sen_label = [0]\n",
    "            if sen_i == (sen_num - 1):\n",
    "                break\n",
    "            else:\n",
    "                word_i = 1\n",
    "                sen_i += 1\n",
    "                inputs_sentence = tokenizer(df.iloc[i,2][sen_i], padding='max_length', \\\n",
    "                                            max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "df.to_pickle(\"bbc-text-with-important-003.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 3, 3, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 3, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 3, 3, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 3, 2, 1, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3, 3, 3, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 2, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 3, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], [0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 3, 1, 0, 1, 0], [0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 3, 0, 2, 2, 2, 1, 2, 2, 2, 3, 2, 1, 2, 0], [0, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 0], [0, 2, 3, 2, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 0], [0, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 0], [0, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0], [0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 0], [0, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[24,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tevr top like business programming can network cable network connections to taking of their gad and what they play on them. but it might not suit everyone the panel recognised. older generations are more comfortable with familiar schedules and channel because they know what they are getting. they perhaps do not want so much of the choice put into their hands mr hanlon suggested. on the other end you have the just out of dis who are pushing already - everything is possible and available to them said mr hanlon. ultimately the will tell the market they want. of the 50 000 new gad and being showcased at ces many of them are about enhancing the tv - watching experience. high - tv sets are everywhere and many new of lcd ( ) tvs have been launched with dvr into them instead of being. one such launched at the is hum s 26 - inch lcd tv with an 80 - hour t dvr and dvd. one of the us s biggest tv directtv has even launched its own dvr at the with 100 - hours of replay and a. the set can pause and rewind tv for up to 90 hours. andsoft chief bill announced in his pre - speech a partnership with t called ttogo which means people can play recorded programmes on pcs and. all these reflect the increasing trend of up so that people can watch what they want when they want.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.iloc[0,5]\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score = 0.02 0.04 0.06 pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "datapath = '../bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "\n",
    "df[\"text_separate\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"important_labels\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"most_important\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"second_important\"] = [[] for _ in range(len(df.index))]\n",
    "df[\"third_important\"] = [[] for _ in range(len(df.index))]\n",
    "df.to_pickle(\"bbc-text-with-important-2_4_6.pkl\")\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for str in df.iloc[i,1].split('. '):\n",
    "        if '! ' in str:\n",
    "            for idx, str_e in enumerate(str.split('! ')):\n",
    "                if idx == len(str.split('! ')) - 1:\n",
    "                    df.iloc[i,2].append(str_e + '!')\n",
    "                else:\n",
    "                    df.iloc[i,2].append(str_e + '.')\n",
    "        elif '? ' in str:\n",
    "            for idx, str_q in enumerate(str.split('? ')):\n",
    "                if idx == len(str.split('? ')) - 1:\n",
    "                    df.iloc[i,2].append(str_q + '?')\n",
    "                else:\n",
    "                    df.iloc[i,2].append(str_q + '.')\n",
    "        else:\n",
    "            df.iloc[i,2].append(str + '.')\n",
    "    df.iloc[i,2][-1] = df.iloc[i,2][-1][:-1]\n",
    "df.to_pickle(\"bbc-text-with-important-2_4_6.pkl\")\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sentences.append(df.iloc[i, 1])\n",
    "\n",
    "most_threshold = 0.02\n",
    "second_threshold = 0.04\n",
    "third_threshold = 0.06\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [5:48:09<00:00,  9.39s/it]  \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tok_SEP = tokenizer(\"[SEP]\", add_special_tokens=False, return_tensors='pt')\n",
    "tok_SEP_ids = tok_SEP['input_ids'][0]\n",
    "att_mask_1 = tok_SEP['attention_mask'][0]\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    inputs_text = tokenizer(df.iloc[i,1], padding='max_length', \\\n",
    "                            max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    model.to(device)\n",
    "    inputs_text = inputs_text.to(device)\n",
    "    outputs_sentence = model(**inputs_text)\n",
    "\n",
    "    logits_sentence = outputs_sentence.logits\n",
    "    probs_sentence = F.softmax(logits_sentence, dim=-1)\n",
    "    pred_label_sentence = torch.argmax(probs_sentence, dim=-1).item()\n",
    "    confidence_sentence = probs_sentence[0][pred_label_sentence].item()\n",
    "\n",
    "    sentence2_idx = 0\n",
    "    sentence2_input = torch.full(inputs_text['input_ids'].shape, 0)\n",
    "    sentence2_tkn_type = torch.full(inputs_text['token_type_ids'].shape, 0)\n",
    "    sentence2_att_mask = torch.full(inputs_text['attention_mask'].shape, 0)\n",
    "\n",
    "    sen_i = 0\n",
    "    word_i = 0\n",
    "    sen_num = len(df.iloc[i,2])\n",
    "    sen_label = []\n",
    "    \n",
    "    inputs_sentence = tokenizer(df.iloc[i,2][sen_i], padding='max_length', \\\n",
    "                                max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    while True:\n",
    "        token_now = inputs_sentence['input_ids'][0][word_i]\n",
    "        token_now_int = token_now.item()\n",
    "        \n",
    "        if sentence2_idx == 511:\n",
    "            for s2_idx in range(1, 510):\n",
    "                sentence2_input[0][s2_idx] = sentence2_input[0][s2_idx + 1].detach().clone()\n",
    "            sentence2_idx -= 1\n",
    "        sentence2_input[0][sentence2_idx] = token_now.detach().clone()\n",
    "        sentence2_att_mask[0][sentence2_idx] = att_mask_1.detach().clone()\n",
    "            \n",
    "        if sen_i == 0 and word_i == 0:\n",
    "            sen_label.append(0)\n",
    "            word_i += 1\n",
    "            sentence2_idx += 1\n",
    "            continue\n",
    "          \n",
    "        sentence2_input[0][sentence2_idx + 1] = tok_SEP_ids.detach().clone()\n",
    "        sentence2_att_mask[0][sentence2_idx + 1] = att_mask_1.detach().clone()\n",
    "        sentence2_input = sentence2_input.to(device)\n",
    "        sentence2_tkn_type = sentence2_tkn_type.to(device)\n",
    "        sentence2_att_mask = sentence2_att_mask.to(device)\n",
    "        \n",
    "        outputs_sentence2 = model(input_ids=sentence2_input, attention_mask=sentence2_att_mask, \\\n",
    "                                  token_type_ids=sentence2_tkn_type)\n",
    "\n",
    "        logits_sentence2 = outputs_sentence2.logits\n",
    "        probs_sentence2 = F.softmax(logits_sentence2, dim=-1)\n",
    "        pred_label_sentence2 = torch.argmax(probs_sentence2, dim=-1).item()\n",
    "        confidence_sentence2 = probs_sentence2[0][pred_label_sentence2].item()\n",
    "\n",
    "        if pred_label_sentence != pred_label_sentence2:\n",
    "            sen_label.append(0)\n",
    "            sentence2_idx += 1\n",
    "        else:\n",
    "            score = abs(confidence_sentence - confidence_sentence2)\n",
    "            \n",
    "            if score <= most_threshold:\n",
    "                sen_label.append(3)\n",
    "                df.iloc[i, 4].append(token_now_int)\n",
    "            elif score <= second_threshold:\n",
    "                sen_label.append(2)\n",
    "                df.iloc[i, 5].append(token_now_int)\n",
    "            elif score <= third_threshold:\n",
    "                sen_label.append(1)\n",
    "                df.iloc[i, 6].append(token_now_int)\n",
    "            else:\n",
    "                sen_label.append(0)\n",
    "                sentence2_idx += 1\n",
    "        word_i += 1\n",
    "        if inputs_sentence['input_ids'][0][word_i].item() == 102:\n",
    "            sen_label.append(0)\n",
    "            df.iloc[i, 3].append(sen_label)\n",
    "            sen_label = [0]\n",
    "            if sen_i == (sen_num - 1):\n",
    "                break\n",
    "            else:\n",
    "                word_i = 1\n",
    "                sen_i += 1\n",
    "                inputs_sentence = tokenizer(df.iloc[i,2][sen_i], padding='max_length', \\\n",
    "                                            max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "df.to_pickle(\"bbc-text-with-important-2_4_6.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, 3, 1, 0, 3, 0, 3, 1, 3, 3, 3, 0, 0, 0, 0, 3, 3, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 3, 3, 0, 2, 1, 0, 0, 0, 0, 0, 0], [1, 2, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 3, 2, 1, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 3, 2, 2, 3, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 3, 2, 2, 2, 3, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2], [2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2], [2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2], [2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2], [2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'videov set top like allows programming can network networks viewer channels network brands communications connections content model generation their gad and what they play on them. but it might not everyone the recognised. older generations are more comfortable with familiar schedules and channel because they know what they are getting. they perhaps do not want so much of the choice put into their hands mr hanlon suggested. on the other end you have the kids just out of dis who are pushing already - everything is possible and to them said mr hanlon. ultimately the will tell the they want. of the 50 000 new gad and being showcased at ces many of them are about enhancing the tv - watching experience. high - tv are everywhere and many new of lcd ( ) tvs have been with dvr into them instead of being. one such at the show is hum s 26 - inch lcd tv with an - hour t dvr and dvd. one of the us s biggest tvtv has even its own dvr at the show with 100 - hours of and a. the set pause and rewind tv for up to 90 hours. andso chief bill announced in his pre - show speech a partnership with t called ttogo which means people play recorded programmes on p and. all these reflect the increasing of up so that people watch what they want when they want.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.iloc[0,3])\n",
    "test = df.iloc[0,5]\n",
    "text1 = tokenizer.decode(test)\n",
    "text1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT_Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
