{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkout the dataset\n",
    "Dataset: [bbc dataset][def]  \n",
    "- 根據文章片段，分類該文章是屬於什麼領域的報導\n",
    "- 總共有五種分類：entertainment, sport, tech, business, politics\n",
    "- 資料總筆數：2225\n",
    "\n",
    "我們可以從下方圖表得知：  \n",
    "* 此資料集有五種類別，即是等等 BERT 判斷分類的 Label  \n",
    "* 而每筆 Text 就是 BERT 的 input 資料\n",
    "\n",
    "目的就是判斷此 Text 的類別 (Label) 是什麼  \n",
    "\n",
    "[def]: https://www.kaggle.com/datasets/sainijagjit/bbc-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production.   The filming t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production.   The filming t...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datapath = 'IMDB Dataset.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data (資料前處理)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BertTokenizer\n",
    "[**BertTokenizer**](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer):\n",
    "以 Word-based 的方式做 Tokenization，並加上 \\[CLS\\]、\\[SEP\\]、\\[PAD\\] 這三個特殊 token  \n",
    "使用方式為： ```BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)```   \n",
    "其中，```PRETRAINED_MODEL_NAME``` 可依需求尋找適合任務的預訓練模型 ([可用模型總攬](https://huggingface.co/models))  \n",
    "本例子使用常見的英文模型 ```'bert-base-uncased'```\n",
    "\n",
    "BertTokenizer 繼承於 [PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中較常使用的參數如下：\n",
    "\n",
    "* ```text``` / ```text_pair ```: 要做 Tokenize 的序列 / 要做 Tokenize 的一對序列\n",
    "* ```padding```: 選擇要不要補 padding (\\[PAD\\])  \n",
    "  * ```True``` or ```'longest'```: 做 padding 直到序列長度等於 batch 中最長的序列\n",
    "  * ```'max_length'```: padding 直到設定的值 'max_length'\n",
    "  * ```False``` or ```'do_not_pad'```: 不做 padding\n",
    "* ```max_length```: 設定序列的最長長度 (BERT 中是 512) \n",
    "* ```truncation```: 若序列(序列對)長度總和大於上限，是否截斷序列\n",
    "  * ```True``` or ```'longest_first'```: 以迭代的方式，從最長的序列開始慢慢截斷。  \n",
    "    ex:  \n",
    "    seq1 = \"a b c\", seq2 = \"d e\", max_length = 2。因為 len(seq1) + len(seq2) > 2，需要要做截斷  \n",
    "    (註：這邊先忽略 \\[CLS\\]、\\[SEP\\]、\\[PAD\\] 這些 Token)  \n",
    "    迭代1：seq1 = \"a b\", seq2 = \"d e\"  \n",
    "    迭代2：seq1 = \"a\", seq2 = \"d e\"  \n",
    "    迭代3：seq1 = \"a\", seq2 = \"d\" => 最後輸出 \"a d\"  \n",
    "  * ```'only_first'```: 只截斷一對序列對中的第一個序列\n",
    "  * ```'only_second'```: 只截斷一對序列對中的第二個序列\n",
    "  * ```False``` or ```'do_not_truncate'```: 不做截斷\n",
    "* ```return_tensors```: 決定回傳張量的資料型態\n",
    "  * ```'tf'```: TensorFlow tf.constant.\n",
    "  * ```'pt'```: PyTorch torch.Tensor.\n",
    "  * ```'np'```: Numpy np.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Class\n",
    "1. 決定要用什麼模型的 tokenizer  \n",
    "2. 給定對應到 Dataset 類別的標籤 (labels)，即將資料集中的各類別對應到 id (數值)  \n",
    "   這邊將 ```'business'```, ```'entertainment'```, ```'sport'```, ```'tech'```, ```'politics'``` 對應到 0 ~ 4，\n",
    "3. 初始化 Dataset 時：\n",
    "   - 各筆資料的分類 (category) 轉換成 id\n",
    "   - 再把資料集丟進 BertTokenizer，轉換成 BERT 可以使用的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 決定 tokenizer 類型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# 決定資料集中各分類對應的 id\n",
    "labels = {'negative':0,\n",
    "          'positive':1,\n",
    "          }\n",
    "\n",
    "# 資料集處理\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        # 把每一筆資料的類別改成 id\n",
    "        self.labels = [labels[label] for label in df['sentiment']]  \n",
    "        # 對每筆資料做 BERT tokenize\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['review']]\n",
    "\n",
    "    # 回傳資料集各類別 (id)\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    # 回傳該 label 的資料數\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # 取得當前資料的 label\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    # 取得當前資料的 text\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分 訓練 (training)、驗證 (validation)、測試 (testing) 資料集   \n",
    "這邊切分比例為 80:10:10  \n",
    "\n",
    "註：\n",
    "1. ```df.sample(frac=1, random_state=42)```：從 dataframe 隨機取樣  \n",
    "   ```frac```：要抽取 dataframe 的比例 (0 ~ 1)  \n",
    "   ```random_state```：隨機的狀態，可以想樣乘亂數表的位置 (？)  \n",
    "2. ```np.split(..., [int(.6*len(df)), int(.8*len(df))])```：把陣列做分割  \n",
    "   上述第二個參數的 list，代表分別取道的相對位置，分割個數 = 該 list 的長度，以本例子來說：  \n",
    "   第一區塊：```int(.6*len(df))```：取到 df 的 60 % => 分割的第一組陣列是 0% ~ 60% 的 df  \n",
    "   第二區塊：```int(.8*len(df))```：取到 df 的 80 % => 分割的第一組陣列是 60% ~ 80% 的 df  \n",
    "   第三區塊：分割的第一組陣列是 80% ~ 100% 的 df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "[**BertModel**](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)：主要是 transformer 的 Encoder 部分  \n",
    "繼承於 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/model#transformers.PreTrainedModel)，在 ```forward```中較常使用的參數如下：  \n",
    "* ```input_ids```: 輸入序列  \n",
    "* ```attention_mask```: 輸入序列對應的 mask，以告知 attention 略過他們  \n",
    "  1: 該 token 是 真實的字、\\[CLS\\]、\\[SEP\\]  \n",
    "  0: 該 token 是 \\[PAD\\]，即要被 mask 掉的部分\n",
    "* ```return_dict ```: True: 回傳值是 [ModelOutput](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/output#transformers.utils.ModelOutput)；False: 回傳值是 tuple\n",
    "\n",
    "若 ```return_dict = True```，BertModel 的輸出為 [transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)  \n",
    "而範例中，```return_dict = False```，BertModel 的輸出為 ```tuple(torch.FloatTensor)```，範例以 ```_``` 與 ```pooled_output``` 表示：\n",
    "* ```_```: sequence output，為序列在 BERT 最後一層 hidden layer 的輸出\n",
    "* ```pooled_output```: 取出 BERT 最後一層 layer中，\\[CLS\\] 對應的向量 (vector)  \n",
    "由上可知，BERT 可以處理序列任務 (如輸出```_```) 及 分類、迴歸分析任務 (如輸出```pooled_output```)\n",
    "\n",
    "我們把 ```pooled_output``` 經過 dropout、線性轉換、ReLU 激活函數後，我們在最後的線性層可以得到維度為 5 的向量，代表著我們資料及的分類 (sport, business, politics, entertainment, and tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5, path = \"\", pretrained_model = \"\"):\n",
    "        super(BertClassifier, self).__init__()                      # 繼承 nn.Module\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model, num_labels=2)    # 選擇 BertModel\n",
    "        self.dropout = nn.Dropout(dropout)                          # dropout = 0.5 => 去掉 50% neural，避免 overfitting\n",
    "        self.linear = nn.Linear(768, 2)                             # BERT Base size: 768，每句有 2 種可能的分類要做選擇  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.path = path\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.bert.save_pretrained(self.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "* Epoch: 4\n",
    "* Loss Function: Categorical cross entropy (因為要做複數類別分類)\n",
    "* Optimizer: Adam\n",
    "* Learning Rate: 10<sup>-6</sup> (1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, model_name):\n",
    "\n",
    "    if os.path.isfile(\"fine_tune_record_IMDB.csv\"):\n",
    "        rec = pd.read_csv(\"fine_tune_record_IMDB.csv\")\n",
    "    else:\n",
    "        rec = pd.DataFrame({\"model_name\":[], \"train_acc\":[], \"train_loss\":[], \"train_Recall\":[], \"train_precision\":[], \"train_f1\":[],\\\n",
    "                             \"val_acc\":[], \"val_loss\":[], \"val_Recall\":[], \"val_precision\":[], \"val_f1\":[]})\n",
    "\n",
    "    # 把原本的資料經過 Dataset 類別包裝起來\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    # 把訓練、驗證資料集丟進 Dataloader 定義取樣資訊 (ex: 設定 batch_size...等等)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=3, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=3)\n",
    "\n",
    "    # 偵測有 GPU，有就用\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()                       # Loss Function: Categorical cross entropy\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate) # Optimizer: Adam\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    train_recall = []\n",
    "    train_precision = []\n",
    "    train_f1 = []\n",
    "    val_acc = []\n",
    "    val_loss = []\n",
    "    val_recall = []\n",
    "    val_precision = []\n",
    "    val_f1 = []\n",
    "    # 每次完整訓練 (每個 epoch) 要做的事\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            # ---------- 訓練的部分 ----------\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            all_pred_train = []\n",
    "            all_label_train = []\n",
    "\n",
    "            # 這邊加上 tqdm 模組來顯示 dataloader 處理進度條\n",
    "            # 所以在程式意義上，可以直接把這行當作 for train_input, train_label in train_dataloader:\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                # .to(device): 把東西 (tensor) 丟到 GPU 的概念\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                # 把 data 丟進 BERT\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                # 計算 Cross Entropy，以此計算 loss\n",
    "                batch_loss = criterion(output, train_label.long())  # 參數解釋：(模型的輸出, 原本預計的輸出)\n",
    "                total_loss_train += batch_loss.item()               # .item(): tensor 轉 純量\n",
    "                \n",
    "                # 看 model output \"可能性最高\" 的 label 是不是和 data 一樣，是的話，acc + 1\n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                all_pred_train += output.argmax(dim=1)\n",
    "                all_label_train += train_label\n",
    "                \n",
    "                recall_train = recall_score(y_true=all_label_train, y_pred=all_pred_train)\n",
    "                f1_train = f1_score(y_true=all_label_train, y_pred=all_pred_train)\n",
    "                precision_train = precision_score(y_true=all_label_train, y_pred=all_pred_train)\n",
    "\n",
    "                model.zero_grad()       # 清空前一次 Gradient\n",
    "                batch_loss.backward()   # 根據 lost 計算 back propagation\n",
    "                optimizer.step()        # 做 Gradient Decent\n",
    "            \n",
    "            # ---------- 驗證的部分 ----------\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            all_pred_val = []\n",
    "            all_label_val = []\n",
    "\n",
    "            # 步驟和訓練時差不多，差在沒做 Gradient Decent\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "\n",
    "                    all_pred_val += output.argmax(dim=1)\n",
    "                    all_label_val += train_label\n",
    "                    \n",
    "                    recall_val = recall_score(y_true=all_label_val, y_pred=all_pred_val)\n",
    "                    f1_val = f1_score(y_true=all_label_val, y_pred=all_pred_val)\n",
    "                    precision_val = precision_score(y_true=all_label_val, y_pred=all_pred_val)\n",
    "            \n",
    "            train_loss.append(total_loss_train / len(train_data))\n",
    "            train_acc.append(total_acc_train / len(train_data))\n",
    "            train_recall.append(recall_train)\n",
    "            train_precision.append(precision_train)\n",
    "            train_f1.append(f1_train)\n",
    "            val_loss.append(total_loss_val / len(val_data))\n",
    "            val_acc.append(total_acc_val / len(val_data))\n",
    "            val_recall.append(recall_val)\n",
    "            val_precision.append(precision_val)\n",
    "            val_f1.append(f1_val)\n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Train Recall: {recall_train: .3f} \\\n",
    "                | Train precision: {precision_train: .3f} \\\n",
    "                | Train F1: {f1_train: .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}\\\n",
    "                | Val Recall: {recall_val: .3f} \\\n",
    "                | Val precision: {precision_val: .3f} \\\n",
    "                | Val F1: {f1_val: .3f}')\n",
    "            \n",
    "    rec = pd.DataFrame({\"model_name\":[], \"train_acc\":[], \"train_loss\":[], \"train_Recall\":[], \"train_precision\":[], \"train_f1\":[],\\\n",
    "                             \"val_acc\":[], \"val_loss\":[], \"val_Recall\":[], \"val_precision\":[], \"val_f1\":[]})\n",
    "    \n",
    "    new_rec = pd.concat([rec, pd.DataFrame(pd.DataFrame({'model_name': model_name,\\\n",
    "                                                         'train_acc': [train_acc], 'train_loss': [train_loss], \"train_Recall\":[train_recall], \"train_precision\":[train_precision], \"train_f1\":[train_f1],\\\n",
    "                                                         'val_acc': [val_acc], 'val_loss': [val_loss], \"val_Recall\":[val_recall], \"val_precision\":[val_precision], \"val_f1\":[val_f1]}))], ignore_index=True)\n",
    "    new_rec.to_csv(\"fine_tune_record_IMDB.csv\", index = None)\n",
    "    model.save_model()\n",
    "\n",
    "EPOCHS = 8\n",
    "model = BertClassifier(path=\"fine_tuned_model/saved_model_mask_dyn10\", pretrained_model = '../saved_model/saved_model_mask_grow')\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS, \"mask_dyn10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at saved_model/saved_model_mask15 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 297/297 [01:27<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.272                 | Train Accuracy:  0.255                 | Val Loss:  0.263                 | Val Accuracy:  0.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:28<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.245                 | Train Accuracy:  0.388                 | Val Loss:  0.220                 | Val Accuracy:  0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:27<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.198                 | Train Accuracy:  0.599                 | Val Loss:  0.171                 | Val Accuracy:  0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:27<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.150                 | Train Accuracy:  0.753                 | Val Loss:  0.123                 | Val Accuracy:  0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:28<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.099                 | Train Accuracy:  0.874                 | Val Loss:  0.068                 | Val Accuracy:  0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:28<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.058                 | Train Accuracy:  0.947                 | Val Loss:  0.039                 | Val Accuracy:  0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:29<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.036                 | Train Accuracy:  0.968                 | Val Loss:  0.028                 | Val Accuracy:  0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:29<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.024                 | Train Accuracy:  0.980                 | Val Loss:  0.019                 | Val Accuracy:  0.982\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "model = BertClassifier(path=\"fine_tuned_model/saved_model_mask15\", pretrained_model = 'saved_model/saved_model_mask15')\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS, \"mask15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at saved_model/saved_model_mask6 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 297/297 [01:27<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.271                 | Train Accuracy:  0.235                 | Val Loss:  0.261                 | Val Accuracy:  0.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:28<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.256                 | Train Accuracy:  0.314                 | Val Loss:  0.249                 | Val Accuracy:  0.338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:28<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.219                 | Train Accuracy:  0.520                 | Val Loss:  0.192                 | Val Accuracy:  0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:28<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.152                 | Train Accuracy:  0.795                 | Val Loss:  0.101                 | Val Accuracy:  0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:28<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.082                 | Train Accuracy:  0.951                 | Val Loss:  0.067                 | Val Accuracy:  0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:27<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.052                 | Train Accuracy:  0.971                 | Val Loss:  0.045                 | Val Accuracy:  0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:26<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.036                 | Train Accuracy:  0.979                 | Val Loss:  0.036                 | Val Accuracy:  0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:27<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.027                 | Train Accuracy:  0.981                 | Val Loss:  0.027                 | Val Accuracy:  0.968\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "model = BertClassifier(path=\"fine_tuned_model/saved_model_mask6\", pretrained_model = 'saved_model/saved_model_mask6')\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS, \"mask6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at saved_model/saved_model_mask_dyn_grow1 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 297/297 [01:25<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.250                 | Train Accuracy:  0.339                 | Val Loss:  0.217                 | Val Accuracy:  0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:25<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.218                 | Train Accuracy:  0.475                 | Val Loss:  0.199                 | Val Accuracy:  0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:26<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.194                 | Train Accuracy:  0.593                 | Val Loss:  0.167                 | Val Accuracy:  0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:26<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.156                 | Train Accuracy:  0.710                 | Val Loss:  0.130                 | Val Accuracy:  0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:26<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.123                 | Train Accuracy:  0.780                 | Val Loss:  0.099                 | Val Accuracy:  0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:26<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.090                 | Train Accuracy:  0.882                 | Val Loss:  0.066                 | Val Accuracy:  0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:26<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.053                 | Train Accuracy:  0.971                 | Val Loss:  0.039                 | Val Accuracy:  0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [01:27<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.031                 | Train Accuracy:  0.987                 | Val Loss:  0.029                 | Val Accuracy:  0.959\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "model = BertClassifier(path=\"fine_tuned_model/saved_model_mask_dyn_grow1\", pretrained_model = 'saved_model/saved_model_mask_dyn_grow1')\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS, \"mask_dyn_grow1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model on Test Data\n",
    "把最後 10% 的 Unseen Data 拿來做測試  \n",
    "程式流程基本上和驗證差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.982\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    # 把那 10% unseen data 拿來用\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=3)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # 不用做 loss 了，所以不用丟 criterion 進去\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    \n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊再做個極端點的測試  \n",
    "這邊再丟一個 [**新的 BBC 資料集**](https://www.kaggle.com/datasets/chalikamihiran/bbc-news-classification) 當作測試資料，看正確率會不會還是那麼高  \n",
    "(註：該資料集有先經過處理，取 'text' 與 'category' 部分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.987\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"new_test/BBC_News_processed.csv\")\n",
    "evaluate(model, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('BERT_Practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684b83e2f83316061361748e41b2620a10a3e9a8f2545480c20c18cf426689ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
