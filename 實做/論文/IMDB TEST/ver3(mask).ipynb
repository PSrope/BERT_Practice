{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定 Tokenizer 與使用 BertForPretraining 來做 BERT 預訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining, AdamW\n",
    "from transformers.models.bert.modeling_bert import BertForPreTrainingOutput, BertPreTrainingHeads, BertConfig, BERT_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "from transformers.models.albert.modeling_albert import AlbertSOPHead\n",
    "from transformers.utils import ModelOutput\n",
    "from transformers.utils.doc import add_start_docstrings_to_model_forward, replace_return_docstrings\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTrainingOutput(BertForPreTrainingOutput):\n",
    "    \"\"\"\n",
    "    Output type of [`MyBertForPreTraining`].\n",
    "    Args:\n",
    "        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        mlm_loss (`float`):\n",
    "            MLM loss.\n",
    "        nsp_loss (`float`):\n",
    "            NSP loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, loss=None, prediction_logits=None, seq_relationship_logits=None, hidden_states=None, attentions=None, mlm_loss=None, nsp_loss=None):\n",
    "        super().__init__(loss=loss, prediction_logits=prediction_logits, seq_relationship_logits=seq_relationship_logits, hidden_states=hidden_states, attentions=attentions)\n",
    "        self.mlm_loss = mlm_loss\n",
    "        self.nsp_loss = nsp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlbertSOPHead(torch.nn.Module):\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size , config.num_labels)\n",
    "\n",
    "    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n",
    "        dropout_pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPretrainingHeadsWithSOP(BertPreTrainingHeads):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.seq_relationship = MyAlbertSOPHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTraining(BertForPreTraining):\n",
    "    def __init__(self, config, nspTask = \"NSP\"):\n",
    "        super().__init__(config)\n",
    "        if nspTask == \"SOP\":\n",
    "            self.cls = BertPretrainingHeadsWithSOP(config)\n",
    "            \n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=MyBertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MyBertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return MyBertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            mlm_loss = masked_lm_loss,\n",
    "            nsp_loss = next_sentence_loss,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取出資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getData():\n",
    "    def __init__(self, modelType, datapath, nspTask = \"NSP\"):\n",
    "        self.datapath = datapath\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.nspTask = nspTask\n",
    "        self.text = self.toText()\n",
    "        self.sentence_a = []\n",
    "        self.sentence_b = []\n",
    "        self.label = []\n",
    "        self.inputs = None\n",
    "        self.nspPrepare()\n",
    "        self.inputs['labels'] = self.inputs.input_ids.detach().clone()\n",
    "    \n",
    "    def toText(self):\n",
    "        df = pd.read_csv(self.datapath)\n",
    "        text = []\n",
    "        for review in df[\"review\"]:\n",
    "            text.append(review)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def nspPrepare(self):\n",
    "        bag = []\n",
    "        for sentence in self.text:\n",
    "            for s_str in sentence.split('.<br /><br />'):\n",
    "                if '. ' in s_str:\n",
    "                    bag.extend(s_str.split('. '))\n",
    "                elif '!' in s_str:\n",
    "                    bag.extend(s_str.split('!'))\n",
    "                elif '?' in s_str:\n",
    "                    bag.extend(s_str.split('?'))\n",
    "                else:\n",
    "                    bag.append(s_str)\n",
    "        bag_size = len(bag)\n",
    "\n",
    "        if self.nspTask == \"NSP\":\n",
    "            self.nspData(bag, bag_size)\n",
    "        elif self.nspTask == \"SOP\":\n",
    "            self.sopData()\n",
    "\n",
    "        self.inputs = self.tokenizer(self.sentence_a, self.sentence_b, return_tensors='pt',\n",
    "                   max_length=512, truncation='longest_first', padding='max_length')\n",
    "        self.inputs['next_sentence_label'] = torch.LongTensor([self.label]).T\n",
    "    \n",
    "    def nspData(self, bag, bag_size):\n",
    "        for paragraph in self.text:\n",
    "            sentences = []\n",
    "            for s_str in paragraph.split('.  '):\n",
    "                if '!' in s_str:\n",
    "                    sentences.extend(s_str.split('!'))\n",
    "                elif '?' in s_str:\n",
    "                    sentences.extend(s_str.split('?'))\n",
    "                elif ';' in s_str:\n",
    "                    sentences.extend(s_str.split(';'))\n",
    "                else:\n",
    "                    sentences.append(s_str)\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    self.sentence_a.append(sentences[start])\n",
    "                    self.sentence_b.append(sentences[start+1])\n",
    "                    self.label.append(0)\n",
    "                else:\n",
    "                    index = random.randint(0, bag_size-1)\n",
    "                    # this is NotNextSentence\n",
    "                    self.sentence_a.append(sentences[start])\n",
    "                    self.sentence_b.append(bag[index])\n",
    "                    self.label.append(1)\n",
    "    \n",
    "    def sopData(self):\n",
    "        for paragraph in self.text:\n",
    "            sentences = [\n",
    "                sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "            ]\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    self.sentence_a.append(sentences[start])\n",
    "                    self.sentence_b.append(sentences[start+1])\n",
    "                    self.label.append(0)\n",
    "                else:\n",
    "                    # this is NotNextSentence\n",
    "                    self.sentence_a.append(sentences[start+1])\n",
    "                    self.sentence_b.append(sentences[start])\n",
    "                    self.label.append(1)\n",
    "    \n",
    "    def returnInput(self):\n",
    "        return self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel():\n",
    "    def __init__(self, modelType, inputs, batch_size, epoch, acc_goal_each_epoch, masking_method = \"propose\", saveModelName = \"\", saveCSV = True, nspTask = \"NSP\", epoch_now = 0):\n",
    "        self.model = MyBertForPreTraining.from_pretrained(modelType)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.acc_goal_each_epoch = acc_goal_each_epoch  # 每個 epoch 的 MLM 正確率基準\n",
    "        self.masking_method = masking_method\n",
    "        self.saveModelName = saveModelName\n",
    "        self.saveCSV = saveCSV\n",
    "        self.loader = torch.utils.data.DataLoader(OurDataset(self.inputs), \\\n",
    "                                             batch_size=self.batch_size, shuffle=True)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        self.optim = AdamW(self.model.parameters(), lr = 5e-5)\n",
    "        self.epoch_now = epoch_now\n",
    "        \n",
    "        if os.path.isfile(\"record_mask_grow.csv\"):\n",
    "            self.rec = pd.read_csv(\"record_mask_grow.csv\")\n",
    "        else:\n",
    "            self.rec = pd.DataFrame({\"mlm_acc_each_epoch\":[], \"mlm_loss_each_epoch\":[], 'Mask_Percent_each_epoch':[]})\n",
    "            \n",
    "        self.training()\n",
    "        self.save_model(self.saveModelName)\n",
    "    \n",
    "    def mlmPrepare(self, input_sentences, maskPercentNow):\n",
    "        rand = torch.rand(input_sentences.shape)\n",
    "        # create mask array\n",
    "        mask_arr = (rand < maskPercentNow * 0.01) * (input_sentences != 101) * \\\n",
    "                (input_sentences != 102) * (input_sentences != 0)\n",
    "        \n",
    "        selection = []\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            selection.append(\n",
    "                torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            )\n",
    "\n",
    "        rand_mask_type = copy.deepcopy(selection)\n",
    "\n",
    "        for row in range(len(rand_mask_type)):\n",
    "            for col in range(len(rand_mask_type[row])):\n",
    "                rand_mask_type[row][col] = random.random()\n",
    "\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        special_tokens = [vocab['[CLS]'], vocab['[SEP]'], vocab['[MASK]'], vocab['[UNK]'],  vocab['[PAD]']]\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            for j in range(len(selection[i])):\n",
    "                if rand_mask_type[i][j] < 0.10:\n",
    "                    continue\n",
    "                elif rand_mask_type[i][j] < 0.20:\n",
    "                    rand_num = vocab['[CLS]']\n",
    "                    while rand_num in special_tokens:\n",
    "                        rand_num = random.randint(1, vocab_size-1)\n",
    "                    input_sentences[i, selection[i][j]] = rand_num\n",
    "                else:\n",
    "                    input_sentences[i, selection[i][j]] = 103\n",
    "        \n",
    "        return input_sentences, mask_arr\n",
    "\n",
    "    def training(self):\n",
    "        acc_each_epoch = []\n",
    "        loss_each_epoch = []\n",
    "        Mask_Percent_each_epoch = []\n",
    "        stay = 0\n",
    "        percent_now = 6\n",
    "\n",
    "        for epoch in range(self.epoch_now, self.epoch):\n",
    "            # setup loop with TQDM and dataloader\n",
    "            mask_nums = 0\n",
    "            mlm_correct = 0\n",
    "            nsp_nums = 0\n",
    "            nsp_correct = 0\n",
    "            loop = tqdm(self.loader, leave=True)\n",
    "\n",
    "            for batch in loop:\n",
    "                input_sentences, mask_arr = self.mlmPrepare(batch[\"input_ids\"].detach().clone(), percent_now)\n",
    "\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                self.optim.zero_grad()\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = input_sentences.to(self.device)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                next_sentence_label = batch['next_sentence_label'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                # process\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                next_sentence_label=next_sentence_label,\n",
    "                                labels=labels)\n",
    "                \n",
    "                prediction_logits = outputs.prediction_logits[mask_arr]\n",
    "                predicted_ids = prediction_logits.argmax(-1)\n",
    "                \n",
    "                seq_relationship_logits = outputs.seq_relationship_logits\n",
    "                predicted_labels = torch.argmax(seq_relationship_logits, dim=1)\n",
    "                predicted_label = predicted_labels\n",
    "\n",
    "                mask_nums += len(predicted_ids)\n",
    "                mlm_correct += torch.eq(predicted_ids, labels[mask_arr]).sum().item()\n",
    "                nsp_nums += len(predicted_label)\n",
    "                nsp_correct += predicted_label.eq(torch.squeeze(next_sentence_label)).sum().item()\n",
    "                \n",
    "                # extract loss\n",
    "                loss = outputs.loss\n",
    "                mlm_loss = outputs.mlm_loss.item()\n",
    "                nsp_loss = outputs.nsp_loss.item()\n",
    "                mlm_acc = mlm_correct / mask_nums\n",
    "                nsp_acc = nsp_correct / nsp_nums\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                self.optim.step()\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(Total_loss='{:.4f}'.format(loss.item()), MLM_Accuracy='{:.4f}'.format(mlm_acc), NSP_Accuracy='{:.4f}'.format(nsp_acc), \\\n",
    "                                MLM_loss='{:.4f}'.format(mlm_loss), NSP_loss='{:.4f}'.format(nsp_loss), Mask_Percent=percent_now)\n",
    "\n",
    "            acc_each_epoch.append(mlm_acc)\n",
    "            loss_each_epoch.append(mlm_loss)\n",
    "            Mask_Percent_each_epoch.append(percent_now)\n",
    "\n",
    "            if self.masking_method == \"DMLM\":\n",
    "                percent_now += 1\n",
    "            elif self.masking_method == \"propose\":\n",
    "                if (mlm_acc >= self.acc_goal_each_epoch[epoch] * 0.01) or stay >= 2:\n",
    "                    stay = 0\n",
    "                    percent_now = 6 + epoch + 1\n",
    "                else:\n",
    "                    stay += 1\n",
    "            \n",
    "            if epoch % 2 == 1:\n",
    "                self.save_model(self.saveModelName + \"_epoch\" + str(epoch + 1))\n",
    "\n",
    "        if self.saveCSV:\n",
    "            \n",
    "            new_rec = pd.concat([self.rec, pd.DataFrame(pd.DataFrame({'mlm_acc_each_epoch': [acc_each_epoch], 'mlm_loss_each_epoch': [loss_each_epoch], 'Mask_Percent_each_epoch': [Mask_Percent_each_epoch]}))], ignore_index=True)\n",
    "            new_rec.to_csv(\"record_mask_grow.csv\", index = False)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        self.model.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "datapath = 'IMDB Dataset.csv'\n",
    "modelType = 'bert-base-uncased'\n",
    "epoch = 10\n",
    "batch_size = 8\n",
    "nsp_input = getData(modelType = modelType, datapath = datapath, nspTask = \"NSP\")\n",
    "epoch_acc = [48.7, 47.2, 45.4, 44.1, 43.0, 42.3, 41.5, 41.1, 40.8, 40.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/6207 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_9800\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 6207/6207 [57:39<00:00,  1.79it/s, MLM_Accuracy=0.4918, MLM_loss=0.0095, Mask_Percent=6, NSP_Accuracy=0.7235, NSP_loss=0.6853, Total_loss=0.6948] \n",
      "Epoch 1: 100%|██████████| 6207/6207 [58:09<00:00,  1.78it/s, MLM_Accuracy=0.4663, MLM_loss=0.0257, Mask_Percent=7, NSP_Accuracy=0.8029, NSP_loss=0.2006, Total_loss=0.2262] \n",
      "Epoch 2: 100%|██████████| 6207/6207 [57:59<00:00,  1.78it/s, MLM_Accuracy=0.4441, MLM_loss=0.0325, Mask_Percent=7, NSP_Accuracy=0.8711, NSP_loss=0.1801, Total_loss=0.2126]\n",
      "Epoch 3: 100%|██████████| 6207/6207 [57:57<00:00,  1.78it/s, MLM_Accuracy=0.4224, MLM_loss=0.0265, Mask_Percent=7, NSP_Accuracy=0.9137, NSP_loss=0.2131, Total_loss=0.2396]\n",
      "Epoch 4: 100%|██████████| 6207/6207 [58:04<00:00,  1.78it/s, MLM_Accuracy=0.4022, MLM_loss=0.0503, Mask_Percent=10, NSP_Accuracy=0.9295, NSP_loss=0.4736, Total_loss=0.5239]\n",
      "Epoch 5: 100%|██████████| 6207/6207 [58:07<00:00,  1.78it/s, MLM_Accuracy=0.3936, MLM_loss=0.0294, Mask_Percent=10, NSP_Accuracy=0.9422, NSP_loss=0.3590, Total_loss=0.3884]\n",
      "Epoch 6: 100%|██████████| 6207/6207 [58:12<00:00,  1.78it/s, MLM_Accuracy=0.3809, MLM_loss=0.0282, Mask_Percent=10, NSP_Accuracy=0.9497, NSP_loss=0.0191, Total_loss=0.0473]\n",
      "Epoch 7: 100%|██████████| 6207/6207 [58:10<00:00,  1.78it/s, MLM_Accuracy=0.3718, MLM_loss=0.0503, Mask_Percent=13, NSP_Accuracy=0.9505, NSP_loss=0.0212, Total_loss=0.0716]\n",
      "Epoch 8: 100%|██████████| 6207/6207 [58:20<00:00,  1.77it/s, MLM_Accuracy=0.3666, MLM_loss=0.0574, Mask_Percent=13, NSP_Accuracy=0.9539, NSP_loss=0.0947, Total_loss=0.1520]\n",
      "Epoch 9: 100%|██████████| 6207/6207 [58:23<00:00,  1.77it/s, MLM_Accuracy=0.3608, MLM_loss=0.0532, Mask_Percent=13, NSP_Accuracy=0.9594, NSP_loss=0.0093, Total_loss=0.0625]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn = trainModel(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_propose\")\n",
    "mask_dyn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6207 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_9800\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 6207/6207 [57:48<00:00,  1.79it/s, MLM_Accuracy=0.4905, MLM_loss=0.0157, Mask_Percent=6, NSP_Accuracy=0.7223, NSP_loss=0.4816, Total_loss=0.4973] \n",
      "Epoch 1: 100%|██████████| 6207/6207 [2:06:23<00:00,  1.22s/it, MLM_Accuracy=0.4668, MLM_loss=0.0200, Mask_Percent=7, NSP_Accuracy=0.8032, NSP_loss=0.5300, Total_loss=0.5500]  \n",
      "Epoch 2: 100%|██████████| 6207/6207 [2:06:02<00:00,  1.22s/it, MLM_Accuracy=0.4376, MLM_loss=0.0226, Mask_Percent=8, NSP_Accuracy=0.8708, NSP_loss=0.0653, Total_loss=0.0879]  \n",
      "Epoch 3:  87%|████████▋ | 5382/6207 [1:45:01<22:56,  1.67s/it, MLM_Accuracy=0.4162, MLM_loss=0.0414, Mask_Percent=9, NSP_Accuracy=0.9110, NSP_loss=0.3847, Total_loss=0.4261]  "
     ]
    }
   ],
   "source": [
    "mask_dyn_grow1 = trainModel(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, masking_method = \"DMLM\", saveModelName = \"saved_model/saved_model_DMLM\")\n",
    "mask_dyn_grow1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_23280\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:17<00:00,  2.69it/s, MLM_Accuracy=0.3401, MLM_loss=0.0421, Mask_Percent=6, NSP_Accuracy=0.8319, NSP_loss=0.5053, Total_loss=0.5474]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4154, MLM_loss=0.0242, Mask_Percent=7, NSP_Accuracy=0.9515, NSP_loss=0.1555, Total_loss=0.1797]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:17<00:00,  2.69it/s, MLM_Accuracy=0.4337, MLM_loss=0.0264, Mask_Percent=7, NSP_Accuracy=0.9613, NSP_loss=0.1169, Total_loss=0.1434]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4349, MLM_loss=0.0347, Mask_Percent=7, NSP_Accuracy=0.9726, NSP_loss=1.2970, Total_loss=1.3317]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4176, MLM_loss=0.0379, Mask_Percent=10, NSP_Accuracy=0.9766, NSP_loss=0.0095, Total_loss=0.0474]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4217, MLM_loss=0.0395, Mask_Percent=10, NSP_Accuracy=0.9744, NSP_loss=0.0015, Total_loss=0.0410]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4266, MLM_loss=0.0454, Mask_Percent=10, NSP_Accuracy=0.9748, NSP_loss=0.0058, Total_loss=0.0511]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4166, MLM_loss=0.0713, Mask_Percent=13, NSP_Accuracy=0.9789, NSP_loss=0.0002, Total_loss=0.0715]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4208, MLM_loss=0.0301, Mask_Percent=13, NSP_Accuracy=0.9798, NSP_loss=0.0011, Total_loss=0.0312]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:17<00:00,  2.71it/s, MLM_Accuracy=0.4201, MLM_loss=0.0512, Mask_Percent=13, NSP_Accuracy=0.9834, NSP_loss=0.0027, Total_loss=0.0539]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn_input = getData(modelType = modelType, datapath = datapath, nspTask = \"NSP\")\n",
    "mask_dyn = trainModel(modelType = modelType, inputs = mask_dyn_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_mask_grow\")\n",
    "mask_dyn_input = None\n",
    "mask_dyn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_23280\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:15<00:00,  2.74it/s, MLM_Accuracy=0.3401, MLM_loss=0.0770, Mask_Percent=6, NSP_Accuracy=0.8463, NSP_loss=0.2719, Total_loss=0.3489]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4102, MLM_loss=0.0361, Mask_Percent=7, NSP_Accuracy=0.9375, NSP_loss=0.0480, Total_loss=0.0841]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4227, MLM_loss=0.0197, Mask_Percent=7, NSP_Accuracy=0.9604, NSP_loss=0.2295, Total_loss=0.2492]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4423, MLM_loss=0.0255, Mask_Percent=7, NSP_Accuracy=0.9694, NSP_loss=0.0002, Total_loss=0.0257]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4253, MLM_loss=0.0334, Mask_Percent=10, NSP_Accuracy=0.9694, NSP_loss=0.0138, Total_loss=0.0472]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:16<00:00,  2.71it/s, MLM_Accuracy=0.4204, MLM_loss=0.0398, Mask_Percent=10, NSP_Accuracy=0.9789, NSP_loss=0.3097, Total_loss=0.3494]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:16<00:00,  2.71it/s, MLM_Accuracy=0.4174, MLM_loss=0.0539, Mask_Percent=10, NSP_Accuracy=0.9780, NSP_loss=0.0017, Total_loss=0.0555]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:17<00:00,  2.70it/s, MLM_Accuracy=0.4047, MLM_loss=0.0320, Mask_Percent=13, NSP_Accuracy=0.9717, NSP_loss=0.0142, Total_loss=0.0462]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:16<00:00,  2.71it/s, MLM_Accuracy=0.4163, MLM_loss=0.0555, Mask_Percent=13, NSP_Accuracy=0.9739, NSP_loss=0.0142, Total_loss=0.0696]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:17<00:00,  2.70it/s, MLM_Accuracy=0.4188, MLM_loss=0.0514, Mask_Percent=13, NSP_Accuracy=0.9744, NSP_loss=0.0007, Total_loss=0.0521]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn_input = getData(modelType = modelType, datapath = datapath, nspTask = \"NSP\")\n",
    "mask_dyn = trainModel(modelType = modelType, inputs = mask_dyn_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_mask_grow\")\n",
    "mask_dyn_input = None\n",
    "mask_dyn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_23280\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:15<00:00,  2.74it/s, MLM_Accuracy=0.3199, MLM_loss=0.0272, Mask_Percent=6, NSP_Accuracy=0.8310, NSP_loss=0.2244, Total_loss=0.2516]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:15<00:00,  2.73it/s, MLM_Accuracy=0.4123, MLM_loss=0.0177, Mask_Percent=6, NSP_Accuracy=0.9317, NSP_loss=0.0256, Total_loss=0.0433]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4206, MLM_loss=0.0237, Mask_Percent=6, NSP_Accuracy=0.9663, NSP_loss=0.0535, Total_loss=0.0773]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4188, MLM_loss=0.0222, Mask_Percent=9, NSP_Accuracy=0.9690, NSP_loss=0.1858, Total_loss=0.2080]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4274, MLM_loss=0.0377, Mask_Percent=9, NSP_Accuracy=0.9811, NSP_loss=0.0007, Total_loss=0.0383]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:16<00:00,  2.73it/s, MLM_Accuracy=0.4326, MLM_loss=0.0490, Mask_Percent=9, NSP_Accuracy=0.9807, NSP_loss=0.0251, Total_loss=0.0742]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4302, MLM_loss=0.0529, Mask_Percent=12, NSP_Accuracy=0.9757, NSP_loss=0.0002, Total_loss=0.0531]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4123, MLM_loss=0.0373, Mask_Percent=12, NSP_Accuracy=0.9780, NSP_loss=0.2914, Total_loss=0.3287]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:16<00:00,  2.72it/s, MLM_Accuracy=0.4162, MLM_loss=0.0721, Mask_Percent=12, NSP_Accuracy=0.9820, NSP_loss=0.1853, Total_loss=0.2574]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:15<00:00,  2.74it/s, MLM_Accuracy=0.4009, MLM_loss=0.0398, Mask_Percent=15, NSP_Accuracy=0.9771, NSP_loss=0.0060, Total_loss=0.0458]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn_input = getData(modelType = modelType, datapath = datapath, nspTask = \"NSP\")\n",
    "mask_dyn = trainModel(modelType = modelType, inputs = mask_dyn_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_mask_grow\")\n",
    "mask_dyn_input = None\n",
    "mask_dyn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyBertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\pauls\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_34300\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:15<00:00,  2.73it/s, MLM_Accuracy=0.3481, MLM_loss=0.0439, Mask_Percent=6, NSP_Accuracy=0.8288, NSP_loss=1.3025, Total_loss=1.3463]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:17<00:00,  2.70it/s, MLM_Accuracy=0.4150, MLM_loss=0.0277, Mask_Percent=7, NSP_Accuracy=0.9371, NSP_loss=0.3044, Total_loss=0.3321]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:18<00:00,  2.68it/s, MLM_Accuracy=0.4103, MLM_loss=0.0228, Mask_Percent=7, NSP_Accuracy=0.9393, NSP_loss=0.0585, Total_loss=0.0813]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:19<00:00,  2.66it/s, MLM_Accuracy=0.4285, MLM_loss=0.0242, Mask_Percent=7, NSP_Accuracy=0.9690, NSP_loss=0.0011, Total_loss=0.0253]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:19<00:00,  2.66it/s, MLM_Accuracy=0.4242, MLM_loss=0.0411, Mask_Percent=10, NSP_Accuracy=0.9699, NSP_loss=0.3485, Total_loss=0.3896]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:19<00:00,  2.66it/s, MLM_Accuracy=0.4253, MLM_loss=0.0306, Mask_Percent=10, NSP_Accuracy=0.9789, NSP_loss=0.0120, Total_loss=0.0426]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:19<00:00,  2.66it/s, MLM_Accuracy=0.4291, MLM_loss=0.0318, Mask_Percent=10, NSP_Accuracy=0.9807, NSP_loss=0.0001, Total_loss=0.0319]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:17<00:00,  2.70it/s, MLM_Accuracy=0.4144, MLM_loss=0.0581, Mask_Percent=13, NSP_Accuracy=0.9613, NSP_loss=0.0040, Total_loss=0.0621]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:17<00:00,  2.70it/s, MLM_Accuracy=0.4123, MLM_loss=0.0406, Mask_Percent=13, NSP_Accuracy=0.9690, NSP_loss=0.1363, Total_loss=0.1769]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:17<00:00,  2.70it/s, MLM_Accuracy=0.4301, MLM_loss=0.0232, Mask_Percent=13, NSP_Accuracy=0.9856, NSP_loss=0.0058, Total_loss=0.0290]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn_input = getData(modelType = modelType, datapath = datapath, nspTask = \"NSP\")\n",
    "mask_dyn = trainModel(modelType = modelType, inputs = mask_dyn_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_mask_grow\")\n",
    "mask_dyn_input = None\n",
    "mask_dyn = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT_Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684b83e2f83316061361748e41b2620a10a3e9a8f2545480c20c18cf426689ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
