{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定 Tokenizer 與使用 BertForPretraining 來做 BERT 預訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining, AdamW\n",
    "from transformers.models.bert.modeling_bert import BertForPreTrainingOutput, BertPreTrainingHeads, BertConfig, BERT_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "from transformers.models.albert.modeling_albert import AlbertSOPHead\n",
    "from transformers.utils import ModelOutput\n",
    "from transformers.utils.doc import add_start_docstrings_to_model_forward, replace_return_docstrings\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTrainingOutput(BertForPreTrainingOutput):\n",
    "    \"\"\"\n",
    "    Output type of [`MyBertForPreTraining`].\n",
    "    Args:\n",
    "        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        mlm_loss (`float`):\n",
    "            MLM loss.\n",
    "        nsp_loss (`float`):\n",
    "            NSP loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, loss=None, prediction_logits=None, seq_relationship_logits=None, hidden_states=None, attentions=None, mlm_loss=None, nsp_loss=None):\n",
    "        super().__init__(loss=loss, prediction_logits=prediction_logits, seq_relationship_logits=seq_relationship_logits, hidden_states=hidden_states, attentions=attentions)\n",
    "        self.mlm_loss = mlm_loss\n",
    "        self.nsp_loss = nsp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlbertSOPHead(torch.nn.Module):\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size , config.num_labels)\n",
    "\n",
    "    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n",
    "        dropout_pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPretrainingHeadsWithSOP(BertPreTrainingHeads):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.seq_relationship = MyAlbertSOPHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForPreTraining(BertForPreTraining):\n",
    "    def __init__(self, config, nspTask = \"NSP\"):\n",
    "        super().__init__(config)\n",
    "        if nspTask == \"SOP\":\n",
    "            self.cls = BertPretrainingHeadsWithSOP(config)\n",
    "            \n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=MyBertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MyBertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return MyBertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            mlm_loss = masked_lm_loss,\n",
    "            nsp_loss = next_sentence_loss,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取出資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getData():\n",
    "    def __init__(self, modelType, datapath, nspTask = \"NSP\"):\n",
    "        self.datapath = datapath\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.nspTask = nspTask\n",
    "        self.text = self.toText()\n",
    "        self.sentence_a = []\n",
    "        self.sentence_b = []\n",
    "        self.label = []\n",
    "        self.inputs = None\n",
    "        self.nspPrepare()\n",
    "        self.inputs['labels'] = self.inputs.input_ids.detach().clone()\n",
    "    \n",
    "    def toText(self):\n",
    "        df = pd.read_csv(self.datapath)\n",
    "        text = []\n",
    "        for review in df[\"text\"]:\n",
    "            text.append(review)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def nspPrepare(self):\n",
    "        bag = [item for sentence in self.text for item in sentence.split('. ') if item != '']\n",
    "        bag_size = len(bag)\n",
    "\n",
    "        if self.nspTask == \"NSP\":\n",
    "            self.nspData(bag, bag_size)\n",
    "        elif self.nspTask == \"SOP\":\n",
    "            self.sopData()\n",
    "\n",
    "        self.inputs = self.tokenizer(self.sentence_a, self.sentence_b, return_tensors='pt',\n",
    "                   max_length=512, truncation=True, padding='max_length')\n",
    "        self.inputs['next_sentence_label'] = torch.LongTensor([self.label]).T\n",
    "    \n",
    "    def nspData(self, bag, bag_size):\n",
    "        for paragraph in self.text:\n",
    "            sentences = [\n",
    "                sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "            ]\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    self.sentence_a.append(sentences[start])\n",
    "                    self.sentence_b.append(sentences[start+1])\n",
    "                    self.label.append(0)\n",
    "                else:\n",
    "                    index = random.randint(0, bag_size-1)\n",
    "                    # this is NotNextSentence\n",
    "                    self.sentence_a.append(sentences[start])\n",
    "                    self.sentence_b.append(bag[index])\n",
    "                    self.label.append(1)\n",
    "    \n",
    "    def sopData(self):\n",
    "        for paragraph in self.text:\n",
    "            sentences = [\n",
    "                sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "            ]\n",
    "            num_sentences = len(sentences)\n",
    "            if num_sentences > 1:\n",
    "                start = random.randint(0, num_sentences-2)\n",
    "                # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "                if random.random() >= 0.5:\n",
    "                    # this is IsNextSentence\n",
    "                    self.sentence_a.append(sentences[start])\n",
    "                    self.sentence_b.append(sentences[start+1])\n",
    "                    self.label.append(0)\n",
    "                else:\n",
    "                    # this is NotNextSentence\n",
    "                    self.sentence_a.append(sentences[start+1])\n",
    "                    self.sentence_b.append(sentences[start])\n",
    "                    self.label.append(1)\n",
    "    \n",
    "    def returnInput(self):\n",
    "        return self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel():\n",
    "    def __init__(self, modelType, inputs, batch_size, epoch, acc_goal_each_epoch, masking_method = \"propose\", saveModelName = \"\", saveCSV = True, nspTask = \"NSP\"):\n",
    "        self.model = MyBertForPreTraining.from_pretrained(modelType)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.acc_goal_each_epoch = acc_goal_each_epoch  # 每個 epoch 的 MLM 正確率基準\n",
    "        self.masking_method = masking_method\n",
    "        self.saveModelName = saveModelName\n",
    "        self.saveCSV = saveCSV\n",
    "        self.loader = torch.utils.data.DataLoader(OurDataset(self.inputs), \\\n",
    "                                             batch_size=self.batch_size, shuffle=True)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        self.optim = AdamW(self.model.parameters(), lr = 5e-5)\n",
    "        self.last_acc = 0.0\n",
    "        \n",
    "        if os.path.isfile(\"record_mask_grow_move.csv\"):\n",
    "            self.rec = pd.read_csv(\"record_mask_grow_move.csv\")\n",
    "        else:\n",
    "            self.rec = pd.DataFrame({\"mlm_acc_each_epoch\":[], \"mlm_loss_each_epoch\":[], 'Mask_Percent_each_epoch':[]})\n",
    "            \n",
    "        self.training()\n",
    "        # self.save_model(self.saveModelName)\n",
    "    \n",
    "    def mlmPrepare(self, input_sentences, maskPercentNow, mask_ori, mask_avai):\n",
    "        # create mask array\n",
    "        mask_arr = torch.full(mask_ori.shape, False)\n",
    "        \n",
    "        for i in range(len(mask_ori)):\n",
    "            num_to_mask = round(len(torch.where(mask_ori[i])[0]) * (maskPercentNow * 0.01))\n",
    "            avai_can_mask = torch.where(mask_avai[i])\n",
    "            avai_can_mask_len = len(avai_can_mask[0])\n",
    "\n",
    "            if num_to_mask <= avai_can_mask_len:\n",
    "                mask_index = torch.randperm(avai_can_mask_len)[:num_to_mask]\n",
    "                mask_arr[i, avai_can_mask[0][mask_index]] = True\n",
    "                mask_avai[i] = mask_avai[i] ^ mask_arr[i]\n",
    "            else:\n",
    "                mask_index = torch.randperm(avai_can_mask_len)[:avai_can_mask_len]\n",
    "                num_to_mask -= avai_can_mask_len\n",
    "                mask_arr[i, avai_can_mask[0][mask_index]] = True\n",
    "                \n",
    "                set_mask_index = set(avai_can_mask[0][mask_index].numpy())\n",
    "\n",
    "                mask_avai[i] = mask_avai[i] ^ mask_ori[i]\n",
    "                avai_can_mask = torch.where(mask_avai[i])\n",
    "                avai_can_mask_len = len(avai_can_mask[0])\n",
    "                new_index = torch.randperm(avai_can_mask_len)[:num_to_mask]\n",
    "                \n",
    "                set_new_index = set(avai_can_mask[0][new_index].numpy())\n",
    "\n",
    "                intersection = set_mask_index.intersection(set_new_index)\n",
    "                while len(intersection) > 0:\n",
    "                    new_index = torch.randperm(avai_can_mask_len)[:num_to_mask]\n",
    "                    set_new_index = set(avai_can_mask[0][new_index].numpy())\n",
    "                    intersection = set_mask_index.intersection(set_new_index)\n",
    "\n",
    "                mask_arr[i, avai_can_mask[0][new_index]] = True\n",
    "                mask_avai[i] = mask_avai[i] ^ mask_arr[i]\n",
    "\n",
    "        selection = []\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            selection.append(\n",
    "                torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            )\n",
    "\n",
    "        rand_mask_type = copy.deepcopy(selection)\n",
    "\n",
    "        for row in range(len(rand_mask_type)):\n",
    "            for col in range(len(rand_mask_type[row])):\n",
    "                rand_mask_type[row][col] = random.random()\n",
    "\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        special_tokens = [vocab['[CLS]'], vocab['[SEP]'], vocab['[MASK]'], vocab['[UNK]'],  vocab['[PAD]']]\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            for j in range(len(selection[i])):\n",
    "                if rand_mask_type[i][j] < 0.10:\n",
    "                    continue\n",
    "                elif rand_mask_type[i][j] < 0.20:\n",
    "                    rand_num = vocab['[CLS]']\n",
    "                    while rand_num in special_tokens:\n",
    "                        rand_num = random.randint(1, vocab_size-1)\n",
    "                    input_sentences[i, selection[i][j]] = rand_num\n",
    "                else:\n",
    "                    input_sentences[i, selection[i][j]] = 103\n",
    "        \n",
    "        return input_sentences, mask_arr\n",
    "\n",
    "    def training(self):\n",
    "        acc_each_epoch = []\n",
    "        loss_each_epoch = []\n",
    "        Mask_Percent_each_epoch = []\n",
    "        stay = 0\n",
    "        percent_now = 6\n",
    "        masking_position = {\"avai_pos\":[], \"available\":[]}\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            # setup loop with TQDM and dataloader\n",
    "            mask_nums = 0\n",
    "            mlm_correct = 0\n",
    "            nsp_nums = 0\n",
    "            nsp_correct = 0\n",
    "            loop = tqdm(self.loader, leave=True)\n",
    "\n",
    "            for batch_index, batch in enumerate(loop):\n",
    "                if epoch == 0:\n",
    "                    can_mask = (batch[\"input_ids\"] != 101) * (batch[\"input_ids\"] != 102) * (batch[\"input_ids\"] != 0)\n",
    "                    masking_position[\"avai_pos\"].append((can_mask).detach().clone())\n",
    "                    masking_position[\"available\"].append((can_mask).detach().clone())\n",
    "                \n",
    "                input_sentences, mask_arr = self.mlmPrepare(batch[\"input_ids\"].detach().clone(), percent_now, \\\n",
    "                                                            masking_position[\"avai_pos\"][batch_index], masking_position[\"available\"][batch_index])\n",
    "\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                self.optim.zero_grad()\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = input_sentences.to(self.device)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                next_sentence_label = batch['next_sentence_label'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                # process\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                next_sentence_label=next_sentence_label,\n",
    "                                labels=labels)\n",
    "                \n",
    "                prediction_logits = outputs.prediction_logits[mask_arr]\n",
    "                predicted_ids = prediction_logits.argmax(-1)\n",
    "                \n",
    "                seq_relationship_logits = outputs.seq_relationship_logits\n",
    "                predicted_labels = torch.argmax(seq_relationship_logits, dim=1)\n",
    "                predicted_label = predicted_labels\n",
    "\n",
    "                mask_nums += len(predicted_ids)\n",
    "                mlm_correct += torch.eq(predicted_ids, labels[mask_arr]).sum().item()\n",
    "                nsp_nums += len(predicted_label)\n",
    "                nsp_correct += predicted_label.eq(torch.squeeze(next_sentence_label)).sum().item()\n",
    "                \n",
    "                # extract loss\n",
    "                loss = outputs.loss\n",
    "                mlm_loss = outputs.mlm_loss.item()\n",
    "                nsp_loss = outputs.nsp_loss.item()\n",
    "                mlm_acc = mlm_correct / mask_nums\n",
    "                nsp_acc = nsp_correct / nsp_nums\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                self.optim.step()\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(Total_loss='{:.4f}'.format(loss.item()), MLM_Accuracy='{:.4f}'.format(mlm_acc), NSP_Accuracy='{:.4f}'.format(nsp_acc), \\\n",
    "                                MLM_loss='{:.4f}'.format(mlm_loss), NSP_loss='{:.4f}'.format(nsp_loss), Mask_Percent=percent_now)\n",
    "            \n",
    "            acc_each_epoch.append(mlm_acc)\n",
    "            loss_each_epoch.append(mlm_loss)\n",
    "            Mask_Percent_each_epoch.append(percent_now)\n",
    "\n",
    "            if self.masking_method == \"DMLM\":\n",
    "                percent_now += 1\n",
    "            elif self.masking_method == \"propose\":\n",
    "                if (mlm_acc >= self.acc_goal_each_epoch[epoch] * 0.01) or stay >= 2:\n",
    "                    stay = 0\n",
    "                    percent_now = 6 + epoch + 1\n",
    "                else:\n",
    "                    stay += 1\n",
    "            elif self.masking_method == \"adaptive\":\n",
    "                if mlm_acc > self.last_acc:\n",
    "                    percent_now += 1\n",
    "                else:\n",
    "                    percent_now -= 1\n",
    "                self.last_acc = mlm_acc\n",
    "            \n",
    "            if epoch % 2 == 1:\n",
    "                self.save_model(self.saveModelName + \"_epoch\" + str(epoch + 1))\n",
    "\n",
    "\n",
    "        if self.saveCSV:\n",
    "            \n",
    "            new_rec = pd.concat([self.rec, pd.DataFrame(pd.DataFrame({'mlm_acc_each_epoch': [acc_each_epoch], 'mlm_loss_each_epoch': [loss_each_epoch], 'Mask_Percent_each_epoch': [Mask_Percent_each_epoch]}))], ignore_index=True)\n",
    "            new_rec.to_csv(\"record_mask_grow_move.csv\", index = False)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        self.model.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "datapath = 'bbc-text.csv'\n",
    "modelType = 'bert-base-cased'\n",
    "epoch = 10\n",
    "batch_size = 6\n",
    "nsp_input = getData(modelType = modelType, datapath = datapath, nspTask = \"NSP\")\n",
    "epoch_acc = [34.0, 42.1, 44.2, 45.7, 47.3, 49.7, 50.8, 52.8, 53.8 , 55.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_16300\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:55<00:00,  2.11it/s, MLM_Accuracy=0.3424, MLM_loss=0.0631, Mask_Percent=6, NSP_Accuracy=0.8400, NSP_loss=0.4383, Total_loss=0.5014]\n",
      "Epoch 1: 100%|██████████| 371/371 [03:02<00:00,  2.03it/s, MLM_Accuracy=0.4911, MLM_loss=0.0307, Mask_Percent=7, NSP_Accuracy=0.9411, NSP_loss=0.3854, Total_loss=0.4161]\n",
      "Epoch 2: 100%|██████████| 371/371 [03:06<00:00,  1.99it/s, MLM_Accuracy=0.4972, MLM_loss=0.0423, Mask_Percent=8, NSP_Accuracy=0.9645, NSP_loss=0.0698, Total_loss=0.1122]\n",
      "Epoch 3: 100%|██████████| 371/371 [03:06<00:00,  1.99it/s, MLM_Accuracy=0.4963, MLM_loss=0.0462, Mask_Percent=9, NSP_Accuracy=0.9744, NSP_loss=0.9217, Total_loss=0.9679]\n",
      "Epoch 4: 100%|██████████| 371/371 [03:06<00:00,  1.99it/s, MLM_Accuracy=0.5131, MLM_loss=0.0252, Mask_Percent=10, NSP_Accuracy=0.9771, NSP_loss=0.1771, Total_loss=0.2023]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:57<00:00,  2.09it/s, MLM_Accuracy=0.4951, MLM_loss=0.0313, Mask_Percent=11, NSP_Accuracy=0.9829, NSP_loss=0.0038, Total_loss=0.0351]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:52<00:00,  2.16it/s, MLM_Accuracy=0.5010, MLM_loss=0.0578, Mask_Percent=12, NSP_Accuracy=0.9730, NSP_loss=0.0414, Total_loss=0.0992]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.5088, MLM_loss=0.0549, Mask_Percent=13, NSP_Accuracy=0.9847, NSP_loss=0.0221, Total_loss=0.0770]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.5125, MLM_loss=0.0979, Mask_Percent=14, NSP_Accuracy=0.9843, NSP_loss=0.0359, Total_loss=0.1338]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:50<00:00,  2.17it/s, MLM_Accuracy=0.5086, MLM_loss=0.0374, Mask_Percent=15, NSP_Accuracy=0.9784, NSP_loss=0.0090, Total_loss=0.0465]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn = trainModel(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_maskPos_propose\")\n",
    "mask_dyn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_16300\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.3568, MLM_loss=0.0459, Mask_Percent=6, NSP_Accuracy=0.8387, NSP_loss=0.6125, Total_loss=0.6584]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.4906, MLM_loss=0.0457, Mask_Percent=7, NSP_Accuracy=0.9452, NSP_loss=0.0662, Total_loss=0.1119]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.5027, MLM_loss=0.0314, Mask_Percent=8, NSP_Accuracy=0.9694, NSP_loss=0.0120, Total_loss=0.0434]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.5043, MLM_loss=0.0369, Mask_Percent=9, NSP_Accuracy=0.9780, NSP_loss=0.0054, Total_loss=0.0423]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.5102, MLM_loss=0.0294, Mask_Percent=10, NSP_Accuracy=0.9757, NSP_loss=0.0361, Total_loss=0.0654]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.5189, MLM_loss=0.0231, Mask_Percent=11, NSP_Accuracy=0.9820, NSP_loss=0.0002, Total_loss=0.0233]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.5132, MLM_loss=0.0192, Mask_Percent=12, NSP_Accuracy=0.9766, NSP_loss=0.0138, Total_loss=0.0329]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.5194, MLM_loss=0.0474, Mask_Percent=13, NSP_Accuracy=0.9744, NSP_loss=0.0002, Total_loss=0.0476]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4994, MLM_loss=0.0599, Mask_Percent=14, NSP_Accuracy=0.9753, NSP_loss=0.0153, Total_loss=0.0752]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.5016, MLM_loss=0.0519, Mask_Percent=15, NSP_Accuracy=0.9802, NSP_loss=0.0025, Total_loss=0.0543]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn_grow1 = trainModel(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, masking_method = \"DMLM\", saveModelName = \"saved_model/saved_model_maskPos_DMLM\")\n",
    "mask_dyn_grow1 = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel_V3():\n",
    "    def __init__(self, modelType, inputs, batch_size, epoch, acc_goal_each_epoch, masking_method = \"propose\", saveModelName = \"\", saveCSV = True, nspTask = \"NSP\"):\n",
    "        self.model = MyBertForPreTraining.from_pretrained(modelType)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(modelType)\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.acc_goal_each_epoch = acc_goal_each_epoch  # 每個 epoch 的 MLM 正確率基準\n",
    "        self.masking_method = masking_method\n",
    "        self.saveModelName = saveModelName\n",
    "        self.saveCSV = saveCSV\n",
    "        self.loader = torch.utils.data.DataLoader(OurDataset(self.inputs), \\\n",
    "                                             batch_size=self.batch_size, shuffle=True)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        self.optim = AdamW(self.model.parameters(), lr = 5e-5)\n",
    "        self.last_acc = 0.0\n",
    "        \n",
    "        if os.path.isfile(\"record_mask_grow_move.csv\"):\n",
    "            self.rec = pd.read_csv(\"record_mask_grow_move.csv\")\n",
    "        else:\n",
    "            self.rec = pd.DataFrame({\"mlm_acc_each_epoch\":[], \"mlm_loss_each_epoch\":[], 'Mask_Percent_each_epoch':[]})\n",
    "            \n",
    "        self.training()\n",
    "        self.save_model(self.saveModelName)\n",
    "    \n",
    "    def mlmPrepare(self, input_sentences, maskPercentNow):\n",
    "        rand = torch.rand(input_sentences.shape)\n",
    "        # create mask array\n",
    "        mask_arr = (rand < maskPercentNow * 0.01) * (input_sentences != 101) * \\\n",
    "                (input_sentences != 102) * (input_sentences != 0)\n",
    "        \n",
    "        selection = []\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            selection.append(\n",
    "                torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            )\n",
    "\n",
    "        rand_mask_type = copy.deepcopy(selection)\n",
    "\n",
    "        for row in range(len(rand_mask_type)):\n",
    "            for col in range(len(rand_mask_type[row])):\n",
    "                rand_mask_type[row][col] = random.random()\n",
    "\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        special_tokens = [vocab['[CLS]'], vocab['[SEP]'], vocab['[MASK]'], vocab['[UNK]'],  vocab['[PAD]']]\n",
    "\n",
    "        for i in range(input_sentences.shape[0]):\n",
    "            for j in range(len(selection[i])):\n",
    "                if rand_mask_type[i][j] < 0.10:\n",
    "                    continue\n",
    "                elif rand_mask_type[i][j] < 0.20:\n",
    "                    rand_num = vocab['[CLS]']\n",
    "                    while rand_num in special_tokens:\n",
    "                        rand_num = random.randint(1, vocab_size-1)\n",
    "                    input_sentences[i, selection[i][j]] = rand_num\n",
    "                else:\n",
    "                    input_sentences[i, selection[i][j]] = 103\n",
    "        \n",
    "        return input_sentences, mask_arr\n",
    "\n",
    "    def training(self):\n",
    "        acc_each_epoch = []\n",
    "        loss_each_epoch = []\n",
    "        Mask_Percent_each_epoch = []\n",
    "        stay = 0\n",
    "        percent_now = 6\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            # setup loop with TQDM and dataloader\n",
    "            mask_nums = 0\n",
    "            mlm_correct = 0\n",
    "            nsp_nums = 0\n",
    "            nsp_correct = 0\n",
    "            loop = tqdm(self.loader, leave=True)\n",
    "\n",
    "            for batch in loop:\n",
    "                input_sentences, mask_arr = self.mlmPrepare(batch[\"input_ids\"].detach().clone(), percent_now)\n",
    "\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                self.optim.zero_grad()\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = input_sentences.to(self.device)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                next_sentence_label = batch['next_sentence_label'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                # process\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                next_sentence_label=next_sentence_label,\n",
    "                                labels=labels)\n",
    "                \n",
    "                prediction_logits = outputs.prediction_logits[mask_arr]\n",
    "                predicted_ids = prediction_logits.argmax(-1)\n",
    "                \n",
    "                seq_relationship_logits = outputs.seq_relationship_logits\n",
    "                predicted_labels = torch.argmax(seq_relationship_logits, dim=1)\n",
    "                predicted_label = predicted_labels\n",
    "\n",
    "                mask_nums += len(predicted_ids)\n",
    "                mlm_correct += torch.eq(predicted_ids, labels[mask_arr]).sum().item()\n",
    "                nsp_nums += len(predicted_label)\n",
    "                nsp_correct += predicted_label.eq(torch.squeeze(next_sentence_label)).sum().item()\n",
    "                \n",
    "                # extract loss\n",
    "                loss = outputs.loss\n",
    "                mlm_loss = outputs.mlm_loss.item()\n",
    "                nsp_loss = outputs.nsp_loss.item()\n",
    "                mlm_acc = mlm_correct / mask_nums\n",
    "                nsp_acc = nsp_correct / nsp_nums\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                self.optim.step()\n",
    "                \n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(Total_loss='{:.4f}'.format(loss.item()), MLM_Accuracy='{:.4f}'.format(mlm_acc), NSP_Accuracy='{:.4f}'.format(nsp_acc), \\\n",
    "                                MLM_loss='{:.4f}'.format(mlm_loss), NSP_loss='{:.4f}'.format(nsp_loss), Mask_Percent=percent_now)\n",
    "\n",
    "            acc_each_epoch.append(mlm_acc)\n",
    "            loss_each_epoch.append(mlm_loss)\n",
    "            Mask_Percent_each_epoch.append(percent_now)\n",
    "\n",
    "            if self.masking_method == \"DMLM\":\n",
    "                percent_now += 1\n",
    "            elif self.masking_method == \"propose\":\n",
    "                if (mlm_acc >= self.acc_goal_each_epoch[epoch] * 0.01) or stay >= 2:\n",
    "                    stay = 0\n",
    "                    percent_now = 6 + epoch + 1\n",
    "                else:\n",
    "                    stay += 1\n",
    "            elif self.masking_method == \"adaptive\":\n",
    "                if mlm_acc > self.last_acc:\n",
    "                    percent_now += 1\n",
    "                else:\n",
    "                    percent_now -= 1\n",
    "                self.last_acc = mlm_acc\n",
    "            \n",
    "            if epoch % 2 == 1:\n",
    "                self.save_model(self.saveModelName + \"_epoch\" + str(epoch + 1))\n",
    "\n",
    "        if self.saveCSV:\n",
    "            \n",
    "            new_rec = pd.concat([self.rec, pd.DataFrame(pd.DataFrame({'mlm_acc_each_epoch': [acc_each_epoch], 'mlm_loss_each_epoch': [loss_each_epoch], 'Mask_Percent_each_epoch': [Mask_Percent_each_epoch]}))], ignore_index=True)\n",
    "            new_rec.to_csv(\"record_mask_grow_move.csv\", index = False)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        self.model.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_16300\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:47<00:00,  2.21it/s, MLM_Accuracy=0.3447, MLM_loss=0.0351, Mask_Percent=6, NSP_Accuracy=0.8557, NSP_loss=0.1295, Total_loss=0.1646]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4122, MLM_loss=0.0476, Mask_Percent=7, NSP_Accuracy=0.9344, NSP_loss=0.3681, Total_loss=0.4157]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.4185, MLM_loss=0.0178, Mask_Percent=8, NSP_Accuracy=0.9613, NSP_loss=0.2208, Total_loss=0.2386]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:48<00:00,  2.21it/s, MLM_Accuracy=0.4196, MLM_loss=0.0409, Mask_Percent=9, NSP_Accuracy=0.9658, NSP_loss=0.5859, Total_loss=0.6268]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:46<00:00,  2.22it/s, MLM_Accuracy=0.4263, MLM_loss=0.0317, Mask_Percent=10, NSP_Accuracy=0.9784, NSP_loss=0.0150, Total_loss=0.0467]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:47<00:00,  2.22it/s, MLM_Accuracy=0.4272, MLM_loss=0.0626, Mask_Percent=11, NSP_Accuracy=0.9820, NSP_loss=0.0193, Total_loss=0.0819]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:46<00:00,  2.23it/s, MLM_Accuracy=0.4196, MLM_loss=0.0300, Mask_Percent=12, NSP_Accuracy=0.9717, NSP_loss=0.0684, Total_loss=0.0984]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.4091, MLM_loss=0.0404, Mask_Percent=13, NSP_Accuracy=0.9721, NSP_loss=0.0004, Total_loss=0.0409]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.4152, MLM_loss=0.0627, Mask_Percent=14, NSP_Accuracy=0.9843, NSP_loss=0.0088, Total_loss=0.0715]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.4127, MLM_loss=0.0369, Mask_Percent=15, NSP_Accuracy=0.9807, NSP_loss=0.0060, Total_loss=0.0429]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn = trainModel_V3(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, saveModelName = \"saved_model/saved_model_mask_propose\")\n",
    "mask_dyn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_16300\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.3344, MLM_loss=0.0391, Mask_Percent=6, NSP_Accuracy=0.8413, NSP_loss=0.8922, Total_loss=0.9313]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.4216, MLM_loss=0.0415, Mask_Percent=7, NSP_Accuracy=0.9344, NSP_loss=0.2081, Total_loss=0.2496]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:50<00:00,  2.18it/s, MLM_Accuracy=0.4110, MLM_loss=0.0353, Mask_Percent=8, NSP_Accuracy=0.9654, NSP_loss=0.0823, Total_loss=0.1177]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.4201, MLM_loss=0.0253, Mask_Percent=9, NSP_Accuracy=0.9784, NSP_loss=0.0042, Total_loss=0.0295]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:49<00:00,  2.18it/s, MLM_Accuracy=0.4234, MLM_loss=0.0584, Mask_Percent=10, NSP_Accuracy=0.9721, NSP_loss=0.0191, Total_loss=0.0775]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4138, MLM_loss=0.0280, Mask_Percent=11, NSP_Accuracy=0.9762, NSP_loss=0.0727, Total_loss=0.1008]\n",
      "Epoch 6: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.4142, MLM_loss=0.0349, Mask_Percent=12, NSP_Accuracy=0.9708, NSP_loss=0.0020, Total_loss=0.0369]\n",
      "Epoch 7: 100%|██████████| 371/371 [02:48<00:00,  2.20it/s, MLM_Accuracy=0.4176, MLM_loss=0.0446, Mask_Percent=13, NSP_Accuracy=0.9775, NSP_loss=0.0196, Total_loss=0.0642]\n",
      "Epoch 8: 100%|██████████| 371/371 [02:53<00:00,  2.14it/s, MLM_Accuracy=0.4132, MLM_loss=0.0437, Mask_Percent=14, NSP_Accuracy=0.9744, NSP_loss=0.0862, Total_loss=0.1299]\n",
      "Epoch 9: 100%|██████████| 371/371 [02:52<00:00,  2.15it/s, MLM_Accuracy=0.4069, MLM_loss=0.0545, Mask_Percent=15, NSP_Accuracy=0.9690, NSP_loss=0.0107, Total_loss=0.0651]\n"
     ]
    }
   ],
   "source": [
    "mask_dyn_grow1 = trainModel_V3(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, masking_method = \"DMLM\", saveModelName = \"saved_model/saved_model_mask_DMLM\")\n",
    "mask_dyn_grow1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\transformer_torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/371 [00:00<?, ?it/s]C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_13556\\541448751.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 371/371 [02:49<00:00,  2.19it/s, MLM_Accuracy=0.3339, MLM_loss=0.0421, Mask_Percent=6, NSP_Accuracy=0.8274, NSP_loss=0.1160, Total_loss=0.1580]\n",
      "Epoch 1: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.4124, MLM_loss=0.0314, Mask_Percent=7, NSP_Accuracy=0.9321, NSP_loss=0.0280, Total_loss=0.0594]\n",
      "Epoch 2: 100%|██████████| 371/371 [02:51<00:00,  2.16it/s, MLM_Accuracy=0.4202, MLM_loss=0.0401, Mask_Percent=8, NSP_Accuracy=0.9649, NSP_loss=0.0473, Total_loss=0.0873]\n",
      "Epoch 3: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.4130, MLM_loss=0.0570, Mask_Percent=9, NSP_Accuracy=0.9654, NSP_loss=0.0435, Total_loss=0.1005]\n",
      "Epoch 4: 100%|██████████| 371/371 [02:51<00:00,  2.17it/s, MLM_Accuracy=0.4279, MLM_loss=0.0536, Mask_Percent=8, NSP_Accuracy=0.9762, NSP_loss=0.0053, Total_loss=0.0589]\n",
      "Epoch 5: 100%|██████████| 371/371 [02:53<00:00,  2.14it/s, MLM_Accuracy=0.4184, MLM_loss=0.0135, Mask_Percent=9, NSP_Accuracy=0.9753, NSP_loss=0.0073, Total_loss=0.0208]\n",
      "Epoch 6: 100%|██████████| 371/371 [03:01<00:00,  2.04it/s, MLM_Accuracy=0.4338, MLM_loss=0.0247, Mask_Percent=8, NSP_Accuracy=0.9847, NSP_loss=0.0005, Total_loss=0.0253]\n",
      "Epoch 7: 100%|██████████| 371/371 [03:01<00:00,  2.04it/s, MLM_Accuracy=0.4338, MLM_loss=0.0416, Mask_Percent=9, NSP_Accuracy=0.9874, NSP_loss=0.0001, Total_loss=0.0417]\n",
      "Epoch 8: 100%|██████████| 371/371 [03:03<00:00,  2.02it/s, MLM_Accuracy=0.4261, MLM_loss=0.0343, Mask_Percent=10, NSP_Accuracy=0.9771, NSP_loss=0.1255, Total_loss=0.1598]\n",
      "Epoch 9: 100%|██████████| 371/371 [03:06<00:00,  1.99it/s, MLM_Accuracy=0.4206, MLM_loss=0.0241, Mask_Percent=9, NSP_Accuracy=0.9748, NSP_loss=0.0065, Total_loss=0.0307]\n"
     ]
    }
   ],
   "source": [
    "adp = trainModel_V3(modelType = modelType, inputs = nsp_input.returnInput(), batch_size = batch_size, epoch = epoch, acc_goal_each_epoch = epoch_acc, masking_method = \"adaptive\", saveModelName = \"saved_model/saved_model_mask_adaptive\")\n",
    "adp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_input = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT_Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684b83e2f83316061361748e41b2620a10a3e9a8f2545480c20c18cf426689ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
