{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkout the dataset\n",
    "Dataset: [bbc dataset][def]  \n",
    "- 根據文章片段，分類該文章是屬於什麼領域的報導\n",
    "- 總共有五種分類：entertainment, sport, tech, business, politics\n",
    "- 資料總筆數：2225\n",
    "\n",
    "我們可以從下方圖表得知：  \n",
    "* 此資料集有五種類別，即是等等 BERT 判斷分類的 Label  \n",
    "* 而每筆 Text 就是 BERT 的 input 資料\n",
    "\n",
    "目的就是判斷此 Text 的類別 (Label) 是什麼  \n",
    "\n",
    "[def]: https://www.kaggle.com/datasets/sainijagjit/bbc-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datapath = 'bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d0492bd970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['category']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data (資料前處理)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BertTokenizer\n",
    "[**BertTokenizer**](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer):\n",
    "以 Word-based 的方式做 Tokenization，並加上 \\[CLS\\]、\\[SEP\\]、\\[PAD\\] 這三個特殊 token  \n",
    "使用方式為： ```BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)```   \n",
    "其中，```PRETRAINED_MODEL_NAME``` 可依需求尋找適合任務的預訓練模型 ([可用模型總攬](https://huggingface.co/models))  \n",
    "本例子使用常見的英文模型 ```'bert-base-cased'```\n",
    "\n",
    "BertTokenizer 繼承於 [PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中較常使用的參數如下：\n",
    "\n",
    "* ```text``` / ```text_pair ```: 要做 Tokenize 的序列 / 要做 Tokenize 的一對序列\n",
    "* ```padding```: 選擇要不要補 padding (\\[PAD\\])  \n",
    "  * ```True``` or ```'longest'```: 做 padding 直到序列長度等於 batch 中最長的序列\n",
    "  * ```'max_length'```: padding 直到設定的值 'max_length'\n",
    "  * ```False``` or ```'do_not_pad'```: 不做 padding\n",
    "* ```max_length```: 設定序列的最長長度 (BERT 中是 512) \n",
    "* ```truncation```: 若序列(序列對)長度總和大於上限，是否截斷序列\n",
    "  * ```True``` or ```'longest_first'```: 以迭代的方式，從最長的序列開始慢慢截斷。  \n",
    "    ex:  \n",
    "    seq1 = \"a b c\", seq2 = \"d e\", max_length = 2。因為 len(seq1) + len(seq2) > 2，需要要做截斷  \n",
    "    (註：這邊先忽略 \\[CLS\\]、\\[SEP\\]、\\[PAD\\] 這些 Token)  \n",
    "    迭代1：seq1 = \"a b\", seq2 = \"d e\"  \n",
    "    迭代2：seq1 = \"a\", seq2 = \"d e\"  \n",
    "    迭代3：seq1 = \"a\", seq2 = \"d\" => 最後輸出 \"a d\"  \n",
    "  * ```'only_first'```: 只截斷一對序列對中的第一個序列\n",
    "  * ```'only_second'```: 只截斷一對序列對中的第二個序列\n",
    "  * ```False``` or ```'do_not_truncate'```: 不做截斷\n",
    "* ```return_tensors```: 決定回傳張量的資料型態\n",
    "  * ```'tf'```: TensorFlow tf.constant.\n",
    "  * ```'pt'```: PyTorch torch.Tensor.\n",
    "  * ```'np'```: Numpy np.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "example_text1 = 'I am practicing using Bert Model'\n",
    "example_text2 = 'I am trying'\n",
    "bert_input = tokenizer(example_text1, padding='max_length', max_length = 10, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "bert_input2 = tokenizer(example_text1,example_text2, padding='max_length', max_length = 10, \n",
    "                       truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BertTokenizer` 的輸出 (這邊為 `bert_input`) 有三種：\n",
    "* `input_ids`: 輸出每個 token 代表的數字代號  \n",
    "    (其中 \\[CLS\\] = 101, \\[SEP\\] = 102, \\[PAD\\] = 0)\n",
    "* `token_type_ids`: 輸出每個 token 是屬於序列對中的哪一個序列，用 0、1 表示  \n",
    "    (如果輸入序列只有一個，那每個 token 都是 0)\n",
    "    (啊簡單來說就是讓 BERT 知道 Token 屬於哪個句子)\n",
    "* `attention_mask`: 判斷各個 token 是原句資訊還是 padding  \n",
    "    如果該 token 是 真實的字、\\[CLS\\]、\\[SEP\\]：輸出 1  \n",
    "    如果該 token 是 \\[PAD\\]：輸出 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence:  I am practicing using Bert Model\n",
      "input_ids:  tensor([[  101,   146,  1821, 13029,  1606, 15035,  6747,   102,     0,     0]])\n",
      "token_type_ids:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "decode:  [CLS] I am practicing using Bert Model [SEP] [PAD] [PAD]\n",
      "-----------------------------------------------------------\n",
      "Sequence1:  I am practicing using Bert Model\n",
      "Sequence2:  I am trying\n",
      "input_ids:  tensor([[  101,   146,  1821, 13029,  1606,   102,   146,  1821,  1774,   102]])\n",
      "token_type_ids:  tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])\n",
      "attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "decode:  [CLS] I am practicing using [SEP] I am trying [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequence: \", example_text1)\n",
    "print(\"input_ids: \",bert_input['input_ids'])\n",
    "print(\"token_type_ids: \", bert_input['token_type_ids'])\n",
    "print(\"attention_mask: \", bert_input['attention_mask'])\n",
    "\n",
    "example_text1_decode = tokenizer.decode(bert_input.input_ids[0])\n",
    "\n",
    "print(\"decode: \",example_text1_decode)\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(\"Sequence1: \", example_text1)\n",
    "print(\"Sequence2: \", example_text2)\n",
    "print(\"input_ids: \",bert_input2['input_ids'])\n",
    "print(\"token_type_ids: \", bert_input2['token_type_ids'])\n",
    "print(\"attention_mask: \", bert_input2['attention_mask'])\n",
    "\n",
    "example_text2_decode = tokenizer.decode(bert_input2.input_ids[0])\n",
    "\n",
    "print(\"decode: \",example_text2_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Class\n",
    "1. 決定要用什麼模型的 tokenizer  \n",
    "2. 給定對應到 Dataset 類別的標籤 (labels)，即將資料集中的各類別對應到 id (數值)  \n",
    "   這邊將 ```'business'```, ```'entertainment'```, ```'sport'```, ```'tech'```, ```'politics'``` 對應到 0 ~ 4，\n",
    "3. 初始化 Dataset 時：\n",
    "   - 各筆資料的分類 (category) 轉換成 id\n",
    "   - 再把資料集丟進 BertTokenizer，轉換成 BERT 可以使用的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 決定 tokenizer 類型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# 決定資料集中各分類對應的 id\n",
    "labels = {'business':0,\n",
    "          'entertainment':1,\n",
    "          'sport':2,\n",
    "          'tech':3,\n",
    "          'politics':4\n",
    "          }\n",
    "\n",
    "# 資料集處理\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        # 把每一筆資料的類別改成 id\n",
    "        self.labels = [labels[label] for label in df['category']]  \n",
    "        # 對每筆資料做 BERT tokenize\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    # 回傳資料集各類別 (id)\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    # 回傳該 label 的資料數\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # 取得當前資料的 label\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    # 取得當前資料的 text\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分 訓練 (training)、驗證 (validation)、測試 (testing) 資料集   \n",
    "這邊切分比例為 80:10:10  \n",
    "\n",
    "註：\n",
    "1. ```df.sample(frac=1, random_state=42)```：從 dataframe 隨機取樣  \n",
    "   ```frac```：要抽取 dataframe 的比例 (0 ~ 1)  \n",
    "   ```random_state```：隨機的狀態，可以想樣乘亂數表的位置 (？)  \n",
    "2. ```np.split(..., [int(.8*len(df)), int(.9*len(df))])```：把陣列做分割  \n",
    "   上述第二個參數的 list，代表分別取道的相對位置，分割個數 = 該 list 的長度，以本例子來說：  \n",
    "   第一區塊：```int(.8*len(df))```：取到 df 的 80 % => 分割的第一組陣列是 0% ~ 80% 的 df  \n",
    "   第二區塊：```int(.9*len(df))```：取到 df 的 90 % => 分割的第一組陣列是 80% ~ 90% 的 df  \n",
    "   第三區塊：分割的第一組陣列是 90% ~ 100% 的 df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780 222 223\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "[**BertModel**](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)：主要是 transformer 的 Encoder 部分  \n",
    "繼承於 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/model#transformers.PreTrainedModel)，在 ```forward```中較常使用的參數如下：  \n",
    "* ```input_ids```: 輸入序列  \n",
    "* ```attention_mask```: 輸入序列對應的 mask，以告知 attention 略過他們  \n",
    "  1: 該 token 是 真實的字、\\[CLS\\]、\\[SEP\\]  \n",
    "  0: 該 token 是 \\[PAD\\]，即要被 mask 掉的部分\n",
    "* ```return_dict ```: True: 回傳值是 [ModelOutput](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/output#transformers.utils.ModelOutput)；False: 回傳值是 tuple\n",
    "\n",
    "若 ```return_dict = True```，BertModel 的輸出為 [transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)  \n",
    "而範例中，```return_dict = False```，BertModel 的輸出為 ```tuple(torch.FloatTensor)```，範例以 ```_``` 與 ```pooled_output``` 表示：\n",
    "* ```_```: sequence output，為序列在 BERT 最後一層 hidden layer 的輸出\n",
    "* ```pooled_output```: 取出 BERT 最後一層 layer中，\\[CLS\\] 對應的向量 (vector)  \n",
    "由上可知，BERT 可以處理序列任務 (如輸出```_```) 及 分類、迴歸分析任務 (如輸出```pooled_output```)\n",
    "\n",
    "我們把 ```pooled_output``` 經過 dropout、線性轉換、ReLU 激活函數後，我們在最後的線性層可以得到維度為 5 的向量，代表著我們資料及的分類 (sport, business, politics, entertainment, and tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier, self).__init__()                      # 繼承 nn.Module\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')    # 選擇 Model\n",
    "        self.dropout = nn.Dropout(dropout)                          # dropout = 0.5 => 去掉 50% neural，避免 overfitting\n",
    "        self.linear = nn.Linear(768, 5)                             # BERT Base size: 768，每句有 5 種可能的分類要做選擇  \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "* Epoch: 4\n",
    "* Loss Function: Categorical cross entropy (因為要做複數類別分類)\n",
    "* Optimizer: Adam\n",
    "* Learning Rate: 10<sup>-6</sup> (1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 416M/416M [02:56<00:00, 2.47MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 890/890 [07:40<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.749                 | Train Accuracy:  0.348                 | Val Loss:  0.646                 | Val Accuracy:  0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 890/890 [07:55<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.591                 | Train Accuracy:  0.562                 | Val Loss:  0.465                 | Val Accuracy:  0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 890/890 [07:57<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.340                 | Train Accuracy:  0.821                 | Val Loss:  0.154                 | Val Accuracy:  0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 890/890 [08:00<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.118                 | Train Accuracy:  0.971                 | Val Loss:  0.080                 | Val Accuracy:  0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 890/890 [08:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.061                 | Train Accuracy:  0.988                 | Val Loss:  0.045                 | Val Accuracy:  0.995\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    # 把原本的資料經過 Dataset 類別包裝起來\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    # 把訓練、驗證資料集丟進 Dataloader 定義取樣資訊 (ex: 設定 batch_size...等等)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    # 偵測有 GPU，有就用\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()                       # Loss Function: Categorical cross entropy\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate) # Optimizer: Adam\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    # 每次完整訓練 (每個 epoch) 要做的事\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            # ---------- 訓練的部分 ----------\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            # 這邊加上 tqdm 模組來顯示 dataloader 處理進度條\n",
    "            # 所以在程式意義上，可以直接把這行當作 for train_input, train_label in train_dataloader:\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                # .to(device): 把東西 (tensor) 丟到 GPU 的概念\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                # 把 data 丟進 BERT\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                # 計算 Cross Entropy，以此計算 loss\n",
    "                batch_loss = criterion(output, train_label.long())  # 參數解釋：(模型的輸出, 原本預計的輸出)\n",
    "                total_loss_train += batch_loss.item()               # .item(): tensor 轉 純量\n",
    "                \n",
    "                # 看 model output \"可能性最高\" 的 label 是不是和 data 一樣，是的話，acc + 1\n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()       # 清空前一次 Gradient\n",
    "                batch_loss.backward()   # 根據 lost 計算 back propagation\n",
    "                optimizer.step()        # 做 Gradient Decent\n",
    "            \n",
    "            # ---------- 驗證的部分 ----------\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            # 步驟和訓練時差不多，差在沒做 Gradient Decent\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model on Test Data\n",
    "把最後 10% 的 Unseen Data 拿來做測試  \n",
    "程式流程基本上和驗證差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.996\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    # 把那 10% unseen data 拿來用\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # 不用做 loss 了，所以不用丟 criterion 進去\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    \n",
    "evaluate(model, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('BERT_Practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57600a7262e08017b091bd11d6f5edc60101976fb9b9f176febaff60ea345d59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
