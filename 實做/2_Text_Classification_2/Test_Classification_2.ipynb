{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 32000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 48000\n",
      "  1%|          | 500/48000 [04:30<7:16:49,  1.81it/s]***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7575, 'learning_rate': 4.947916666666667e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "  1%|          | 500/48000 [15:43<7:16:49,  1.81it/s]Saving model checkpoint to output\\checkpoint-500\n",
      "Configuration saved in output\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7360178232192993, 'eval_accuracy': 0.492625, 'eval_precision': 1.0, 'eval_recall': 0.0007385524372230429, 'eval_f1': 0.0014760147601476016, 'eval_runtime': 673.1306, 'eval_samples_per_second': 11.885, 'eval_steps_per_second': 5.942, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-500\\pytorch_model.bin\n",
      "  2%|▏         | 1000/48000 [20:40<7:38:31,  1.71it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7204, 'learning_rate': 4.8958333333333335e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 1000/48000 [32:21<7:38:31,  1.71it/s]Saving model checkpoint to output\\checkpoint-1000\n",
      "Configuration saved in output\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2637816667556763, 'eval_accuracy': 0.767875, 'eval_precision': 0.7849573533212717, 'eval_recall': 0.7476612506154604, 'eval_f1': 0.7658555037195816, 'eval_runtime': 701.2826, 'eval_samples_per_second': 11.408, 'eval_steps_per_second': 5.704, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-1000\\pytorch_model.bin\n",
      "  3%|▎         | 1500/48000 [37:21<7:45:57,  1.66it/s]    ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.754, 'learning_rate': 4.8437500000000005e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  3%|▎         | 1500/48000 [49:03<7:45:57,  1.66it/s]Saving model checkpoint to output\\checkpoint-1500\n",
      "Configuration saved in output\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6933923363685608, 'eval_accuracy': 0.50775, 'eval_precision': 0.50775, 'eval_recall': 1.0, 'eval_f1': 0.673520145912784, 'eval_runtime': 701.9344, 'eval_samples_per_second': 11.397, 'eval_steps_per_second': 5.699, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-1500\\pytorch_model.bin\n",
      "  4%|▍         | 2000/48000 [54:05<7:40:27,  1.67it/s]    ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.704, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  4%|▍         | 2000/48000 [1:05:52<7:40:27,  1.67it/s]Saving model checkpoint to output\\checkpoint-2000\n",
      "Configuration saved in output\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6930730938911438, 'eval_accuracy': 0.50775, 'eval_precision': 0.50775, 'eval_recall': 1.0, 'eval_f1': 0.673520145912784, 'eval_runtime': 706.772, 'eval_samples_per_second': 11.319, 'eval_steps_per_second': 5.66, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-2000\\pytorch_model.bin\n",
      "  5%|▌         | 2500/48000 [1:10:55<7:38:27,  1.65it/s]    ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6966, 'learning_rate': 4.739583333333333e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                        \n",
      "  5%|▌         | 2500/48000 [1:22:44<7:38:27,  1.65it/s]Saving model checkpoint to output\\checkpoint-2500\n",
      "Configuration saved in output\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6957231163978577, 'eval_accuracy': 0.49225, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 708.704, 'eval_samples_per_second': 11.288, 'eval_steps_per_second': 5.644, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-2500\\pytorch_model.bin\n",
      "  6%|▋         | 3000/48000 [1:27:48<7:29:11,  1.67it/s]    ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6965, 'learning_rate': 4.6875e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  6%|▋         | 3000/48000 [1:39:39<7:29:11,  1.67it/s]Saving model checkpoint to output\\checkpoint-3000\n",
      "Configuration saved in output\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6930149793624878, 'eval_accuracy': 0.50775, 'eval_precision': 0.50775, 'eval_recall': 1.0, 'eval_f1': 0.673520145912784, 'eval_runtime': 710.7202, 'eval_samples_per_second': 11.256, 'eval_steps_per_second': 5.628, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-3000\\pytorch_model.bin\n",
      "  7%|▋         | 3500/48000 [1:44:44<7:25:35,  1.66it/s]    ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7147, 'learning_rate': 4.635416666666667e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                        \n",
      "  7%|▋         | 3500/48000 [1:56:36<7:25:35,  1.66it/s]Saving model checkpoint to output\\checkpoint-3500\n",
      "Configuration saved in output\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6931833028793335, 'eval_accuracy': 0.49225, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 711.332, 'eval_samples_per_second': 11.247, 'eval_steps_per_second': 5.623, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-3500\\pytorch_model.bin\n",
      "  8%|▊         | 4000/48000 [2:01:41<7:23:01,  1.66it/s]    ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7061, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                        \n",
      "  8%|▊         | 4000/48000 [2:13:32<7:23:01,  1.66it/s]Saving model checkpoint to output\\checkpoint-4000\n",
      "Configuration saved in output\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.701452374458313, 'eval_accuracy': 0.49225, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 710.6084, 'eval_samples_per_second': 11.258, 'eval_steps_per_second': 5.629, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-4000\\pytorch_model.bin\n",
      "  9%|▉         | 4500/48000 [2:18:37<7:17:07,  1.66it/s]    ***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6993, 'learning_rate': 4.5312500000000004e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\BERT_Practice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                        \n",
      "  9%|▉         | 4500/48000 [2:30:25<7:17:07,  1.66it/s]Saving model checkpoint to output\\checkpoint-4500\n",
      "Configuration saved in output\\checkpoint-4500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6986303925514221, 'eval_accuracy': 0.49225, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 708.369, 'eval_samples_per_second': 11.294, 'eval_steps_per_second': 5.647, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output\\checkpoint-4500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output\\checkpoint-3000 (score: 0.6930149793624878).\n",
      "  9%|▉         | 4500/48000 [2:30:30<24:14:52,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9030.238, 'train_samples_per_second': 10.631, 'train_steps_per_second': 5.315, 'train_loss': 0.7165679050021702, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file output/checkpoint-4000\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/checkpoint-4000\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at output/checkpoint-4000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "100%|██████████| 1250/1250 [14:15<00:00,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "for i in range(len(df['sentiment'])):\n",
    "    if df.at[i, 'sentiment'] == 'positive':\n",
    "        df.at[i, 'sentiment'] = 1\n",
    "    else:\n",
    "        df.at[i, 'sentiment'] = 0\n",
    "\n",
    "groups = df.groupby(df.sentiment)\n",
    "data_positive = groups.get_group(1)\n",
    "data_negative = groups.get_group(0)\n",
    "\n",
    "data_positive = data_positive.sample(frac=1.0)\n",
    "data_negative = data_negative.sample(frac=1.0)\n",
    "\n",
    "test_positive = data_positive.iloc[20000:, :]\n",
    "test_negative = data_negative.iloc[20000:, :]\n",
    "test_data = pd.concat([test_positive, test_negative], axis = 0, ignore_index=True).sample(frac=1)\n",
    "train_positive = data_positive.iloc[:20000, :]\n",
    "train_negative = data_negative.iloc[:20000, :]\n",
    "train_data = pd.concat([train_positive, train_negative], axis = 0, ignore_index=True).sample(frac=1)\n",
    "\n",
    "test_data.to_csv('test.csv', index = None)\n",
    "train_data.to_csv('train.csv', index = None)\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Define pretrained tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ----- 1. Preprocess data -----#\n",
    "# Preprocess data\n",
    "X = list(data[\"review\"])\n",
    "y = list(data[\"sentiment\"])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Train pre-trained model\n",
    "trainer.train()\n",
    "\n",
    "# ----- 3. Predict -----#\n",
    "# Load test data\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "X_test = list(test_data[\"review\"])\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"output/checkpoint-4000\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('BERT_Practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57600a7262e08017b091bd11d6f5edc60101976fb9b9f176febaff60ea345d59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
