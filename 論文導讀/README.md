# 論文導讀
## Seq2seq
Sequence to Sequence Learning with Neural Networks
## Transformer
Attention is all you need
## BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding