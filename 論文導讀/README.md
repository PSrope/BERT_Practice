# 論文導讀
## Sequence to Sequence Learning with Neural Networks
## Attention is all you need
## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding